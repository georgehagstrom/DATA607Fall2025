[
  {
    "objectID": "course/instructors.html",
    "href": "course/instructors.html",
    "title": "Instructor",
    "section": "",
    "text": "George Hagstrom, Ph.D.\nEmail: george.hagstrom@cuny.edu\nI am currently a Doctoral Lecturer in Data Science and Information Systems at the City University of New York. Additionally, I am a consultant with Princeton University where I development mathematical models of marine phytoplankton and heterotrophic bacteria to improve our understanding of the Earth’s Biogeochemical Cycles. Prior to joining CUNY, I was a Research Scientist at Princeton University in the Levin lab at the Department of Ecology and Evolutionary Biology and a postdoctoral researcher in the Magneto Fluids Division at the Courant Institute for Mathematical Sciences. My research interests include the application of Bayesian statistics and Machine Learning to incorporate nontraditional datasets (such as from ’omics) into mechanistic models of marine ecosystems and biogeochemical cycling, trait-based modeling, and critical transitions in complex ecological, social, or economic systems. In my free time, I enjoy cycling and exploring New York City.\n\nContact\nOffice Hours (Zoom is preferred): By appointment. You’re encouraged to schedule an appointment and I have time nearly everyday. You are also encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructors"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\n\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2023). R for Data Science (2e). O’Reilly\n\nThis is an open source textbook that can be freely read online at r4ds.hadley.nz/ but can be purchased from Amazon. The textbook was written in quarto and the source code can be found on github. This is the primary textbook for this course and the one that we will follow most closely and comprehensively.\n\n\nJennifer Bryan. Happy Git and GitHub for the R User.\n\nThis is an open source textbook that can be freely read online at happygitwithr.com/. This short online textbook introduces Git and GitHub to data scientists and statisticians and illustrates how to integrate them with the R ecosystem/toolkit.\n\n\nJulia Silge and David Robinson. (2017). Text Mining with R. O’Reilly\n\nThis is an open source textbook that can be freely read online at www.tidytextmining.com/ but can be purchased from Amazon.\n\n\n\nRecommended\nWickham, H. Advanced R. Baca Raton, FL: Taylor & Francis Group.\n\nMost of this book is available freely online at adv-r.had.co.nz but can be purchased from Amazon.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "R and RStudio\n\nWe will make use of R, an open source statistics program and language. Be sure to install R and RStudio on your own computers within the first few days of the class.\n\nR - download for Windows, Mac, or Linux.\nRStudio - Download Windows, Mac, or Linux versions from here\n\nIf using Windows, you also need to download RTools.\n\n\nSource Control\nAll course materials will be made available on Github which provides an implementation of the git open source version control system. RStudio supports git directly, but I recommend downloading Sourcetree. This is a free desktop client that provides an easier interface for working with Github. You will also need to create an account on Github.\nFor more information, Jenny Bryan’s Happy Git and Github for the useR is a free online book covering the important features of source control for R users.\n\n\nR Packages\n\nOnce everything is installed, execute the following command in RStudio to install the packages we will use for this class (you can copy-and-paste):\n\ninstall.packages(c('openintro','devtools','tidyverse', 'ggplot2',\n                   'psych','reshape2','knitr','markdown','shiny','R.rsp',\n                   'fivethirtyeight'))\n\n\n\nTinyTeX\n\nTinyTeX is a lightweight implementation of LaTeX that is highly compatible with RStudio and can be installed relatively easily compared to other LaTeX implementations that you might encounter. LaTeX is an advanced typesetting library that is essential for properly formatting pdf documents for submission. One of the most common “tech” issues arising for students in this class is getting a working LaTeX implementation. Thus I highly recommend that you install TinyTeX right away. You can install TinyTeX following this link:\n\nTinyTeX instructions",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "posts/2025-10-28-EPA-API-Vignette.html",
    "href": "posts/2025-10-28-EPA-API-Vignette.html",
    "title": "httr2 EPA Air Quality Coding Vignette",
    "section": "",
    "text": "In this video, I go through how to use httr2 to access the EPA AQS API.\nYou can watch the video on youtube by clicking here.\nThe code I wrote for this vignette can be found here: epa_vignette_live.R"
  },
  {
    "objectID": "posts/2025-09-23-Reprex-and-Debugging.html",
    "href": "posts/2025-09-23-Reprex-and-Debugging.html",
    "title": "How to find help and make a reproducible example (Debugging)",
    "section": "",
    "text": "I made a short coding demo (length about 20 minutes) where I slowly go through one example of making a reproducible example of out code with a bug, and debugging the code in the process. I talk a little bit about different places to find help. If you know what a reproducible example is, have used stack overflow, etc, this video may not be useful for you. If those concepts are new to you, then its very important for you to learn about them, unfortunately they are part of the “hidden curriculum” of data science and programming.\nYou can watch the video on youtube youtube by clicking here.\nHelpful resources for this subject include chapter 8 of your textbook, the help page for the reprex library, and the stack overflow website"
  },
  {
    "objectID": "posts/2025-09-02-Transit-Vignette-Week-2-Meetup.html",
    "href": "posts/2025-09-02-Transit-Vignette-Week-2-Meetup.html",
    "title": "Week 2 Meetup Video and Coding Vignette",
    "section": "",
    "text": "I have uploaded the video for Meetup 2, which you can watch by clicking here. After the hour of the meetup was over, I recorded some additional content, finishing the transit costs coding session and also talking a bit more about data visualization quality features and data ethics. If you want to skip to that part of the video it starts approximately at the hour mark. At some point in the middle of recording I was interrupted by something at home but hopefully it didn’t make the video too disconnected.\nIf you want to look at the code for the coding vignette, you can find both it and the data here:\n\nTransit Costs code\nTransit Costs data\nOriginal Tidy Tuesday Site"
  },
  {
    "objectID": "posts/2025-11-09-Week-12-LLMs.html",
    "href": "posts/2025-11-09-Week-12-LLMs.html",
    "title": "Week 12 Info: Large Language Models",
    "section": "",
    "text": "Hello Class,\nWeek 12 has begun. In week 12, we will learn about Large Language Models and how to interact with them systematically through R. Much of this learning will take place in the context of NLP, building on the work we did in Week 11 and illustrating an alternative way to handle language processing problems.\nAs LLMs are very new there doesn’t exist much of a formal, time-tested curriculum or a sense of which tools and methods will be relevant years from now. So we will do a quick introduction to the basics of what LLMs are and how they work, discuss the ecosystem of LLMs available today, and then focus on professional use cases, namely how to interact with LLM APIs to use LLMs to analyze and process text data.\nThis will be accomplished through the use of three R packages, ellmer, which creates a chat data type in R and allows for access to many different LLM APIs and also local instances, mall, which has built in functions to perform NLP tasks using LLMs, and vitals, which is a package for LLM evaluations.\nThis week we will have a lab assignment which involves usin LLMs to perform sentiment analysis and text classification. You will likely need to either ask me for an anthropic API key, setup your own local LLMs (such as ollama), or use your own API key from an LLM provider of your choice. Find Lab 9 here.\nFor your reading assignments and more details about this week, head over to Module 12."
  },
  {
    "objectID": "posts/2025-10-29-Github-Coding-Vignette.html",
    "href": "posts/2025-10-29-Github-Coding-Vignette.html",
    "title": "Github and RStudio Project Coding Vignette",
    "section": "",
    "text": "This video has two parts. In the first part, I finish covering the git slides that were missed last meetup. In the second part, I show you the git and github steps you need to clone the repository for this week’s assignment, to add your name and project description to the project README.md file, and to add your qmd file to the github repository. I also show you how it is possible for something to go wrong if two people are working on the same file at the same time, and how you might fix it if that happens to you.\nClick here to watch the video\nI recommend reviewing the Pull, but you have local work chapter of the happy git for the useR book for additional information."
  },
  {
    "objectID": "posts/2025-10-05-Week-7-Info.html",
    "href": "posts/2025-10-05-Week-7-Info.html",
    "title": "Week 7 Info: Joins and Databases",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 7, where we will learn how to work with data that is stored in multiple different different sources. We will study the join function and learn about primary and foreign keys, as well as simple and compound or composite keys. The discussion of keys and joins will lead in to an introduction to relational databases, which are one of the most common ways for large organizations to store data. We will talk about the trade-offs of using databases versus other data storage schemes and introduce SQL (the most common standard for languages that access and modify databases) through R packages DBI and dbplyr and duckdb, which is a relational database system designed for data analysis and portability and which is also easy to learn from. In the first weeks of this program we have discussed how R and python are two required languages for data scientists to have proficiency in. SQL and databases are equally important.\nYou will have a lab assignment titled R and SQL due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 7."
  },
  {
    "objectID": "posts/2025-10-01-Coding-Vignette-Text-Strings-Babynames.html",
    "href": "posts/2025-10-01-Coding-Vignette-Text-Strings-Babynames.html",
    "title": "Exploring Changes in US Baby Names Using stringr, forcats, and regex",
    "section": "",
    "text": "This coding demo (length about 50 minutes) shows how to use the stringr library and regular expressions to find insights about how baby names have varied throughout the past 140 years of US history.\nYou can watch the video on youtube youtube by clicking here.\nThe code for this vignette can be found by clicking here\nHelpful resources for this subject include chapter 14-16 of your textbook, the stringr package, the ‘forcats’ package regex buddy and regex crossword"
  },
  {
    "objectID": "posts/2025-11-02-Week-11-Tidy-Text-and-NLP.html",
    "href": "posts/2025-11-02-Week-11-Tidy-Text-and-NLP.html",
    "title": "Week 11 Info: Tidy Text and Natural Language Processing",
    "section": "",
    "text": "Hello Class,\nWeek 11 has begun. In week 11, we will build on our previous module on strings and learn how to work with text data. Text data comes in many different forms, from books and news articles to technical manuals, legal documents, or product reviews. This data lacks the rectangular structure that many other datasets naturally posses, and requires processing in order to generate features that enable us to run analyses or generate insights.\nWe will read the first 4 chapters of the online book text mining with R: a tidy approach, which cover how to wrangle textual data and convert it into a tidy format, how to work with lexicons to peform basic sentiment analysis, how to interpret the frequency of words occuring in text, and how to make use of word combinations (n-grams) and to compute correlations. The primary tool will be a tidyverse package called tidytext which has built in functions to tokenize text data into a tidy format and to analyze the resulting dataset.\nThis week we will have a lab assignment which involves extracting data stored in a hierarchical format, and combining webscraping tools with functions and iteration to automatically download many files at once. Find Lab 8 here. Note: I have added an alternative, shorter data file for those whoser personal computers struggle with the original one short_reviews.json\nFor your reading assignments and more details about this week, head over to Module 11.."
  },
  {
    "objectID": "posts/2025-09-01-Meetup-2-Slides.html",
    "href": "posts/2025-09-01-Meetup-2-Slides.html",
    "title": "Meetup 2 Slides",
    "section": "",
    "text": "I have posted the slides for Meetup 2 which you can view by clicking here\nThis meetup we are going to discuss why visualizations matter, go over some basics of how to make them, do a practice vignette showing how to use dplyr and ggplot, and if we have time some data visualization ethics.\nSome things if you need:\n\nMeetup Link Zoom\nUse the google doodle poll to sign up for your Data Science in Context presentation"
  },
  {
    "objectID": "posts/2025-08-25-Welcome_to_DATA_607.html",
    "href": "posts/2025-08-25-Welcome_to_DATA_607.html",
    "title": "Welcome to DATA 607",
    "section": "",
    "text": "Welcome to DATA 607, Data Acquisition and Management.\nClick this post now to get started!\nHere are the first steps:\n\nRead the syllabus.\nGo through the course overview materials, and complete the week 1 readings.\nDownload, install, and configure R, then RStudio.\nSign up for a (free) GitHub account.\nIntroduce yourself on the course discussion forum on Brightspace.\nJoin our Slack workspace:\n\nI will invite you to our DATA 607 Slack Channel within that workspace\n\n\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nUse the google doodle poll to sign up for your Data Science in Context presentation\n\nI recorded a short video which introduces me and gives you more background on the course:"
  },
  {
    "objectID": "posts/2025-11-11-Meetup-12-Extra.html",
    "href": "posts/2025-11-11-Meetup-12-Extra.html",
    "title": "Meetup 12 Extras: Structured Chats and Tool Calling",
    "section": "",
    "text": "This video completes the coverage of the material we ended to cover in last night’s meetup. We introduce tools for forcing chats to have structured output, which is very useful for fuzzy data processing tasks, and we illustrate how to register tools with chat objects, enabling them to call user defined functions.\nYou can watch the video on youtube youtube by clicking here.\nThe slides are the same as the meetup 11 slides found by clicking here"
  },
  {
    "objectID": "posts/2025-11-14-Vitals-Vignette.html",
    "href": "posts/2025-11-14-Vitals-Vignette.html",
    "title": "Evaluating LLMs for R Coding using Vitals",
    "section": "",
    "text": "In this coding vignette I go through the introductory vignette from the vitals webpage, which shows how to use vitals to evaluate the performance on different coding challenges. The dataset has both text questions and also a description of the answer in text with samples of allowable code. We will use the “LLM-as-a-judge” approach, in which a grader LLM compares the output of each LLM on each question either to an answer or evaluates it using a description of answer criteria. My video follows the vitals example closely except for a few choices of prompt and the LLMs used.\nClick here to watch the video\nThe live code that was used for this vignette can be found on github at Vitals_Live.R."
  },
  {
    "objectID": "posts/2025-08-25-Meetup-1-Slides.html",
    "href": "posts/2025-08-25-Meetup-1-Slides.html",
    "title": "Meetup 1 Slides",
    "section": "",
    "text": "I have posted the slides for Meetup 1 which you can view by clicking here\nThis week we are going to discuss the nuts and bolts of what this course is about, and give a brief introduction to the data science workflow and the tools that you will be using.\nHere are some key first steps (duplicated from the initial announcement):\n\nRead the syllabus.\nGo through the course overview materials, and complete the week 1 readings.\nDownload, install, and configure R, then RStudio.\nSign up for a (free) GitHub account.\nIntroduce yourself on the course discussion forum on Brightspace.\nJoin our Slack workspace:\n\nI will invite you to our DATA 607 Slack Channel within that workspace\n\n\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nUse the google doodle poll to sign up for your Data Science in Context presentation"
  },
  {
    "objectID": "posts/2025-09-08-Meetup-3-Slides.html",
    "href": "posts/2025-09-08-Meetup-3-Slides.html",
    "title": "Meetup 3 Slides",
    "section": "",
    "text": "I have posted the slides for Meetup 3 which you can view by clicking here\nDuring this meetup, we are going to introduce the concept of tidy data, which is a style of structuring and storing data that makes it easy to analyze with R and the tidyverse. We will go over the basic motivations and definitions and hopefully get through some live coding sessions using tidyr.\nIf you want to follow along with the coding vignette, you can download the data and the code here:\n\nWeather Station Vignette Code\nWeather Station Vignette Data\n\nSome things if you need:\n\nMeetup Link Zoom\nUse the google doodle poll to sign up for your Data Science in Context presentation"
  },
  {
    "objectID": "posts/2025-10-06-Joins-and-SQL-Videos.html",
    "href": "posts/2025-10-06-Joins-and-SQL-Videos.html",
    "title": "Coding Vignettes on Joins and Databases and a SQL Lecture",
    "section": "",
    "text": "Hello,\nThere are 3 additional videos this week.\n\nThe first video covers some material on R and SQL that we won’t be able to cover in tonight’s meetup. I talk about what databases are and when you should use them instead of regular files, and I introduce the concept of database normalization and explain what it is for. Then I show some tools for working with SQL using R and use it to go over some basic SQL queries.\n\nClick here to watch the video and click here for the slides\n\nThe second video is a coding practice session using joins. We practice joins on the Lahman database of baseball statistics. We use joins to combine information from different tibbles to determine how salary and award status relates to homeruns hit.\n\nClick here to watch the Joins Coding Vignette and download the R that I used in the vignette here\n\nIn the third video, I convert the billboard dataset which contains songs and their billboard rankings into a database, using duckdb. Forgive me for the weird confusion at the end of the video where I realize that the read_csv interpreted read my time in with unexpected units, you can skip that part if you like as I don’t finish resolving it.\n\nClick here to watch the billboard vignette . If you want to download the data, click here for billboard.csv and you can find the code at DatabasePrepBillboardVignette.R"
  },
  {
    "objectID": "posts/2025-10-21-Webscraping-Vignette.html",
    "href": "posts/2025-10-21-Webscraping-Vignette.html",
    "title": "Webscraping Vignette",
    "section": "",
    "text": "This coding demo (45 minutes) shows three examples of webscraping. The frist two are very easy examples where a table exists on the page and two or three function calls are enough to download the data and covert it into a tibble. The third is much more complex, and shows you how to use the code inspector to find the elements corresponding to your data, how to extract those elements using rvest, and how to use a function to automate the process for the entire website.\nYou can download the code for this vignette for the course github page by clicking here: WebscrapingVignette.R\nSomewhat more prepared file: WebscrapingVignetteOld.R (this isn’t the live file that I was typing)\nYou can watch the video on youtube youtube by clicking here.\nThis video had some mishaps (had to stop several times because of stuff falling in my house, also sunlight started blocking my computer screen so the process became clunky). A past version of this vignette took me 45 minutes and was a bit smoother for the IMDB website, so if you are watching and finding my video annoying check the past one by clicking here.\nHelpful resources for this subject include chapter 26 of your textbook and the ‘rvest’ package, and the website for selector gadget\nOther helpful resources:\n\nThe CSS Selector Tutorial\nThe polite package"
  },
  {
    "objectID": "modules/module1.html",
    "href": "modules/module1.html",
    "title": "Module 1 - Introduction to R and the Data Science Workflow",
    "section": "",
    "text": "Overview and Deliverables\nThis week is about meeting the other students, making sure you get a good start with required course software, and taking a bird’s eye view at data science workflows and the material that we will cover in the rest of the course. As such, the only official required assignments that you must turn in are to make a post on course Brightspace page introducing yourself, and to complete the meetup reflection (this is the same thing as the 1 minute papers of Data 606 if you are also taking that course). Howeer, you should also read the introduction posts of your classmates, perform the readings, and make sure to follow the tutorials and install the required software. Start early if you can with the software so we can sort out any issues asap. I’ll be monitoring the course slack page, which is the place where we will have most of our asynchronous communication during the course.\n\n08/25: Attend the initial course meetup at 6:45PM Eastern\nDue 9/01: Introduction Post in Brightspace Discussions\nSign up for GitHub and the course Slack channel\nComplete the readings of RDS (Intro and 28), the quarto tutorial, and up to section 15 in the happygitwithr tutorial\nInstall R, RStudio, git, etc (see the software page for more details\nSign up using the doodle poll for your data science in context presentation.\n\n\nLearning Objectives\n\nData Science Workflow\nCourse Toolkit: R, RStudio, tidyverse, Quarto, git\n\n\n\nReadings\n\nRDS (R for Data Science): Introduction, Chapter 28\n\nThe Introduction chapter will describe the workflow of a typical data science project and pave the road for the rest of the material we are covering in this course. Chapter 28 describes the quarto markdown language, which a flexible system for authoring projects and presentations that produces well typeset documents in a variety of file formats and integrates with a large number of different programming languages, so that computations and their results can be directly embedded and visualized in your documents. I suggest that you use quarto to create your homework assignments.\n\nQuarto tutorial\n\nThis is an official quarto tutorial.\n\nHappy Git and GitHub for the R User: Tutorial up through Section 15\n\nGit is a tool that allows a group of people to work together on a software project, enabling version control that allows differences to be more easily resolved. GitHub is a repository for software projects that integrates well with Git. Git is a commonly used tool, and is almost a requirement for teams with large codebases and more than a few people working on them. GitHub has almost been like the “LinkedIn” for Data Scientists and Software Developers. One of the goals of this degree program is to help you build a portfolio of work on GitHub that you can use to demonstrate your coding proficiency to potential employers. These tools can be intimidating because they have a steep learning curve, so I think it is important to start introducing them slowly, at the very start. There are many ways to interact with Git and GitHub, but this excellent book by Jenny Bryant focuses on the specific case of a Data Scientist working in R and RStudio. If you already know Git and have a good system, you aren’t required to follow this tutorial, but for those of you who are new to these tools this I recommend taking some time this week to get Git up and running and integrated with your RStudio installation.\n\nData Visualization: A Practical Introduction Appendix A1: How to Read an R Help File\n\nThis is another excellent resource on learning R, but the reading I’ve suggested here is an appendix which discusses the structure of help files. Being able to use help files successfully is a skill that separates novice and intermediate programmers from experienced ones, as help files often have a very formal structure that doesn’t read like a normal text document or piece of writing. This section can you know what to expect and how to get what you need from them instead of feeling frustrated.\n\n\nVideos\nHadley Wickham Introduces the Tidyverse\nThis course uses something called the tidyverse. The tidyverse is a series of packages that replaces much of the functionality of base R while at the same time giving the resulting R code a very different flavor. This course, and many of the courses in this program, start by jumping right into the tidyverse. This video provides an introduction to the tidyverse from one of its main creators. However, while this decision has some initial benefits, it isn’t without tradeoffs, so it is important for you to keep in mind that there are other, different ways to do things in R. We will explore some of these later in the class in the Big Data module and the Advanced R programming module.\nIf you want to read a little bit more about the trade offs of the different R ecosystems this is a short link:\nR Comparisons\nSpeaking with colleagues, the choice to use base R versus the tidyverse is quite polarizing, especially as you work in more complex projects.",
    "crumbs": [
      "Topics",
      "1 - Data Science Workflow and Toolkit"
    ]
  },
  {
    "objectID": "modules/module7.html",
    "href": "modules/module7.html",
    "title": "Module 7 - Working with Databases and SQL",
    "section": "",
    "text": "Learning Objectives\n\nWorking with multiple dataframes, Joins and Keys\nRelational Database basics: design and tradeoffs\nWorking with R and SQL\nBasic database normalization\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 18-21\n\n\n\nAdditional Resources:\n\nSQL Cheat Sheet\nPractical SQL: A Beginner’s Guide to Storytelling with Data. Anthony DeBarros. No Start Press (2022)\nSQL for Data Scientists. Renee Teate. Wiley.\nduckdb page and SQL intro\n\n\n\nVideos\nMother Duck tutorial",
    "crumbs": [
      "Topics",
      "7 - Working with Databases and SQL"
    ]
  },
  {
    "objectID": "modules/module3.html",
    "href": "modules/module3.html",
    "title": "Module 3 - Data Tidying",
    "section": "",
    "text": "Learning Objectives\n\nConverting between wide and long data formats with tidyr\nChanging the shape of your data with dplyr\nImporting data into R\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 5-7\n\n\n\nAdditional Resources:\n\ntidyr pivot vignette\n\nThis vignette has some even more in depth discussion than the textbook.\n\nTidy Data. Hadley Wickham. Journal of statistical software 59 (2014): 1-23.\ntidyr vignette\n\nThe original Tidy Data paper is one of the most influential papers on data processing. It relates Tidy Data to database normalization, which might be familiar to you if you are an expert in SQL. The accompanying vignette has code used to generate the results in the paper. If you do read the paper, note that the definition of tidy data is different than the one in your textbook. The original definition is close to a relational database style of organization, while the current definition is more of a flat file approach that resembles a data matrix and which is more readily analyzed.\n\nTutorial on Cleveland Dot Plots\n\nThis is a classic data visualization tool that we will use in this week’s homework\n\nwidyr package\n\nA useful package that gets a little bit more at the trade-offs between different data organization schemes. We don’t use it this week, but you could use it solve parts of the last problem more efficiently.\n\n\nVideos\nThere are some excellent optional video lectures that discuss tidy data and data wrangling:\n\nData Science Box Intro to Tidy Data\nData Science Box Data Wranglig\nData Science Box Data Transformations\nData Science Box Exploration of Tidy Data",
    "crumbs": [
      "Topics",
      "3 - Data Tidying"
    ]
  },
  {
    "objectID": "modules/module6.html",
    "href": "modules/module6.html",
    "title": "Module 6 - Processing Strings and Text",
    "section": "",
    "text": "Learning Objectives\n\nLearn to use regular expressions (regex) to find text\nProcess and transform strings in R\nWorking with factors for Categorical data\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 14, 15, 16\n\n\n\nAdditional Resources:\n\nRegex Buddy\nRegex Crossword\n\nRegular expressions are useful in a large number of applications. The first website provides an advanced tutorial, the second one provides fun exercises. If you are unsatisfied and want to learn even more, regular expressions are equivalent to a computer science concept known as “finite state automata”. You can dive way deeper by reading these lecture notes:\n\nFinite State Automata\nstringr package: reference index. This is the tidyverse package for processing strings. If you are stuck on a string problem the solution is probably somewhere here\nforcats package reference index . This is the reference for a very useful package for handling categorical data\nWrangling Categorical Data in R. A. McNamara and N. Horton. The American Statistician (2018) Vol 72.\n\nThis paper discusses both tidyverse and base R approaches to categorical data.",
    "crumbs": [
      "Topics",
      "6 - Processing Strings and Text"
    ]
  },
  {
    "objectID": "modules/module9.html",
    "href": "modules/module9.html",
    "title": "Module 9 - Web Scraping and APIs",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand and manipulate hierarchical/web Data Structures: JSON, HTML\nScraping websites with R\nEthics: Data Privacy Issues\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 23-24\nOK Cupid Case Study\n\n\n\nVideos\n\nCambridge Analytica Whistleblower\nExtracting Data from the Web Part I\nExtracting Data from the Web Part II",
    "crumbs": [
      "Topics",
      "9 - Web Scraping and APIs"
    ]
  },
  {
    "objectID": "modules/module12.html",
    "href": "modules/module12.html",
    "title": "Module 12 - Large Language Models",
    "section": "",
    "text": "Learning Objectives\n\nQualitative basics of LLM Function:\n\nTokenization\nTraining\nReinforcement Learning and Finetuning\n\nMap of the LLM model ecosystem\nInteracting with LLMs in R using ellmer\nLLM Evaluation using vitals\nStructured Data from LLMs\nTool Calling\n\n\n\nReadings (In order of importance from essential to optional)\n\nEllmer Package Vignettes\nMall Package Vignettes\nVitals Package\nGood enough prompting\nClaude Prompting Guide\n\n\n\nVideos\n\nDeep Dive into LLMs by Andrej Karpathy Long but classic\nA Hacker’s Guide to Open Source LLMs",
    "crumbs": [
      "Topics",
      "12 - Large Language Models"
    ]
  },
  {
    "objectID": "modules/module5.html",
    "href": "modules/module5.html",
    "title": "Module 5 - Data Transformations",
    "section": "",
    "text": "Learning Objectives\n\nWorking with boolean data, basic logical operations, conditionals, and logical subsetting\nTransformations and summaries of numeric data\nUsing dplyr window functions\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 12, 13, 17\n\n\n\nAdditional Resources:\n\nFeature Engineering: A-Z Absolutely comprehensive encyclopedia of feature engineering teachniques\nFeature Engineering and Selection, Chapter 1 Feature engineering is a natural extension of data transformation that is commonly used prior to the application of machine learning or statistical analyses.\nforcats package reference index . This is the reference for a very useful package for handling categorical data\nWrangling Categorical Data in R. A. McNamara and N. Horton. The American Statistician (2018) Vol 72.\n\nThis paper discusses both tidyverse and base R approaches to categorical data.",
    "crumbs": [
      "Topics",
      "5 - Data Transformations"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "We acknowledge the wonderful resources made available on the open-source course Data Science in a Box, from which we adapted some course materials and homework assignments. This material is shared under a CC BY-SA 4.0 Creative Commons Share Alike License\nThis website would not be possible without the quarto package.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#this-week",
    "href": "meetups/Meetup8/Meetup8.html#this-week",
    "title": "Meetup 8: Functions and Iteration",
    "section": "This week",
    "text": "This week\n\nNo Lab!\nWork on your project proposals, reach out if you want my opinion\nToday:\n\nFunctions\nIterations\n\nVignettes will be synced with next week’s material"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#hw-comments",
    "href": "meetups/Meetup8/Meetup8.html#hw-comments",
    "title": "Meetup 8: Functions and Iteration",
    "section": "HW Comments",
    "text": "HW Comments\n\nCheck LLM answer\n\nAccuracy of LLM solutions varied a lot\nA few exact answers- are you sure my answer wasn’t in your context window?\n\nRegex subtlety\nMy fault: [^aeoui] will match things like “1” or “$”\nSelf Contained HTML (hat tip to Masoud):\n\nBrightspace strips HTML submissions of resources etc"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#when-to-use-each-join",
    "href": "meetups/Meetup8/Meetup8.html#when-to-use-each-join",
    "title": "Meetup 8: Functions and Iteration",
    "section": "When to use each join?",
    "text": "When to use each join?\n\nleft_join and right_join are the most common\nfull_join is rare: for messy datasets or data quality checks\ninner_join only when you want to also filter data\nanti_join usually to search for missing values\nsemi_join when you don’t need the data\nanti_join missing values"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#meetup-next-wednesday",
    "href": "meetups/Meetup8/Meetup8.html#meetup-next-wednesday",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Meetup Next Wednesday",
    "text": "Meetup Next Wednesday\n Buy tickets here"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#case-study-functions-make-your-code-simpler",
    "href": "meetups/Meetup8/Meetup8.html#case-study-functions-make-your-code-simpler",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Case Study: Functions Make Your Code Simpler",
    "text": "Case Study: Functions Make Your Code Simpler\n\nI want to make a statistical model of how the ratio of carbon to phosphorus in marine phytoplankton varies with environmental conditions\nI have a mathematical model for what phytoplankton do at different levels of temperature, nutrients, and environments.\nThere are some unknown parameters, use data to constrain them"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#case-study-statistical-model",
    "href": "meetups/Meetup8/Meetup8.html#case-study-statistical-model",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Case Study: Statistical Model",
    "text": "Case Study: Statistical Model\n\nEnvironmental vars: \\(\\mathrm{Temp}\\), \\(\\mathrm{Irr}\\), \\(\\mathrm{NO_3}\\), \\(\\mathrm{PO_4}\\),\nParameters \\(\\mathrm{pars}\\)\nerror \\(\\sigma\\)\nFunction CP that I created (could be linear regression, a neural network, anything, my case something fancy)\nUse Maximum Likelihood or a Bayesian model with priors on parameters: \\[ \\mathrm{{C{:}P}} \\sim \\mathrm{lognormal}(CP\\mathrm{(Temp,Irr,NO_3,PO_4,pars)},\\sigma)\\]\nWhat does this look like in code?"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-1",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nName\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nFunction name defined on first line"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-2",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-2",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nArguments\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nArguments are external data passed to function"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-3",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-3",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nBody\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nBody of function performs computations"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-4",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-4",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nOutput\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  if_else(is_pct, num / 100, num)\n}\n\n\nLast line is output"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#defining-functions-5",
    "href": "meetups/Meetup8/Meetup8.html#defining-functions-5",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFunction has several parts:\nOutput\n\n\nclean_number = function(x) {\n  is_pct &lt;- str_detect(x, \"%\")\n  num &lt;- x |&gt; \n    str_remove_all(\"%\") |&gt; \n    str_remove_all(\",\") |&gt; \n    str_remove_all(fixed(\"$\")) |&gt; \n    as.numeric()\n  return(if_else(is_pct, num / 100, num))\n}\n\n\nCan use return statement also"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#calling-functions",
    "href": "meetups/Meetup8/Meetup8.html#calling-functions",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Calling Functions:",
    "text": "Calling Functions:\n\nclean_number(\"$120,000,000.1\")\n\n[1] 1.2e+08\n\nvec = c(\"3.1415\",\"200%\",\"-100\")\n\nvec |&gt; clean_number()\n\n[1]    3.1415    2.0000 -100.0000\n\nour_list = list(1,\"1\",\"one\")\n\nour_list |&gt; clean_number()\n\n[1]  1  1 NA\n\nbillboard |&gt; clean_number()\n\n [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n[76] NA NA NA NA"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#what-are-functions-good-for",
    "href": "meetups/Meetup8/Meetup8.html#what-are-functions-good-for",
    "title": "Meetup 8: Functions and Iteration",
    "section": "What are Functions Good For?",
    "text": "What are Functions Good For?\nIf you find yourself repeating code more than a few times, consider turning it into a function\n\nFunctions make code easier to read\nIf you change your code, easier to change it in one place\nFunctions make it easier to avoid bugs, enable testing\nIt is easy to share a function"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#scoping-rules-in-r",
    "href": "meetups/Meetup8/Meetup8.html#scoping-rules-in-r",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Scoping Rules in R",
    "text": "Scoping Rules in R\n\nsin = 1\n\nsin\n\n[1] 1\n\nsin(0)\n\n[1] 0\n\nz=1\nmyFunc = function(){\n  print(z)\n  z = 5\n  print(z)\n  \n}\nmyFunc()\n\n[1] 1\n[1] 5\n\nprint(z)\n\n[1] 1"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#scoping-environments",
    "href": "meetups/Meetup8/Meetup8.html#scoping-environments",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Scoping: Environments",
    "text": "Scoping: Environments\n\nR variables and functions live in nested environments\nfunctions (and some flow control) create inward scope\n\n\nz = 1\nf = function(x,y){\n  x*y + z\n}\n\nf(0,1)\n\n[1] 1\n\n\n\nR will look for variable names starting at inner scope and then moving outward"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#more-scope",
    "href": "meetups/Meetup8/Meetup8.html#more-scope",
    "title": "Meetup 8: Functions and Iteration",
    "section": "More Scope",
    "text": "More Scope\n\nf = function(x){\n  x + a\n}\n\na=0\nf(1)\n\n[1] 1\n\na=1\nf(1)\n\n[1] 2"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#side-effects",
    "href": "meetups/Meetup8/Meetup8.html#side-effects",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Side Effects",
    "text": "Side Effects\n\nLet’s say you really want your function to change something outside function scope\nUse &lt;&lt;- operator:\n\n\nz = 10\nside_effect = function(x){\n  z &lt;&lt;- x^2\n  return()\n}\nside_effect(-9)\n\nNULL\n\nz\n\n[1] 81\n\n\n\nUse sparingly, has side effect of making your code difficult to understand and overly interconnected"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-masking",
    "href": "meetups/Meetup8/Meetup8.html#data-masking",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Masking",
    "text": "Data Masking\n\nWhat if we try to make a function that takes a data frame?\n\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by(group_var) |&gt; \n    summarize(mean(mean_var))\n}\n\n\ndiamonds |&gt; grouped_mean(cut, carat)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\n\nWhen group_by and summarize are called, they treat group_var and mean_var literally as names of variables to search for in data\nCalled data masking because real values of group_var and mean_var ignored"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-masking-1",
    "href": "meetups/Meetup8/Meetup8.html#data-masking-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Masking",
    "text": "Data Masking\n\nWe can see that the real values are ignored:\n\n\ndf &lt;- tibble(\n  mean_var = 1,\n  group_var = \"g\",\n  group = 1,\n  x = 10,\n  y = 100\n)\n\ndf |&gt; grouped_mean(group, x)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\ndf |&gt; grouped_mean(group, y)\n\n# A tibble: 1 × 2\n  group_var `mean(mean_var)`\n  &lt;chr&gt;                &lt;dbl&gt;\n1 g                        1\n\n\n\nIt didn’t matter what the argument names we picked were"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#indirection",
    "href": "meetups/Meetup8/Meetup8.html#indirection",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Indirection",
    "text": "Indirection\n\nFundamental Theorem of Software Engineering:\n“We can solve any problem by introducing an extra layer of indirection”\n\n\nAn indirection or reference is a way to refer to something using a name, reference, or container, instead of the value itself\nR expects name to be typed in\nembrace the variable: filter(df, {{var}} == cond)"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#embracing-in-action",
    "href": "meetups/Meetup8/Meetup8.html#embracing-in-action",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Embracing in Action",
    "text": "Embracing in Action\n\ngrouped_mean &lt;- function(df, group_var, mean_var) {\n  df |&gt; \n    group_by({{group_var}}) |&gt; \n    summarize(mean({{mean_var}}))\n}\n\n\ndiamonds |&gt; grouped_mean(cut, carat)\n\n# A tibble: 5 × 2\n  cut       `mean(carat)`\n  &lt;ord&gt;             &lt;dbl&gt;\n1 Fair              1.05 \n2 Good              0.849\n3 Very Good         0.806\n4 Premium           0.892\n5 Ideal             0.703"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-maskingtidy-selection",
    "href": "meetups/Meetup8/Meetup8.html#data-maskingtidy-selection",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Masking/Tidy Selection",
    "text": "Data Masking/Tidy Selection\n\nlibrary(nycflights13)\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by({{ group_vars }}) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\n\nflights |&gt; \n  count_missing(c(year,month,day), dep_time)\n\nError in `group_by()`:\nℹ In argument: `c(year, month, day)`.\nCaused by error:\n! `c(year, month, day)` must be size 336776 or 1, not 1010328."
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#pick-lets-you-tidy-select",
    "href": "meetups/Meetup8/Meetup8.html#pick-lets-you-tidy-select",
    "title": "Meetup 8: Functions and Iteration",
    "section": "pick lets you tidy select",
    "text": "pick lets you tidy select\n\ncount_missing &lt;- function(df, group_vars, x_var) {\n  df |&gt; \n    group_by(pick({{ group_vars }})) |&gt; \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |&gt; \n  count_missing(c(year,month,day), dep_time) |&gt; head(6)\n\n# A tibble: 6 × 4\n   year month   day n_miss\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  2013     1     1      4\n2  2013     1     2      8\n3  2013     1     3     10\n4  2013     1     4      6\n5  2013     1     5      3\n6  2013     1     6      1"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#functional-programming",
    "href": "meetups/Meetup8/Meetup8.html#functional-programming",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nFunctions are first class objects in R:\nTreated like a normal data type\n\nPass them to functions\nModify them\nAssign to variables\nReturn them from functions"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#functional-programming-1",
    "href": "meetups/Meetup8/Meetup8.html#functional-programming-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Functional Programming",
    "text": "Functional Programming\n\nFunctions are first class objects in R:\n\n\npower &lt;- function(g,exponent) {\n  function(x) {\n    g(x) ^ exponent\n  }\n}\n\n\nf = power(cos,2)\nf(-3.14159)\n\n[1] 1\n\n\n\nStyle of R looks more functional than other languages\npurrr library provides functional programming tools"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns",
    "href": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns",
    "title": "Meetup 8: Functions and Iteration",
    "section": "across iterates on columns",
    "text": "across iterates on columns\nSuppose we want to apply an operation to many columns at once:\n\ndf &lt;- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf\n\n# A tibble: 10 × 4\n         a      b       c      d\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.333   0.408  0.103  -1.39 \n 2  1.21   -0.647 -1.17   -1.76 \n 3 -0.599   0.166 -0.0823 -0.976\n 4 -0.188   0.913 -1.54   -0.439\n 5 -0.350  -0.146  1.50    0.287\n 6  0.0939  1.53   0.957   0.353\n 7 -2.06    0.996 -0.676   1.26 \n 8 -0.265  -0.644 -0.398   0.206\n 9  0.676   0.136 -1.24    1.15 \n10  1.18    0.293 -1.44   -0.883"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-1",
    "href": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "across iterates on columns",
    "text": "across iterates on columns\n\nCount the elements and compute median:\n\n\ndf |&gt; summarise(\n  n = n(),\n  a = mean(a),\n  b = mean(b),\n  c = mean(c),\n  d = mean(d)\n)\n\n# A tibble: 1 × 5\n      n       a     b      c      d\n  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    10 -0.0631 0.300 -0.399 -0.220\n\n\n\nInvolves lots of copy/pasting\nDoesn’t scale well with number of columns"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-2",
    "href": "meetups/Meetup8/Meetup8.html#across-iterates-on-columns-2",
    "title": "Meetup 8: Functions and Iteration",
    "section": "across iterates on columns",
    "text": "across iterates on columns\n\nAlternative is function across\npart of purrr functional programming library\nArguments: function and selection of columns:\n\n\ndf |&gt; summarise(\n  n = n(),\n  across(a:d,mean)\n)\n\n# A tibble: 1 × 5\n      n       a     b      c      d\n  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    10 -0.0631 0.300 -0.399 -0.220\n\n\n\nCan use other selects (i.e. starts_with, everything)"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across",
    "href": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Anonymous functions in across",
    "text": "Anonymous functions in across\n\nHow to pass additional arguments to functions?\nAnonymous (or lambda functions), functions not important enough to be named:\n\n\n\\(x) median(x^2 * exp(-x^2)) \n\nfunction (x) \nmedian(x^2 * exp(-x^2))"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across-1",
    "href": "meetups/Meetup8/Meetup8.html#anonymous-functions-in-across-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Anonymous functions in across",
    "text": "Anonymous functions in across\n\nCan use this to pass arguments:\n\n\ndf |&gt; summarise(\n  n = n(),\n  across(a:d, \\(x) mean(x,na.rm=TRUE))\n)\n\n# A tibble: 1 × 5\n      n       a     b      c      d\n  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    10 -0.0631 0.300 -0.399 -0.220"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map-family-in-purrr",
    "href": "meetups/Meetup8/Meetup8.html#map-family-in-purrr",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map family in purrr",
    "text": "map family in purrr\n\nmap(vec,f) = [f(vec[1]),f(vec[2]),....] \n\n\nmap(1:3,exp)\n\n[[1]]\n[1] 2.718282\n\n[[2]]\n[1] 7.389056\n\n[[3]]\n[1] 20.08554"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map-family-in-purrr-1",
    "href": "meetups/Meetup8/Meetup8.html#map-family-in-purrr-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map family in purrr",
    "text": "map family in purrr\n\nmap(vec,f) = [f(vec[1]),f(vec[2]),....] \nmap returns a list, not a vector\nR functions do not have strict return types"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map_",
    "href": "meetups/Meetup8/Meetup8.html#map_",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map_*",
    "text": "map_*\n\nmap_dbl, map_chr, map_lgl, map_int: assume data type and return vectors\nOperate on data frames \nImagine the data frame has been rotated so the “rows” correspond to columns"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#map_-1",
    "href": "meetups/Meetup8/Meetup8.html#map_-1",
    "title": "Meetup 8: Functions and Iteration",
    "section": "map_*",
    "text": "map_*\n\nmtcars |&gt; head(8)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\nmtcars |&gt; map_dbl(\\(x) length(unique(x)))\n\n mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n  25    3   27   22   22   29   30    2    2    3    6 \n\n\n\nYour function must return a single function of the same type as the map function"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#where-are-the-for-loops",
    "href": "meetups/Meetup8/Meetup8.html#where-are-the-for-loops",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Where are the for loops?",
    "text": "Where are the for loops?\n\nfor and while loops exist in R\nThey are discouraged in favor of tools like map\nOnly use them when you must (such as when you need side effects)\n\n\nfor (element in vector){\n  func(element) # Run some code that uses element\n}"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#while-loop-in-action",
    "href": "meetups/Meetup8/Meetup8.html#while-loop-in-action",
    "title": "Meetup 8: Functions and Iteration",
    "section": "while loop in action",
    "text": "while loop in action\n\ncounter = 0\nwhile (rnorm(1) &lt; 2) {\n  counter = counter + 1\n  \n}\n\ncounter\n\n[1] 0"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#data-science-in-context-presentation",
    "href": "meetups/Meetup8/Meetup8.html#data-science-in-context-presentation",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup8/Meetup8.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup8/Meetup8.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 8: Functions and Iteration",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-color-scheme",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-color-scheme",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6: Color Scheme",
    "text": "Lab 6: Color Scheme\n\nColor Scheme for map:\n\n\nx = rnorm(n=100)\ny = rnorm(n=100)\nz = rcauchy(n=100,location=0,scale=1)\ndata = tibble(x = x, y=y,z=z)\n\ndata |&gt; ggplot(aes(x=x,y=y,color=z)) + geom_point(size=10) +\n  scale_color_gradient2(high=\"blue\",low=\"red\",mid = \"white\") +\n  theme_minimal()"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-color-scheme-1",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-color-scheme-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6: : Color Scheme",
    "text": "Lab 6: : Color Scheme\n\nColor Scheme for map:"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-color-scheme-2",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-color-scheme-2",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6: Color Scheme",
    "text": "Lab 6: Color Scheme\n\nInstead use perceptually uniform color scheme or pick different middle color:\n\nscale_color_viridis\nscale_color_gradient2(high=\"blue\",low=\"red\",mid = \"white\",midpoint=0)"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-review-principle-of-proportional-ink",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-review-principle-of-proportional-ink",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6 Review: Principle of Proportional Ink",
    "text": "Lab 6 Review: Principle of Proportional Ink\n\nOn me since I said to consider using size…..\nscale_size_area, only use for positive variables"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-review",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-review",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6 Review",
    "text": "Lab 6 Review\n\njoin disordering\nIn SQL joins can change the order of columns\n\n\n  left_join(colleges_clean, by = \"unitid\") |&gt;  \n  slice_max(order_by = avg_profit, n = 10) \n\n  slice_max(order_by = avg_profit, n = 10) \n  left_join(colleges_clean, by = \"unitid\") |&gt;"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-review-keys",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-review-keys",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6 Review: Keys",
    "text": "Lab 6 Review: Keys\n\n\n# A tibble: 132,327 × 28\n    year unitid institution_name  city_txt state_cd zip_text classification_code\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                  &lt;dbl&gt;\n 1  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 2  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 3  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 4  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 5  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 6  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 7  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 8  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n 9  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n10  2015 100654 Alabama A & M Un… Normal   AL       35762                      2\n# ℹ 132,317 more rows\n# ℹ 21 more variables: classification_name &lt;chr&gt;, classification_other &lt;chr&gt;,\n#   ef_male_count &lt;dbl&gt;, ef_female_count &lt;dbl&gt;, ef_total_count &lt;dbl&gt;,\n#   sector_cd &lt;dbl&gt;, sector_name &lt;chr&gt;, sportscode &lt;dbl&gt;, partic_men &lt;dbl&gt;,\n#   partic_women &lt;dbl&gt;, partic_coed_men &lt;dbl&gt;, partic_coed_women &lt;dbl&gt;,\n#   sum_partic_men &lt;dbl&gt;, sum_partic_women &lt;dbl&gt;, rev_men &lt;dbl&gt;,\n#   rev_women &lt;dbl&gt;, total_rev_menwomen &lt;dbl&gt;, exp_men &lt;dbl&gt;, …"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#lab-6-primary-keys",
    "href": "meetups/Meetup9/Meetup9.html#lab-6-primary-keys",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Lab 6: Primary Keys",
    "text": "Lab 6: Primary Keys\n\nMost common mistake on Lab 6 was not identifying keys correctly in the college sports dataset\nKey ideas:\n\nRemove duplicate rows from new tables\nKey should uniquely identify each row\nIs year needed?\nunitid versus institution_name?"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#week-ahead-webscraping-and-hierarchical-data",
    "href": "meetups/Meetup9/Meetup9.html#week-ahead-webscraping-and-hierarchical-data",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Week Ahead: Webscraping and Hierarchical Data",
    "text": "Week Ahead: Webscraping and Hierarchical Data\n\nRead: Chapter 23 and 24\nLab 7\nLengthy Vignette"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#hierarchical-data",
    "href": "meetups/Meetup9/Meetup9.html#hierarchical-data",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Hierarchical Data",
    "text": "Hierarchical Data\n\nData stored in a tree-strucutre\nParent nodes are connected to children\n\n\ncran data.tree"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#hierarchical-data-1",
    "href": "meetups/Meetup9/Meetup9.html#hierarchical-data-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Hierarchical Data",
    "text": "Hierarchical Data\n\nOne of the most common ways to store data\nEasy to understand, can be fast\nHas some drawbacks (read Codd 1970)\n\n\ncran data.tree"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#list-data-type",
    "href": "meetups/Meetup9/Meetup9.html#list-data-type",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "List data type",
    "text": "List data type\n\nList is like a vector, but data types don’t have to be the same\n\n\nlist1 &lt;- list(vector = 1:4, letter = \"a\", logical = TRUE)\n\nstr(list1)\n\nList of 3\n $ vector : int [1:4] 1 2 3 4\n $ letter : chr \"a\"\n $ logical: logi TRUE\n\nlist1[[2]]\n\n[1] \"a\"\n\nlist1[\"vector\"]\n\n$vector\n[1] 1 2 3 4\n\n\n\nElements can be named"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#list-can-contain-lists",
    "href": "meetups/Meetup9/Meetup9.html#list-can-contain-lists",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "List can contain lists",
    "text": "List can contain lists\n\nThis allows lists to be hierarchical\n\n\ncountry_list = list( USA = list(capital = \"Washington\", pop = 330, continent = \"NA\"),\n                     Brazil = list(capital = \"Brasilia\", pop = 212, continent = \"SA\"),\n                     Poland = list(capital = \"Warsaw\",pop = 38, continent = \"EU\"))\n\nstr(country_list)\n\nList of 3\n $ USA   :List of 3\n  ..$ capital  : chr \"Washington\"\n  ..$ pop      : num 330\n  ..$ continent: chr \"NA\"\n $ Brazil:List of 3\n  ..$ capital  : chr \"Brasilia\"\n  ..$ pop      : num 212\n  ..$ continent: chr \"SA\"\n $ Poland:List of 3\n  ..$ capital  : chr \"Warsaw\"\n  ..$ pop      : num 38\n  ..$ continent: chr \"EU\"\n\n\n\nUse str or View to visualize hierarchical lists"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#rectangling",
    "href": "meetups/Meetup9/Meetup9.html#rectangling",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Rectangling",
    "text": "Rectangling\n\nTo analyze hierarchical data, need to get it into a table\nUse unnest functions\nTwo common scenarios\n\nNamed children w/ common structure\n\n\nunnest_wider\n\n\nUnnamed children\n\n\nunnest_longer"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#unnest_wider",
    "href": "meetups/Meetup9/Meetup9.html#unnest_wider",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "unnest_wider",
    "text": "unnest_wider\n\nWhen list children have names and common structure, each list member can become a new columns:\n\n\n  df1 &lt;- tribble(\n  ~x, ~y,\n  1, list(a = 11, b = 12),\n  2, list(a = 21, b = 22),\n  3, list(a = 31, b = 32),\n)\n  \ndf1 |&gt; unnest_wider(y)\n\n# A tibble: 3 × 3\n      x     a     b\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    11    12\n2     2    21    22\n3     3    31    32"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#unnest_longer",
    "href": "meetups/Meetup9/Meetup9.html#unnest_longer",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "unnest_longer",
    "text": "unnest_longer\n\nWhen there list elements aren’t named, each list member gets put into a new row\n\n\ndf2 &lt;- tribble(\n  ~x, ~y,\n  1, list(11, 12, 13),\n  2, list(21),\n  3, list(31, 32),\n)\n\ndf2 |&gt; unnest_longer(y)\n\n# A tibble: 6 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1    11\n2     1    12\n3     1    13\n4     2    21\n5     3    31\n6     3    32"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#got-case-study",
    "href": "meetups/Meetup9/Meetup9.html#got-case-study",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "GOT Case Study",
    "text": "GOT Case Study\n\nrepurrrsive package contains hierarchical datasets for practice\ngot_chars contains info on Game of Thrones characters\n\n\nstr(got_chars)\n\nList of 30\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1022\"\n  ..$ id         : int 1022\n  ..$ name       : chr \"Theon Greyjoy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In 278 AC or 279 AC, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Prince of Winterfell\" \"Lord of the Iron Islands (by law of the green lands)\"\n  ..$ aliases    : chr [1:4] \"Prince of Fools\" \"Theon Turncloak\" \"Reek\" \"Theon Kinslayer\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Greyjoy of Pyke\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:2] \"A Clash of Kings\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Alfie Allen\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1052\"\n  ..$ id         : int 1052\n  ..$ name       : chr \"Tyrion Lannister\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 273 AC, at Casterly Rock\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Acting Hand of the King (former)\" \"Master of Coin (former)\"\n  ..$ aliases    : chr [1:11] \"The Imp\" \"Halfman\" \"The boyman\" \"Giant of Lannister\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/2044\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:2] \"A Feast for Crows\" \"The World of Ice and Fire\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Peter Dinklage\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1074\"\n  ..$ id         : int 1074\n  ..$ name       : chr \"Victarion Greyjoy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In 268 AC or before, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Lord Captain of the Iron Fleet\" \"Master of the Iron Victory\"\n  ..$ aliases    : chr \"The Iron Captain\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Greyjoy of Pyke\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1109\"\n  ..$ id         : int 1109\n  ..$ name       : chr \"Will\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"\"\n  ..$ died       : chr \"In 297 AC, at Haunted Forest\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr \"A Clash of Kings\"\n  ..$ povBooks   : chr \"A Game of Thrones\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"Bronson Webb\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1166\"\n  ..$ id         : int 1166\n  ..$ name       : chr \"Areo Hotah\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Norvoshi\"\n  ..$ born       : chr \"In 257 AC or before, at Norvos\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Captain of the Guard at Sunspear\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Nymeros Martell of Sunspear\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:2] \"Season 5\" \"Season 6\"\n  ..$ playedBy   : chr \"DeObia Oparei\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1267\"\n  ..$ id         : int 1267\n  ..$ name       : chr \"Chett\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"At Hag's Mire\"\n  ..$ died       : chr \"In 299 AC, at Fist of the First Men\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr [1:2] \"A Game of Thrones\" \"A Clash of Kings\"\n  ..$ povBooks   : chr \"A Storm of Swords\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1295\"\n  ..$ id         : int 1295\n  ..$ name       : chr \"Cressen\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 219 AC or 220 AC\"\n  ..$ died       : chr \"In 299 AC, at Dragonstone\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Maester\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr [1:2] \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Clash of Kings\"\n  ..$ tvSeries   : chr \"Season 2\"\n  ..$ playedBy   : chr \"Oliver Ford\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/130\"\n  ..$ id         : int 130\n  ..$ name       : chr \"Arianne Martell\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Dornish\"\n  ..$ born       : chr \"In 276 AC, at Sunspear\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Princess of Dorne\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Nymeros Martell of Sunspear\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1303\"\n  ..$ id         : int 1303\n  ..$ name       : chr \"Daenerys Targaryen\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Valyrian\"\n  ..$ born       : chr \"In 284 AC, at Dragonstone\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:5] \"Queen of the Andals and the Rhoynar and the First Men, Lord of the Seven Kingdoms\" \"Khaleesi of the Great Grass Sea\" \"Breaker of Shackles/Chains\" \"Queen of Meereen\" ...\n  ..$ aliases    : chr [1:11] \"Dany\" \"Daenerys Stormborn\" \"The Unburnt\" \"Mother of Dragons\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1346\"\n  ..$ allegiances: chr \"House Targaryen of King's Landing\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Emilia Clarke\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1319\"\n  ..$ id         : int 1319\n  ..$ name       : chr \"Davos Seaworth\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Westeros\"\n  ..$ born       : chr \"In 260 AC or before, at King's Landing\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:4] \"Ser\" \"Lord of the Rainwood\" \"Admiral of the Narrow Sea\" \"Hand of the King\"\n  ..$ aliases    : chr [1:5] \"Onion Knight\" \"Davos Shorthand\" \"Ser Onions\" \"Onion Lord\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1676\"\n  ..$ allegiances: chr [1:2] \"House Baratheon of Dragonstone\" \"House Seaworth of Cape Wrath\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:3] \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 2\" \"Season 3\" \"Season 4\" \"Season 5\" ...\n  ..$ playedBy   : chr \"Liam Cunningham\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/148\"\n  ..$ id         : int 148\n  ..$ name       : chr \"Arya Stark\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 289 AC, at Winterfell\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Princess\"\n  ..$ aliases    : chr [1:16] \"Arya Horseface\" \"Arya Underfoot\" \"Arry\" \"Lumpyface\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : list()\n  ..$ povBooks   : chr [1:5] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\" ...\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Maisie Williams\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/149\"\n  ..$ id         : int 149\n  ..$ name       : chr \"Arys Oakheart\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Reach\"\n  ..$ born       : chr \"At Old Oak\"\n  ..$ died       : chr \"In 300 AC, at the Greenblood\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Ser\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Oakheart of Old Oak\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/150\"\n  ..$ id         : int 150\n  ..$ name       : chr \"Asha Greyjoy\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In 275 AC or 276 AC, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Princess\" \"Captain of the Black Wind\" \"Conqueror of Deepwood Motte\"\n  ..$ aliases    : chr [1:2] \"Esgred\" \"The Kraken's Daughter\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1372\"\n  ..$ allegiances: chr [1:2] \"House Greyjoy of Pyke\" \"House Ironmaker\"\n  ..$ books      : chr [1:2] \"A Game of Thrones\" \"A Clash of Kings\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:3] \"Season 2\" \"Season 3\" \"Season 4\"\n  ..$ playedBy   : chr \"Gemma Whelan\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/168\"\n  ..$ id         : int 168\n  ..$ name       : chr \"Barristan Selmy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Westeros\"\n  ..$ born       : chr \"In 237 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Ser\" \"Hand of the Queen\"\n  ..$ aliases    : chr [1:5] \"Barristan the Bold\" \"Arstan Whitebeard\" \"Ser Grandfather\" \"Barristan the Old\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr [1:2] \"House Selmy of Harvest Hall\" \"House Targaryen of King's Landing\"\n  ..$ books      : chr [1:5] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\" ...\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:4] \"Season 1\" \"Season 3\" \"Season 4\" \"Season 5\"\n  ..$ playedBy   : chr \"Ian McElhinney\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/2066\"\n  ..$ id         : int 2066\n  ..$ name       : chr \"Varamyr\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Free Folk\"\n  ..$ born       : chr \"At a village Beyond the Wall\"\n  ..$ died       : chr \"In 300 AC, at a village Beyond the Wall\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:3] \"Varamyr Sixskins\" \"Haggon\" \"Lump\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr \"A Storm of Swords\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/208\"\n  ..$ id         : int 208\n  ..$ name       : chr \"Brandon Stark\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 290 AC, at Winterfell\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Prince of Winterfell\"\n  ..$ aliases    : chr [1:3] \"Bran\" \"Bran the Broken\" \"The Winged Wolf\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Isaac Hempstead-Wright\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/216\"\n  ..$ id         : int 216\n  ..$ name       : chr \"Brienne of Tarth\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 280 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:3] \"The Maid of Tarth\" \"Brienne the Beauty\" \"Brienne the Blue\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr [1:3] \"House Baratheon of Storm's End\" \"House Stark of Winterfell\" \"House Tarth of Evenfall Hall\"\n  ..$ books      : chr [1:3] \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr [1:5] \"Season 2\" \"Season 3\" \"Season 4\" \"Season 5\" ...\n  ..$ playedBy   : chr \"Gwendoline Christie\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/232\"\n  ..$ id         : int 232\n  ..$ name       : chr \"Catelyn Stark\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Rivermen\"\n  ..$ born       : chr \"In 264 AC, at Riverrun\"\n  ..$ died       : chr \"In 299 AC, at the Twins\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Lady of Winterfell\"\n  ..$ aliases    : chr [1:5] \"Catelyn Tully\" \"Lady Stoneheart\" \"The Silent Sistet\" \"Mother Mercilesr\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/339\"\n  ..$ allegiances: chr [1:2] \"House Stark of Winterfell\" \"House Tully of Riverrun\"\n  ..$ books      : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ tvSeries   : chr [1:3] \"Season 1\" \"Season 2\" \"Season 3\"\n  ..$ playedBy   : chr \"Michelle Fairley\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/238\"\n  ..$ id         : int 238\n  ..$ name       : chr \"Cersei Lannister\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Westerman\"\n  ..$ born       : chr \"In 266 AC, at Casterly Rock\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:5] \"Light of the West\" \"Queen Dowager\" \"Protector of the Realm\" \"Lady of Casterly Rock\" ...\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/901\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Lena Headey\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/339\"\n  ..$ id         : int 339\n  ..$ name       : chr \"Eddard Stark\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 263 AC, at Winterfell\"\n  ..$ died       : chr \"In 299 AC, at Great Sept of Baelor in King's Landing\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr [1:5] \"Lord of Winterfell\" \"Warden of the North\" \"Hand of the King\" \"Protector of the Realm\" ...\n  ..$ aliases    : chr [1:3] \"Ned\" \"The Ned\" \"The Quiet Wolf\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/232\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : chr [1:5] \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\" \"A Dance with Dragons\" ...\n  ..$ povBooks   : chr \"A Game of Thrones\"\n  ..$ tvSeries   : chr [1:2] \"Season 1\" \"Season 6\"\n  ..$ playedBy   : chr [1:3] \"Sean Bean\" \"Sebastian Croft\" \"Robert Aramayo\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/529\"\n  ..$ id         : int 529\n  ..$ name       : chr \"Jaime Lannister\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Westerlands\"\n  ..$ born       : chr \"In 266 AC, at Casterly Rock\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Ser\" \"Lord Commander of the Kingsguard\" \"Warden of the East (formerly)\"\n  ..$ aliases    : chr [1:4] \"The Kingslayer\" \"The Lion of Lannister\" \"The Young Lion\" \"Cripple\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:2] \"A Game of Thrones\" \"A Clash of Kings\"\n  ..$ povBooks   : chr [1:3] \"A Storm of Swords\" \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Nikolaj Coster-Waldau\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/576\"\n  ..$ id         : int 576\n  ..$ name       : chr \"Jon Connington\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Stormlands\"\n  ..$ born       : chr \"In or between 263 AC and 265 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Lord of Griffin's Roost\" \"Hand of the King\" \"Hand of the True King\"\n  ..$ aliases    : chr \"Griffthe Mad King's Hand\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr [1:2] \"House Connington of Griffin's Roost\" \"House Targaryen of King's Landing\"\n  ..$ books      : chr [1:3] \"A Storm of Swords\" \"A Feast for Crows\" \"The World of Ice and Fire\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/583\"\n  ..$ id         : int 583\n  ..$ name       : chr \"Jon Snow\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 283 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Lord Commander of the Night's Watch\"\n  ..$ aliases    : chr [1:8] \"Lord Snow\" \"Ned Stark's Bastard\" \"The Snow of Winterfell\" \"The Crow-Come-Over\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Kit Harington\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/60\"\n  ..$ id         : int 60\n  ..$ name       : chr \"Aeron Greyjoy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In or between 269 AC and 273 AC, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Priest of the Drowned God\" \"Captain of the Golden Storm (formerly)\"\n  ..$ aliases    : chr [1:2] \"The Damphair\" \"Aeron Damphair\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Greyjoy of Pyke\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr \"Season 6\"\n  ..$ playedBy   : chr \"Michael Feast\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/605\"\n  ..$ id         : int 605\n  ..$ name       : chr \"Kevan Lannister\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 244 AC\"\n  ..$ died       : chr \"In 300 AC, at King's Landing\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr [1:4] \"Ser\" \"Master of laws\" \"Lord Regent\" \"Protector of the Realm\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/327\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:4] \"Season 1\" \"Season 2\" \"Season 5\" \"Season 6\"\n  ..$ playedBy   : chr \"Ian Gelder\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/743\"\n  ..$ id         : int 743\n  ..$ name       : chr \"Melisandre\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Asshai\"\n  ..$ born       : chr \"At Unknown\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:5] \"The Red Priestess\" \"The Red Woman\" \"The King's Red Shadow\" \"Lady Red\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr [1:3] \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 2\" \"Season 3\" \"Season 4\" \"Season 5\" ...\n  ..$ playedBy   : chr \"Carice van Houten\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/751\"\n  ..$ id         : int 751\n  ..$ name       : chr \"Merrett Frey\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Rivermen\"\n  ..$ born       : chr \"In 262 AC\"\n  ..$ died       : chr \"In 300 AC, at Near Oldstones\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr \"Merrett Muttonhead\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/712\"\n  ..$ allegiances: chr \"House Frey of the Crossing\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Storm of Swords\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/844\"\n  ..$ id         : int 844\n  ..$ name       : chr \"Quentyn Martell\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Dornish\"\n  ..$ born       : chr \"In 281 AC, at Sunspear, Dorne\"\n  ..$ died       : chr \"In 300 AC, at Meereen\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Prince\"\n  ..$ aliases    : chr [1:4] \"Frog\" \"Prince Frog\" \"The prince who came too late\" \"The Dragonrider\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Nymeros Martell of Sunspear\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/954\"\n  ..$ id         : int 954\n  ..$ name       : chr \"Samwell Tarly\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Andal\"\n  ..$ born       : chr \"In 283 AC, at Horn Hill\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:7] \"Sam\" \"Ser Piggy\" \"Prince Pork-chop\" \"Lady Piggy\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Tarly of Horn Hill\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr [1:2] \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"John Bradley-West\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/957\"\n  ..$ id         : int 957\n  ..$ name       : chr \"Sansa Stark\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 286 AC, at Winterfell\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Princess\"\n  ..$ aliases    : chr [1:3] \"Little bird\" \"Alayne Stone\" \"Jonquil\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1052\"\n  ..$ allegiances: chr [1:2] \"House Baelish of Harrenhal\" \"House Stark of Winterfell\"\n  ..$ books      : chr \"A Dance with Dragons\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Sophie Turner\""
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#rectangling-got",
    "href": "meetups/Meetup9/Meetup9.html#rectangling-got",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Rectangling GOT",
    "text": "Rectangling GOT\n\nStart by turning it into tibble:\n\n\nchars = tibble(json = got_chars)\nchars\n\n# A tibble: 30 × 1\n   json             \n   &lt;list&gt;           \n 1 &lt;named list [18]&gt;\n 2 &lt;named list [18]&gt;\n 3 &lt;named list [18]&gt;\n 4 &lt;named list [18]&gt;\n 5 &lt;named list [18]&gt;\n 6 &lt;named list [18]&gt;\n 7 &lt;named list [18]&gt;\n 8 &lt;named list [18]&gt;\n 9 &lt;named list [18]&gt;\n10 &lt;named list [18]&gt;\n# ℹ 20 more rows"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#rectangling-got-1",
    "href": "meetups/Meetup9/Meetup9.html#rectangling-got-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Rectangling GOT",
    "text": "Rectangling GOT\n\nLists are named, structure is same, so use unnest_wider\n\n\nchars |&gt; unnest_wider(json)\n\n# A tibble: 30 × 18\n   url           id name  gender culture born  died  alive titles aliases father\n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;list&gt; &lt;list&gt;  &lt;chr&gt; \n 1 https://w…  1022 Theo… Male   \"Ironb… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 2 https://w…  1052 Tyri… Male   \"\"      \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 3 https://w…  1074 Vict… Male   \"Ironb… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 4 https://w…  1109 Will  Male   \"\"      \"\"    \"In … FALSE &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 5 https://w…  1166 Areo… Male   \"Norvo… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 6 https://w…  1267 Chett Male   \"\"      \"At … \"In … FALSE &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 7 https://w…  1295 Cres… Male   \"\"      \"In … \"In … FALSE &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 8 https://w…   130 Aria… Female \"Dorni… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n 9 https://w…  1303 Daen… Female \"Valyr… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n10 https://w…  1319 Davo… Male   \"Weste… \"In … \"\"    TRUE  &lt;chr&gt;  &lt;chr&gt;   \"\"    \n# ℹ 20 more rows\n# ℹ 7 more variables: mother &lt;chr&gt;, spouse &lt;chr&gt;, allegiances &lt;list&gt;,\n#   books &lt;list&gt;, povBooks &lt;list&gt;, tvSeries &lt;list&gt;, playedBy &lt;list&gt;"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#create-povbooks-table",
    "href": "meetups/Meetup9/Meetup9.html#create-povbooks-table",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Create povBooks table",
    "text": "Create povBooks table\n\nIn Game of Thrones books, some characters had point of view chapters\nLet’s create a table with all the data on that\n\n\nchars |&gt;\n  unnest_wider(json) |&gt; \n  select(id,name,povBooks) |&gt; head(5)\n\n# A tibble: 5 × 3\n     id name              povBooks \n  &lt;int&gt; &lt;chr&gt;             &lt;list&gt;   \n1  1022 Theon Greyjoy     &lt;chr [2]&gt;\n2  1052 Tyrion Lannister  &lt;chr [4]&gt;\n3  1074 Victarion Greyjoy &lt;chr [2]&gt;\n4  1109 Will              &lt;chr [1]&gt;\n5  1166 Areo Hotah        &lt;chr [2]&gt;"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#create-povbooks-table-1",
    "href": "meetups/Meetup9/Meetup9.html#create-povbooks-table-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Create povBooks table",
    "text": "Create povBooks table\n\nThe lists aren’t named, so we need to use unnest_longer\n\n\nchars |&gt; \n  unnest_wider(json) |&gt; \n  select(id,name,povBooks) |&gt; \n  unnest_longer(povBooks) |&gt; head(10)\n\n# A tibble: 10 × 3\n      id name              povBooks            \n   &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;               \n 1  1022 Theon Greyjoy     A Clash of Kings    \n 2  1022 Theon Greyjoy     A Dance with Dragons\n 3  1052 Tyrion Lannister  A Game of Thrones   \n 4  1052 Tyrion Lannister  A Clash of Kings    \n 5  1052 Tyrion Lannister  A Storm of Swords   \n 6  1052 Tyrion Lannister  A Dance with Dragons\n 7  1074 Victarion Greyjoy A Feast for Crows   \n 8  1074 Victarion Greyjoy A Dance with Dragons\n 9  1109 Will              A Game of Thrones   \n10  1166 Areo Hotah        A Feast for Crows"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#json-files",
    "href": "meetups/Meetup9/Meetup9.html#json-files",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "JSON Files",
    "text": "JSON Files\n\nHierarchical data format commonly used on the internet\n\n\n\n[1] │ {\"menu\": {\n    │   \"id\": \"file\",\n    │   \"value\": \"File\",\n    │   \"popup\": {\n    │     \"menuitem\": [\n    │       {\"value\": \"New\", \"onclick\": \"CreateNewDoc()\"},\n    │       {\"value\": \"Open\", \"onclick\": \"OpenDoc()\"},\n    │       {\"value\": \"Close\", \"onclick\": \"CloseDoc()\"}\n    │     ]\n    │   }\n    │ }}"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#json-files-1",
    "href": "meetups/Meetup9/Meetup9.html#json-files-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "JSON Files",
    "text": "JSON Files\n\nstring, number, bool, null\nArray: [1.0, 3.0, 1, \"Cat\"]\nObject: {\"Capital\":\"Washington\",\"Pop\":330000000}\n\nNested \"key\":value pairs"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#jsonlite",
    "href": "meetups/Meetup9/Meetup9.html#jsonlite",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "jsonlite",
    "text": "jsonlite\n\nSeveral functions exist for reading and parsing json files\n\n\ngot_chars_json()\n\n[1] \"/home/georgehagstrom/R/x86_64-pc-linux-gnu-library/4.5/repurrrsive/extdata/got_chars.json\"\n\ngot_chars_json() |&gt; read_json()\n\n[[1]]\n[[1]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1022\"\n\n[[1]]$id\n[1] 1022\n\n[[1]]$name\n[1] \"Theon Greyjoy\"\n\n[[1]]$gender\n[1] \"Male\"\n\n[[1]]$culture\n[1] \"Ironborn\"\n\n[[1]]$born\n[1] \"In 278 AC or 279 AC, at Pyke\"\n\n[[1]]$died\n[1] \"\"\n\n[[1]]$alive\n[1] TRUE\n\n[[1]]$titles\n[[1]]$titles[[1]]\n[1] \"Prince of Winterfell\"\n\n[[1]]$titles[[2]]\n[1] \"Lord of the Iron Islands (by law of the green lands)\"\n\n\n[[1]]$aliases\n[[1]]$aliases[[1]]\n[1] \"Prince of Fools\"\n\n[[1]]$aliases[[2]]\n[1] \"Theon Turncloak\"\n\n[[1]]$aliases[[3]]\n[1] \"Reek\"\n\n[[1]]$aliases[[4]]\n[1] \"Theon Kinslayer\"\n\n\n[[1]]$father\n[1] \"\"\n\n[[1]]$mother\n[1] \"\"\n\n[[1]]$spouse\n[1] \"\"\n\n[[1]]$allegiances\n[1] \"House Greyjoy of Pyke\"\n\n[[1]]$books\n[[1]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[1]]$books[[2]]\n[1] \"A Storm of Swords\"\n\n[[1]]$books[[3]]\n[1] \"A Feast for Crows\"\n\n\n[[1]]$povBooks\n[[1]]$povBooks[[1]]\n[1] \"A Clash of Kings\"\n\n[[1]]$povBooks[[2]]\n[1] \"A Dance with Dragons\"\n\n\n[[1]]$tvSeries\n[[1]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[1]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[1]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[1]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[1]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[1]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[1]]$playedBy\n[1] \"Alfie Allen\"\n\n\n[[2]]\n[[2]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1052\"\n\n[[2]]$id\n[1] 1052\n\n[[2]]$name\n[1] \"Tyrion Lannister\"\n\n[[2]]$gender\n[1] \"Male\"\n\n[[2]]$culture\n[1] \"\"\n\n[[2]]$born\n[1] \"In 273 AC, at Casterly Rock\"\n\n[[2]]$died\n[1] \"\"\n\n[[2]]$alive\n[1] TRUE\n\n[[2]]$titles\n[[2]]$titles[[1]]\n[1] \"Acting Hand of the King (former)\"\n\n[[2]]$titles[[2]]\n[1] \"Master of Coin (former)\"\n\n\n[[2]]$aliases\n[[2]]$aliases[[1]]\n[1] \"The Imp\"\n\n[[2]]$aliases[[2]]\n[1] \"Halfman\"\n\n[[2]]$aliases[[3]]\n[1] \"The boyman\"\n\n[[2]]$aliases[[4]]\n[1] \"Giant of Lannister\"\n\n[[2]]$aliases[[5]]\n[1] \"Lord Tywin's Doom\"\n\n[[2]]$aliases[[6]]\n[1] \"Lord Tywin's Bane\"\n\n[[2]]$aliases[[7]]\n[1] \"Yollo\"\n\n[[2]]$aliases[[8]]\n[1] \"Hugor Hill\"\n\n[[2]]$aliases[[9]]\n[1] \"No-Nose\"\n\n[[2]]$aliases[[10]]\n[1] \"Freak\"\n\n[[2]]$aliases[[11]]\n[1] \"Dwarf\"\n\n\n[[2]]$father\n[1] \"\"\n\n[[2]]$mother\n[1] \"\"\n\n[[2]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/2044\"\n\n[[2]]$allegiances\n[1] \"House Lannister of Casterly Rock\"\n\n[[2]]$books\n[[2]]$books[[1]]\n[1] \"A Feast for Crows\"\n\n[[2]]$books[[2]]\n[1] \"The World of Ice and Fire\"\n\n\n[[2]]$povBooks\n[[2]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[2]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[2]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n[[2]]$povBooks[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[2]]$tvSeries\n[[2]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[2]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[2]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[2]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[2]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[2]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[2]]$playedBy\n[1] \"Peter Dinklage\"\n\n\n[[3]]\n[[3]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1074\"\n\n[[3]]$id\n[1] 1074\n\n[[3]]$name\n[1] \"Victarion Greyjoy\"\n\n[[3]]$gender\n[1] \"Male\"\n\n[[3]]$culture\n[1] \"Ironborn\"\n\n[[3]]$born\n[1] \"In 268 AC or before, at Pyke\"\n\n[[3]]$died\n[1] \"\"\n\n[[3]]$alive\n[1] TRUE\n\n[[3]]$titles\n[[3]]$titles[[1]]\n[1] \"Lord Captain of the Iron Fleet\"\n\n[[3]]$titles[[2]]\n[1] \"Master of the Iron Victory\"\n\n\n[[3]]$aliases\n[1] \"The Iron Captain\"\n\n[[3]]$father\n[1] \"\"\n\n[[3]]$mother\n[1] \"\"\n\n[[3]]$spouse\n[1] \"\"\n\n[[3]]$allegiances\n[1] \"House Greyjoy of Pyke\"\n\n[[3]]$books\n[[3]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[3]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[3]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n\n[[3]]$povBooks\n[[3]]$povBooks[[1]]\n[1] \"A Feast for Crows\"\n\n[[3]]$povBooks[[2]]\n[1] \"A Dance with Dragons\"\n\n\n[[3]]$tvSeries\n[1] \"\"\n\n[[3]]$playedBy\n[1] \"\"\n\n\n[[4]]\n[[4]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1109\"\n\n[[4]]$id\n[1] 1109\n\n[[4]]$name\n[1] \"Will\"\n\n[[4]]$gender\n[1] \"Male\"\n\n[[4]]$culture\n[1] \"\"\n\n[[4]]$born\n[1] \"\"\n\n[[4]]$died\n[1] \"In 297 AC, at Haunted Forest\"\n\n[[4]]$alive\n[1] FALSE\n\n[[4]]$titles\n[1] \"\"\n\n[[4]]$aliases\n[1] \"\"\n\n[[4]]$father\n[1] \"\"\n\n[[4]]$mother\n[1] \"\"\n\n[[4]]$spouse\n[1] \"\"\n\n[[4]]$allegiances\nlist()\n\n[[4]]$books\n[1] \"A Clash of Kings\"\n\n[[4]]$povBooks\n[1] \"A Game of Thrones\"\n\n[[4]]$tvSeries\n[1] \"\"\n\n[[4]]$playedBy\n[1] \"Bronson Webb\"\n\n\n[[5]]\n[[5]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1166\"\n\n[[5]]$id\n[1] 1166\n\n[[5]]$name\n[1] \"Areo Hotah\"\n\n[[5]]$gender\n[1] \"Male\"\n\n[[5]]$culture\n[1] \"Norvoshi\"\n\n[[5]]$born\n[1] \"In 257 AC or before, at Norvos\"\n\n[[5]]$died\n[1] \"\"\n\n[[5]]$alive\n[1] TRUE\n\n[[5]]$titles\n[1] \"Captain of the Guard at Sunspear\"\n\n[[5]]$aliases\n[1] \"\"\n\n[[5]]$father\n[1] \"\"\n\n[[5]]$mother\n[1] \"\"\n\n[[5]]$spouse\n[1] \"\"\n\n[[5]]$allegiances\n[1] \"House Nymeros Martell of Sunspear\"\n\n[[5]]$books\n[[5]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[5]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[5]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n\n[[5]]$povBooks\n[[5]]$povBooks[[1]]\n[1] \"A Feast for Crows\"\n\n[[5]]$povBooks[[2]]\n[1] \"A Dance with Dragons\"\n\n\n[[5]]$tvSeries\n[[5]]$tvSeries[[1]]\n[1] \"Season 5\"\n\n[[5]]$tvSeries[[2]]\n[1] \"Season 6\"\n\n\n[[5]]$playedBy\n[1] \"DeObia Oparei\"\n\n\n[[6]]\n[[6]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1267\"\n\n[[6]]$id\n[1] 1267\n\n[[6]]$name\n[1] \"Chett\"\n\n[[6]]$gender\n[1] \"Male\"\n\n[[6]]$culture\n[1] \"\"\n\n[[6]]$born\n[1] \"At Hag's Mire\"\n\n[[6]]$died\n[1] \"In 299 AC, at Fist of the First Men\"\n\n[[6]]$alive\n[1] FALSE\n\n[[6]]$titles\n[1] \"\"\n\n[[6]]$aliases\n[1] \"\"\n\n[[6]]$father\n[1] \"\"\n\n[[6]]$mother\n[1] \"\"\n\n[[6]]$spouse\n[1] \"\"\n\n[[6]]$allegiances\nlist()\n\n[[6]]$books\n[[6]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[6]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n\n[[6]]$povBooks\n[1] \"A Storm of Swords\"\n\n[[6]]$tvSeries\n[1] \"\"\n\n[[6]]$playedBy\n[1] \"\"\n\n\n[[7]]\n[[7]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1295\"\n\n[[7]]$id\n[1] 1295\n\n[[7]]$name\n[1] \"Cressen\"\n\n[[7]]$gender\n[1] \"Male\"\n\n[[7]]$culture\n[1] \"\"\n\n[[7]]$born\n[1] \"In 219 AC or 220 AC\"\n\n[[7]]$died\n[1] \"In 299 AC, at Dragonstone\"\n\n[[7]]$alive\n[1] FALSE\n\n[[7]]$titles\n[1] \"Maester\"\n\n[[7]]$aliases\n[1] \"\"\n\n[[7]]$father\n[1] \"\"\n\n[[7]]$mother\n[1] \"\"\n\n[[7]]$spouse\n[1] \"\"\n\n[[7]]$allegiances\nlist()\n\n[[7]]$books\n[[7]]$books[[1]]\n[1] \"A Storm of Swords\"\n\n[[7]]$books[[2]]\n[1] \"A Feast for Crows\"\n\n\n[[7]]$povBooks\n[1] \"A Clash of Kings\"\n\n[[7]]$tvSeries\n[1] \"Season 2\"\n\n[[7]]$playedBy\n[1] \"Oliver Ford\"\n\n\n[[8]]\n[[8]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/130\"\n\n[[8]]$id\n[1] 130\n\n[[8]]$name\n[1] \"Arianne Martell\"\n\n[[8]]$gender\n[1] \"Female\"\n\n[[8]]$culture\n[1] \"Dornish\"\n\n[[8]]$born\n[1] \"In 276 AC, at Sunspear\"\n\n[[8]]$died\n[1] \"\"\n\n[[8]]$alive\n[1] TRUE\n\n[[8]]$titles\n[1] \"Princess of Dorne\"\n\n[[8]]$aliases\n[1] \"\"\n\n[[8]]$father\n[1] \"\"\n\n[[8]]$mother\n[1] \"\"\n\n[[8]]$spouse\n[1] \"\"\n\n[[8]]$allegiances\n[1] \"House Nymeros Martell of Sunspear\"\n\n[[8]]$books\n[[8]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[8]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[8]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n[[8]]$books[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[8]]$povBooks\n[1] \"A Feast for Crows\"\n\n[[8]]$tvSeries\n[1] \"\"\n\n[[8]]$playedBy\n[1] \"\"\n\n\n[[9]]\n[[9]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1303\"\n\n[[9]]$id\n[1] 1303\n\n[[9]]$name\n[1] \"Daenerys Targaryen\"\n\n[[9]]$gender\n[1] \"Female\"\n\n[[9]]$culture\n[1] \"Valyrian\"\n\n[[9]]$born\n[1] \"In 284 AC, at Dragonstone\"\n\n[[9]]$died\n[1] \"\"\n\n[[9]]$alive\n[1] TRUE\n\n[[9]]$titles\n[[9]]$titles[[1]]\n[1] \"Queen of the Andals and the Rhoynar and the First Men, Lord of the Seven Kingdoms\"\n\n[[9]]$titles[[2]]\n[1] \"Khaleesi of the Great Grass Sea\"\n\n[[9]]$titles[[3]]\n[1] \"Breaker of Shackles/Chains\"\n\n[[9]]$titles[[4]]\n[1] \"Queen of Meereen\"\n\n[[9]]$titles[[5]]\n[1] \"Princess of Dragonstone\"\n\n\n[[9]]$aliases\n[[9]]$aliases[[1]]\n[1] \"Dany\"\n\n[[9]]$aliases[[2]]\n[1] \"Daenerys Stormborn\"\n\n[[9]]$aliases[[3]]\n[1] \"The Unburnt\"\n\n[[9]]$aliases[[4]]\n[1] \"Mother of Dragons\"\n\n[[9]]$aliases[[5]]\n[1] \"Mother\"\n\n[[9]]$aliases[[6]]\n[1] \"Mhysa\"\n\n[[9]]$aliases[[7]]\n[1] \"The Silver Queen\"\n\n[[9]]$aliases[[8]]\n[1] \"Silver Lady\"\n\n[[9]]$aliases[[9]]\n[1] \"Dragonmother\"\n\n[[9]]$aliases[[10]]\n[1] \"The Dragon Queen\"\n\n[[9]]$aliases[[11]]\n[1] \"The Mad King's daughter\"\n\n\n[[9]]$father\n[1] \"\"\n\n[[9]]$mother\n[1] \"\"\n\n[[9]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/1346\"\n\n[[9]]$allegiances\n[1] \"House Targaryen of King's Landing\"\n\n[[9]]$books\n[1] \"A Feast for Crows\"\n\n[[9]]$povBooks\n[[9]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[9]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[9]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n[[9]]$povBooks[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[9]]$tvSeries\n[[9]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[9]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[9]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[9]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[9]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[9]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[9]]$playedBy\n[1] \"Emilia Clarke\"\n\n\n[[10]]\n[[10]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/1319\"\n\n[[10]]$id\n[1] 1319\n\n[[10]]$name\n[1] \"Davos Seaworth\"\n\n[[10]]$gender\n[1] \"Male\"\n\n[[10]]$culture\n[1] \"Westeros\"\n\n[[10]]$born\n[1] \"In 260 AC or before, at King's Landing\"\n\n[[10]]$died\n[1] \"\"\n\n[[10]]$alive\n[1] TRUE\n\n[[10]]$titles\n[[10]]$titles[[1]]\n[1] \"Ser\"\n\n[[10]]$titles[[2]]\n[1] \"Lord of the Rainwood\"\n\n[[10]]$titles[[3]]\n[1] \"Admiral of the Narrow Sea\"\n\n[[10]]$titles[[4]]\n[1] \"Hand of the King\"\n\n\n[[10]]$aliases\n[[10]]$aliases[[1]]\n[1] \"Onion Knight\"\n\n[[10]]$aliases[[2]]\n[1] \"Davos Shorthand\"\n\n[[10]]$aliases[[3]]\n[1] \"Ser Onions\"\n\n[[10]]$aliases[[4]]\n[1] \"Onion Lord\"\n\n[[10]]$aliases[[5]]\n[1] \"Smuggler\"\n\n\n[[10]]$father\n[1] \"\"\n\n[[10]]$mother\n[1] \"\"\n\n[[10]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/1676\"\n\n[[10]]$allegiances\n[[10]]$allegiances[[1]]\n[1] \"House Baratheon of Dragonstone\"\n\n[[10]]$allegiances[[2]]\n[1] \"House Seaworth of Cape Wrath\"\n\n\n[[10]]$books\n[1] \"A Feast for Crows\"\n\n[[10]]$povBooks\n[[10]]$povBooks[[1]]\n[1] \"A Clash of Kings\"\n\n[[10]]$povBooks[[2]]\n[1] \"A Storm of Swords\"\n\n[[10]]$povBooks[[3]]\n[1] \"A Dance with Dragons\"\n\n\n[[10]]$tvSeries\n[[10]]$tvSeries[[1]]\n[1] \"Season 2\"\n\n[[10]]$tvSeries[[2]]\n[1] \"Season 3\"\n\n[[10]]$tvSeries[[3]]\n[1] \"Season 4\"\n\n[[10]]$tvSeries[[4]]\n[1] \"Season 5\"\n\n[[10]]$tvSeries[[5]]\n[1] \"Season 6\"\n\n\n[[10]]$playedBy\n[1] \"Liam Cunningham\"\n\n\n[[11]]\n[[11]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/148\"\n\n[[11]]$id\n[1] 148\n\n[[11]]$name\n[1] \"Arya Stark\"\n\n[[11]]$gender\n[1] \"Female\"\n\n[[11]]$culture\n[1] \"Northmen\"\n\n[[11]]$born\n[1] \"In 289 AC, at Winterfell\"\n\n[[11]]$died\n[1] \"\"\n\n[[11]]$alive\n[1] TRUE\n\n[[11]]$titles\n[1] \"Princess\"\n\n[[11]]$aliases\n[[11]]$aliases[[1]]\n[1] \"Arya Horseface\"\n\n[[11]]$aliases[[2]]\n[1] \"Arya Underfoot\"\n\n[[11]]$aliases[[3]]\n[1] \"Arry\"\n\n[[11]]$aliases[[4]]\n[1] \"Lumpyface\"\n\n[[11]]$aliases[[5]]\n[1] \"Lumpyhead\"\n\n[[11]]$aliases[[6]]\n[1] \"Stickboy\"\n\n[[11]]$aliases[[7]]\n[1] \"Weasel\"\n\n[[11]]$aliases[[8]]\n[1] \"Nymeria\"\n\n[[11]]$aliases[[9]]\n[1] \"Squan\"\n\n[[11]]$aliases[[10]]\n[1] \"Saltb\"\n\n[[11]]$aliases[[11]]\n[1] \"Cat of the Canaly\"\n\n[[11]]$aliases[[12]]\n[1] \"Bets\"\n\n[[11]]$aliases[[13]]\n[1] \"The Blind Girh\"\n\n[[11]]$aliases[[14]]\n[1] \"The Ugly Little Girl\"\n\n[[11]]$aliases[[15]]\n[1] \"Mercedenl\"\n\n[[11]]$aliases[[16]]\n[1] \"Mercye\"\n\n\n[[11]]$father\n[1] \"\"\n\n[[11]]$mother\n[1] \"\"\n\n[[11]]$spouse\n[1] \"\"\n\n[[11]]$allegiances\n[1] \"House Stark of Winterfell\"\n\n[[11]]$books\nlist()\n\n[[11]]$povBooks\n[[11]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[11]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[11]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n[[11]]$povBooks[[4]]\n[1] \"A Feast for Crows\"\n\n[[11]]$povBooks[[5]]\n[1] \"A Dance with Dragons\"\n\n\n[[11]]$tvSeries\n[[11]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[11]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[11]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[11]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[11]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[11]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[11]]$playedBy\n[1] \"Maisie Williams\"\n\n\n[[12]]\n[[12]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/149\"\n\n[[12]]$id\n[1] 149\n\n[[12]]$name\n[1] \"Arys Oakheart\"\n\n[[12]]$gender\n[1] \"Male\"\n\n[[12]]$culture\n[1] \"Reach\"\n\n[[12]]$born\n[1] \"At Old Oak\"\n\n[[12]]$died\n[1] \"In 300 AC, at the Greenblood\"\n\n[[12]]$alive\n[1] FALSE\n\n[[12]]$titles\n[1] \"Ser\"\n\n[[12]]$aliases\n[1] \"\"\n\n[[12]]$father\n[1] \"\"\n\n[[12]]$mother\n[1] \"\"\n\n[[12]]$spouse\n[1] \"\"\n\n[[12]]$allegiances\n[1] \"House Oakheart of Old Oak\"\n\n[[12]]$books\n[[12]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[12]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[12]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n[[12]]$books[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[12]]$povBooks\n[1] \"A Feast for Crows\"\n\n[[12]]$tvSeries\n[1] \"\"\n\n[[12]]$playedBy\n[1] \"\"\n\n\n[[13]]\n[[13]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/150\"\n\n[[13]]$id\n[1] 150\n\n[[13]]$name\n[1] \"Asha Greyjoy\"\n\n[[13]]$gender\n[1] \"Female\"\n\n[[13]]$culture\n[1] \"Ironborn\"\n\n[[13]]$born\n[1] \"In 275 AC or 276 AC, at Pyke\"\n\n[[13]]$died\n[1] \"\"\n\n[[13]]$alive\n[1] TRUE\n\n[[13]]$titles\n[[13]]$titles[[1]]\n[1] \"Princess\"\n\n[[13]]$titles[[2]]\n[1] \"Captain of the Black Wind\"\n\n[[13]]$titles[[3]]\n[1] \"Conqueror of Deepwood Motte\"\n\n\n[[13]]$aliases\n[[13]]$aliases[[1]]\n[1] \"Esgred\"\n\n[[13]]$aliases[[2]]\n[1] \"The Kraken's Daughter\"\n\n\n[[13]]$father\n[1] \"\"\n\n[[13]]$mother\n[1] \"\"\n\n[[13]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/1372\"\n\n[[13]]$allegiances\n[[13]]$allegiances[[1]]\n[1] \"House Greyjoy of Pyke\"\n\n[[13]]$allegiances[[2]]\n[1] \"House Ironmaker\"\n\n\n[[13]]$books\n[[13]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[13]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n\n[[13]]$povBooks\n[[13]]$povBooks[[1]]\n[1] \"A Feast for Crows\"\n\n[[13]]$povBooks[[2]]\n[1] \"A Dance with Dragons\"\n\n\n[[13]]$tvSeries\n[[13]]$tvSeries[[1]]\n[1] \"Season 2\"\n\n[[13]]$tvSeries[[2]]\n[1] \"Season 3\"\n\n[[13]]$tvSeries[[3]]\n[1] \"Season 4\"\n\n\n[[13]]$playedBy\n[1] \"Gemma Whelan\"\n\n\n[[14]]\n[[14]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/168\"\n\n[[14]]$id\n[1] 168\n\n[[14]]$name\n[1] \"Barristan Selmy\"\n\n[[14]]$gender\n[1] \"Male\"\n\n[[14]]$culture\n[1] \"Westeros\"\n\n[[14]]$born\n[1] \"In 237 AC\"\n\n[[14]]$died\n[1] \"\"\n\n[[14]]$alive\n[1] TRUE\n\n[[14]]$titles\n[[14]]$titles[[1]]\n[1] \"Ser\"\n\n[[14]]$titles[[2]]\n[1] \"Hand of the Queen\"\n\n\n[[14]]$aliases\n[[14]]$aliases[[1]]\n[1] \"Barristan the Bold\"\n\n[[14]]$aliases[[2]]\n[1] \"Arstan Whitebeard\"\n\n[[14]]$aliases[[3]]\n[1] \"Ser Grandfather\"\n\n[[14]]$aliases[[4]]\n[1] \"Barristan the Old\"\n\n[[14]]$aliases[[5]]\n[1] \"Old Ser\"\n\n\n[[14]]$father\n[1] \"\"\n\n[[14]]$mother\n[1] \"\"\n\n[[14]]$spouse\n[1] \"\"\n\n[[14]]$allegiances\n[[14]]$allegiances[[1]]\n[1] \"House Selmy of Harvest Hall\"\n\n[[14]]$allegiances[[2]]\n[1] \"House Targaryen of King's Landing\"\n\n\n[[14]]$books\n[[14]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[14]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[14]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n[[14]]$books[[4]]\n[1] \"A Feast for Crows\"\n\n[[14]]$books[[5]]\n[1] \"The World of Ice and Fire\"\n\n\n[[14]]$povBooks\n[1] \"A Dance with Dragons\"\n\n[[14]]$tvSeries\n[[14]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[14]]$tvSeries[[2]]\n[1] \"Season 3\"\n\n[[14]]$tvSeries[[3]]\n[1] \"Season 4\"\n\n[[14]]$tvSeries[[4]]\n[1] \"Season 5\"\n\n\n[[14]]$playedBy\n[1] \"Ian McElhinney\"\n\n\n[[15]]\n[[15]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/2066\"\n\n[[15]]$id\n[1] 2066\n\n[[15]]$name\n[1] \"Varamyr\"\n\n[[15]]$gender\n[1] \"Male\"\n\n[[15]]$culture\n[1] \"Free Folk\"\n\n[[15]]$born\n[1] \"At a village Beyond the Wall\"\n\n[[15]]$died\n[1] \"In 300 AC, at a village Beyond the Wall\"\n\n[[15]]$alive\n[1] FALSE\n\n[[15]]$titles\n[1] \"\"\n\n[[15]]$aliases\n[[15]]$aliases[[1]]\n[1] \"Varamyr Sixskins\"\n\n[[15]]$aliases[[2]]\n[1] \"Haggon\"\n\n[[15]]$aliases[[3]]\n[1] \"Lump\"\n\n\n[[15]]$father\n[1] \"\"\n\n[[15]]$mother\n[1] \"\"\n\n[[15]]$spouse\n[1] \"\"\n\n[[15]]$allegiances\nlist()\n\n[[15]]$books\n[1] \"A Storm of Swords\"\n\n[[15]]$povBooks\n[1] \"A Dance with Dragons\"\n\n[[15]]$tvSeries\n[1] \"\"\n\n[[15]]$playedBy\n[1] \"\"\n\n\n[[16]]\n[[16]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/208\"\n\n[[16]]$id\n[1] 208\n\n[[16]]$name\n[1] \"Brandon Stark\"\n\n[[16]]$gender\n[1] \"Male\"\n\n[[16]]$culture\n[1] \"Northmen\"\n\n[[16]]$born\n[1] \"In 290 AC, at Winterfell\"\n\n[[16]]$died\n[1] \"\"\n\n[[16]]$alive\n[1] TRUE\n\n[[16]]$titles\n[1] \"Prince of Winterfell\"\n\n[[16]]$aliases\n[[16]]$aliases[[1]]\n[1] \"Bran\"\n\n[[16]]$aliases[[2]]\n[1] \"Bran the Broken\"\n\n[[16]]$aliases[[3]]\n[1] \"The Winged Wolf\"\n\n\n[[16]]$father\n[1] \"\"\n\n[[16]]$mother\n[1] \"\"\n\n[[16]]$spouse\n[1] \"\"\n\n[[16]]$allegiances\n[1] \"House Stark of Winterfell\"\n\n[[16]]$books\n[1] \"A Feast for Crows\"\n\n[[16]]$povBooks\n[[16]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[16]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[16]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n[[16]]$povBooks[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[16]]$tvSeries\n[[16]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[16]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[16]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[16]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[16]]$tvSeries[[5]]\n[1] \"Season 6\"\n\n\n[[16]]$playedBy\n[1] \"Isaac Hempstead-Wright\"\n\n\n[[17]]\n[[17]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/216\"\n\n[[17]]$id\n[1] 216\n\n[[17]]$name\n[1] \"Brienne of Tarth\"\n\n[[17]]$gender\n[1] \"Female\"\n\n[[17]]$culture\n[1] \"\"\n\n[[17]]$born\n[1] \"In 280 AC\"\n\n[[17]]$died\n[1] \"\"\n\n[[17]]$alive\n[1] TRUE\n\n[[17]]$titles\n[1] \"\"\n\n[[17]]$aliases\n[[17]]$aliases[[1]]\n[1] \"The Maid of Tarth\"\n\n[[17]]$aliases[[2]]\n[1] \"Brienne the Beauty\"\n\n[[17]]$aliases[[3]]\n[1] \"Brienne the Blue\"\n\n\n[[17]]$father\n[1] \"\"\n\n[[17]]$mother\n[1] \"\"\n\n[[17]]$spouse\n[1] \"\"\n\n[[17]]$allegiances\n[[17]]$allegiances[[1]]\n[1] \"House Baratheon of Storm's End\"\n\n[[17]]$allegiances[[2]]\n[1] \"House Stark of Winterfell\"\n\n[[17]]$allegiances[[3]]\n[1] \"House Tarth of Evenfall Hall\"\n\n\n[[17]]$books\n[[17]]$books[[1]]\n[1] \"A Clash of Kings\"\n\n[[17]]$books[[2]]\n[1] \"A Storm of Swords\"\n\n[[17]]$books[[3]]\n[1] \"A Dance with Dragons\"\n\n\n[[17]]$povBooks\n[1] \"A Feast for Crows\"\n\n[[17]]$tvSeries\n[[17]]$tvSeries[[1]]\n[1] \"Season 2\"\n\n[[17]]$tvSeries[[2]]\n[1] \"Season 3\"\n\n[[17]]$tvSeries[[3]]\n[1] \"Season 4\"\n\n[[17]]$tvSeries[[4]]\n[1] \"Season 5\"\n\n[[17]]$tvSeries[[5]]\n[1] \"Season 6\"\n\n\n[[17]]$playedBy\n[1] \"Gwendoline Christie\"\n\n\n[[18]]\n[[18]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/232\"\n\n[[18]]$id\n[1] 232\n\n[[18]]$name\n[1] \"Catelyn Stark\"\n\n[[18]]$gender\n[1] \"Female\"\n\n[[18]]$culture\n[1] \"Rivermen\"\n\n[[18]]$born\n[1] \"In 264 AC, at Riverrun\"\n\n[[18]]$died\n[1] \"In 299 AC, at the Twins\"\n\n[[18]]$alive\n[1] FALSE\n\n[[18]]$titles\n[1] \"Lady of Winterfell\"\n\n[[18]]$aliases\n[[18]]$aliases[[1]]\n[1] \"Catelyn Tully\"\n\n[[18]]$aliases[[2]]\n[1] \"Lady Stoneheart\"\n\n[[18]]$aliases[[3]]\n[1] \"The Silent Sistet\"\n\n[[18]]$aliases[[4]]\n[1] \"Mother Mercilesr\"\n\n[[18]]$aliases[[5]]\n[1] \"The Hangwomans\"\n\n\n[[18]]$father\n[1] \"\"\n\n[[18]]$mother\n[1] \"\"\n\n[[18]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/339\"\n\n[[18]]$allegiances\n[[18]]$allegiances[[1]]\n[1] \"House Stark of Winterfell\"\n\n[[18]]$allegiances[[2]]\n[1] \"House Tully of Riverrun\"\n\n\n[[18]]$books\n[[18]]$books[[1]]\n[1] \"A Feast for Crows\"\n\n[[18]]$books[[2]]\n[1] \"A Dance with Dragons\"\n\n\n[[18]]$povBooks\n[[18]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[18]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[18]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n\n[[18]]$tvSeries\n[[18]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[18]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[18]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n\n[[18]]$playedBy\n[1] \"Michelle Fairley\"\n\n\n[[19]]\n[[19]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/238\"\n\n[[19]]$id\n[1] 238\n\n[[19]]$name\n[1] \"Cersei Lannister\"\n\n[[19]]$gender\n[1] \"Female\"\n\n[[19]]$culture\n[1] \"Westerman\"\n\n[[19]]$born\n[1] \"In 266 AC, at Casterly Rock\"\n\n[[19]]$died\n[1] \"\"\n\n[[19]]$alive\n[1] TRUE\n\n[[19]]$titles\n[[19]]$titles[[1]]\n[1] \"Light of the West\"\n\n[[19]]$titles[[2]]\n[1] \"Queen Dowager\"\n\n[[19]]$titles[[3]]\n[1] \"Protector of the Realm\"\n\n[[19]]$titles[[4]]\n[1] \"Lady of Casterly Rock\"\n\n[[19]]$titles[[5]]\n[1] \"Queen Regent\"\n\n\n[[19]]$aliases\n[1] \"\"\n\n[[19]]$father\n[1] \"\"\n\n[[19]]$mother\n[1] \"\"\n\n[[19]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/901\"\n\n[[19]]$allegiances\n[1] \"House Lannister of Casterly Rock\"\n\n[[19]]$books\n[[19]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[19]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[19]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n\n[[19]]$povBooks\n[[19]]$povBooks[[1]]\n[1] \"A Feast for Crows\"\n\n[[19]]$povBooks[[2]]\n[1] \"A Dance with Dragons\"\n\n\n[[19]]$tvSeries\n[[19]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[19]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[19]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[19]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[19]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[19]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[19]]$playedBy\n[1] \"Lena Headey\"\n\n\n[[20]]\n[[20]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/339\"\n\n[[20]]$id\n[1] 339\n\n[[20]]$name\n[1] \"Eddard Stark\"\n\n[[20]]$gender\n[1] \"Male\"\n\n[[20]]$culture\n[1] \"Northmen\"\n\n[[20]]$born\n[1] \"In 263 AC, at Winterfell\"\n\n[[20]]$died\n[1] \"In 299 AC, at Great Sept of Baelor in King's Landing\"\n\n[[20]]$alive\n[1] FALSE\n\n[[20]]$titles\n[[20]]$titles[[1]]\n[1] \"Lord of Winterfell\"\n\n[[20]]$titles[[2]]\n[1] \"Warden of the North\"\n\n[[20]]$titles[[3]]\n[1] \"Hand of the King\"\n\n[[20]]$titles[[4]]\n[1] \"Protector of the Realm\"\n\n[[20]]$titles[[5]]\n[1] \"Regent\"\n\n\n[[20]]$aliases\n[[20]]$aliases[[1]]\n[1] \"Ned\"\n\n[[20]]$aliases[[2]]\n[1] \"The Ned\"\n\n[[20]]$aliases[[3]]\n[1] \"The Quiet Wolf\"\n\n\n[[20]]$father\n[1] \"\"\n\n[[20]]$mother\n[1] \"\"\n\n[[20]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/232\"\n\n[[20]]$allegiances\n[1] \"House Stark of Winterfell\"\n\n[[20]]$books\n[[20]]$books[[1]]\n[1] \"A Clash of Kings\"\n\n[[20]]$books[[2]]\n[1] \"A Storm of Swords\"\n\n[[20]]$books[[3]]\n[1] \"A Feast for Crows\"\n\n[[20]]$books[[4]]\n[1] \"A Dance with Dragons\"\n\n[[20]]$books[[5]]\n[1] \"The World of Ice and Fire\"\n\n\n[[20]]$povBooks\n[1] \"A Game of Thrones\"\n\n[[20]]$tvSeries\n[[20]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[20]]$tvSeries[[2]]\n[1] \"Season 6\"\n\n\n[[20]]$playedBy\n[[20]]$playedBy[[1]]\n[1] \"Sean Bean\"\n\n[[20]]$playedBy[[2]]\n[1] \"Sebastian Croft\"\n\n[[20]]$playedBy[[3]]\n[1] \"Robert Aramayo\"\n\n\n\n[[21]]\n[[21]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/529\"\n\n[[21]]$id\n[1] 529\n\n[[21]]$name\n[1] \"Jaime Lannister\"\n\n[[21]]$gender\n[1] \"Male\"\n\n[[21]]$culture\n[1] \"Westerlands\"\n\n[[21]]$born\n[1] \"In 266 AC, at Casterly Rock\"\n\n[[21]]$died\n[1] \"\"\n\n[[21]]$alive\n[1] TRUE\n\n[[21]]$titles\n[[21]]$titles[[1]]\n[1] \"Ser\"\n\n[[21]]$titles[[2]]\n[1] \"Lord Commander of the Kingsguard\"\n\n[[21]]$titles[[3]]\n[1] \"Warden of the East (formerly)\"\n\n\n[[21]]$aliases\n[[21]]$aliases[[1]]\n[1] \"The Kingslayer\"\n\n[[21]]$aliases[[2]]\n[1] \"The Lion of Lannister\"\n\n[[21]]$aliases[[3]]\n[1] \"The Young Lion\"\n\n[[21]]$aliases[[4]]\n[1] \"Cripple\"\n\n\n[[21]]$father\n[1] \"\"\n\n[[21]]$mother\n[1] \"\"\n\n[[21]]$spouse\n[1] \"\"\n\n[[21]]$allegiances\n[1] \"House Lannister of Casterly Rock\"\n\n[[21]]$books\n[[21]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[21]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n\n[[21]]$povBooks\n[[21]]$povBooks[[1]]\n[1] \"A Storm of Swords\"\n\n[[21]]$povBooks[[2]]\n[1] \"A Feast for Crows\"\n\n[[21]]$povBooks[[3]]\n[1] \"A Dance with Dragons\"\n\n\n[[21]]$tvSeries\n[[21]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[21]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[21]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[21]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[21]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n\n[[21]]$playedBy\n[1] \"Nikolaj Coster-Waldau\"\n\n\n[[22]]\n[[22]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/576\"\n\n[[22]]$id\n[1] 576\n\n[[22]]$name\n[1] \"Jon Connington\"\n\n[[22]]$gender\n[1] \"Male\"\n\n[[22]]$culture\n[1] \"Stormlands\"\n\n[[22]]$born\n[1] \"In or between 263 AC and 265 AC\"\n\n[[22]]$died\n[1] \"\"\n\n[[22]]$alive\n[1] TRUE\n\n[[22]]$titles\n[[22]]$titles[[1]]\n[1] \"Lord of Griffin's Roost\"\n\n[[22]]$titles[[2]]\n[1] \"Hand of the King\"\n\n[[22]]$titles[[3]]\n[1] \"Hand of the True King\"\n\n\n[[22]]$aliases\n[1] \"Griffthe Mad King's Hand\"\n\n[[22]]$father\n[1] \"\"\n\n[[22]]$mother\n[1] \"\"\n\n[[22]]$spouse\n[1] \"\"\n\n[[22]]$allegiances\n[[22]]$allegiances[[1]]\n[1] \"House Connington of Griffin's Roost\"\n\n[[22]]$allegiances[[2]]\n[1] \"House Targaryen of King's Landing\"\n\n\n[[22]]$books\n[[22]]$books[[1]]\n[1] \"A Storm of Swords\"\n\n[[22]]$books[[2]]\n[1] \"A Feast for Crows\"\n\n[[22]]$books[[3]]\n[1] \"The World of Ice and Fire\"\n\n\n[[22]]$povBooks\n[1] \"A Dance with Dragons\"\n\n[[22]]$tvSeries\n[1] \"\"\n\n[[22]]$playedBy\n[1] \"\"\n\n\n[[23]]\n[[23]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/583\"\n\n[[23]]$id\n[1] 583\n\n[[23]]$name\n[1] \"Jon Snow\"\n\n[[23]]$gender\n[1] \"Male\"\n\n[[23]]$culture\n[1] \"Northmen\"\n\n[[23]]$born\n[1] \"In 283 AC\"\n\n[[23]]$died\n[1] \"\"\n\n[[23]]$alive\n[1] TRUE\n\n[[23]]$titles\n[1] \"Lord Commander of the Night's Watch\"\n\n[[23]]$aliases\n[[23]]$aliases[[1]]\n[1] \"Lord Snow\"\n\n[[23]]$aliases[[2]]\n[1] \"Ned Stark's Bastard\"\n\n[[23]]$aliases[[3]]\n[1] \"The Snow of Winterfell\"\n\n[[23]]$aliases[[4]]\n[1] \"The Crow-Come-Over\"\n\n[[23]]$aliases[[5]]\n[1] \"The 998th Lord Commander of the Night's Watch\"\n\n[[23]]$aliases[[6]]\n[1] \"The Bastard of Winterfell\"\n\n[[23]]$aliases[[7]]\n[1] \"The Black Bastard of the Wall\"\n\n[[23]]$aliases[[8]]\n[1] \"Lord Crow\"\n\n\n[[23]]$father\n[1] \"\"\n\n[[23]]$mother\n[1] \"\"\n\n[[23]]$spouse\n[1] \"\"\n\n[[23]]$allegiances\n[1] \"House Stark of Winterfell\"\n\n[[23]]$books\n[1] \"A Feast for Crows\"\n\n[[23]]$povBooks\n[[23]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[23]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[23]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n[[23]]$povBooks[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[23]]$tvSeries\n[[23]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[23]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[23]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[23]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[23]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[23]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[23]]$playedBy\n[1] \"Kit Harington\"\n\n\n[[24]]\n[[24]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/60\"\n\n[[24]]$id\n[1] 60\n\n[[24]]$name\n[1] \"Aeron Greyjoy\"\n\n[[24]]$gender\n[1] \"Male\"\n\n[[24]]$culture\n[1] \"Ironborn\"\n\n[[24]]$born\n[1] \"In or between 269 AC and 273 AC, at Pyke\"\n\n[[24]]$died\n[1] \"\"\n\n[[24]]$alive\n[1] TRUE\n\n[[24]]$titles\n[[24]]$titles[[1]]\n[1] \"Priest of the Drowned God\"\n\n[[24]]$titles[[2]]\n[1] \"Captain of the Golden Storm (formerly)\"\n\n\n[[24]]$aliases\n[[24]]$aliases[[1]]\n[1] \"The Damphair\"\n\n[[24]]$aliases[[2]]\n[1] \"Aeron Damphair\"\n\n\n[[24]]$father\n[1] \"\"\n\n[[24]]$mother\n[1] \"\"\n\n[[24]]$spouse\n[1] \"\"\n\n[[24]]$allegiances\n[1] \"House Greyjoy of Pyke\"\n\n[[24]]$books\n[[24]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[24]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[24]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n[[24]]$books[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[24]]$povBooks\n[1] \"A Feast for Crows\"\n\n[[24]]$tvSeries\n[1] \"Season 6\"\n\n[[24]]$playedBy\n[1] \"Michael Feast\"\n\n\n[[25]]\n[[25]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/605\"\n\n[[25]]$id\n[1] 605\n\n[[25]]$name\n[1] \"Kevan Lannister\"\n\n[[25]]$gender\n[1] \"Male\"\n\n[[25]]$culture\n[1] \"\"\n\n[[25]]$born\n[1] \"In 244 AC\"\n\n[[25]]$died\n[1] \"In 300 AC, at King's Landing\"\n\n[[25]]$alive\n[1] FALSE\n\n[[25]]$titles\n[[25]]$titles[[1]]\n[1] \"Ser\"\n\n[[25]]$titles[[2]]\n[1] \"Master of laws\"\n\n[[25]]$titles[[3]]\n[1] \"Lord Regent\"\n\n[[25]]$titles[[4]]\n[1] \"Protector of the Realm\"\n\n\n[[25]]$aliases\n[1] \"\"\n\n[[25]]$father\n[1] \"\"\n\n[[25]]$mother\n[1] \"\"\n\n[[25]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/327\"\n\n[[25]]$allegiances\n[1] \"House Lannister of Casterly Rock\"\n\n[[25]]$books\n[[25]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[25]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[25]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n[[25]]$books[[4]]\n[1] \"A Feast for Crows\"\n\n\n[[25]]$povBooks\n[1] \"A Dance with Dragons\"\n\n[[25]]$tvSeries\n[[25]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[25]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[25]]$tvSeries[[3]]\n[1] \"Season 5\"\n\n[[25]]$tvSeries[[4]]\n[1] \"Season 6\"\n\n\n[[25]]$playedBy\n[1] \"Ian Gelder\"\n\n\n[[26]]\n[[26]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/743\"\n\n[[26]]$id\n[1] 743\n\n[[26]]$name\n[1] \"Melisandre\"\n\n[[26]]$gender\n[1] \"Female\"\n\n[[26]]$culture\n[1] \"Asshai\"\n\n[[26]]$born\n[1] \"At Unknown\"\n\n[[26]]$died\n[1] \"\"\n\n[[26]]$alive\n[1] TRUE\n\n[[26]]$titles\n[1] \"\"\n\n[[26]]$aliases\n[[26]]$aliases[[1]]\n[1] \"The Red Priestess\"\n\n[[26]]$aliases[[2]]\n[1] \"The Red Woman\"\n\n[[26]]$aliases[[3]]\n[1] \"The King's Red Shadow\"\n\n[[26]]$aliases[[4]]\n[1] \"Lady Red\"\n\n[[26]]$aliases[[5]]\n[1] \"Lot Seven\"\n\n\n[[26]]$father\n[1] \"\"\n\n[[26]]$mother\n[1] \"\"\n\n[[26]]$spouse\n[1] \"\"\n\n[[26]]$allegiances\nlist()\n\n[[26]]$books\n[[26]]$books[[1]]\n[1] \"A Clash of Kings\"\n\n[[26]]$books[[2]]\n[1] \"A Storm of Swords\"\n\n[[26]]$books[[3]]\n[1] \"A Feast for Crows\"\n\n\n[[26]]$povBooks\n[1] \"A Dance with Dragons\"\n\n[[26]]$tvSeries\n[[26]]$tvSeries[[1]]\n[1] \"Season 2\"\n\n[[26]]$tvSeries[[2]]\n[1] \"Season 3\"\n\n[[26]]$tvSeries[[3]]\n[1] \"Season 4\"\n\n[[26]]$tvSeries[[4]]\n[1] \"Season 5\"\n\n[[26]]$tvSeries[[5]]\n[1] \"Season 6\"\n\n\n[[26]]$playedBy\n[1] \"Carice van Houten\"\n\n\n[[27]]\n[[27]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/751\"\n\n[[27]]$id\n[1] 751\n\n[[27]]$name\n[1] \"Merrett Frey\"\n\n[[27]]$gender\n[1] \"Male\"\n\n[[27]]$culture\n[1] \"Rivermen\"\n\n[[27]]$born\n[1] \"In 262 AC\"\n\n[[27]]$died\n[1] \"In 300 AC, at Near Oldstones\"\n\n[[27]]$alive\n[1] FALSE\n\n[[27]]$titles\n[1] \"\"\n\n[[27]]$aliases\n[1] \"Merrett Muttonhead\"\n\n[[27]]$father\n[1] \"\"\n\n[[27]]$mother\n[1] \"\"\n\n[[27]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/712\"\n\n[[27]]$allegiances\n[1] \"House Frey of the Crossing\"\n\n[[27]]$books\n[[27]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[27]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[27]]$books[[3]]\n[1] \"A Feast for Crows\"\n\n[[27]]$books[[4]]\n[1] \"A Dance with Dragons\"\n\n\n[[27]]$povBooks\n[1] \"A Storm of Swords\"\n\n[[27]]$tvSeries\n[1] \"\"\n\n[[27]]$playedBy\n[1] \"\"\n\n\n[[28]]\n[[28]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/844\"\n\n[[28]]$id\n[1] 844\n\n[[28]]$name\n[1] \"Quentyn Martell\"\n\n[[28]]$gender\n[1] \"Male\"\n\n[[28]]$culture\n[1] \"Dornish\"\n\n[[28]]$born\n[1] \"In 281 AC, at Sunspear, Dorne\"\n\n[[28]]$died\n[1] \"In 300 AC, at Meereen\"\n\n[[28]]$alive\n[1] FALSE\n\n[[28]]$titles\n[1] \"Prince\"\n\n[[28]]$aliases\n[[28]]$aliases[[1]]\n[1] \"Frog\"\n\n[[28]]$aliases[[2]]\n[1] \"Prince Frog\"\n\n[[28]]$aliases[[3]]\n[1] \"The prince who came too late\"\n\n[[28]]$aliases[[4]]\n[1] \"The Dragonrider\"\n\n\n[[28]]$father\n[1] \"\"\n\n[[28]]$mother\n[1] \"\"\n\n[[28]]$spouse\n[1] \"\"\n\n[[28]]$allegiances\n[1] \"House Nymeros Martell of Sunspear\"\n\n[[28]]$books\n[[28]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[28]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[28]]$books[[3]]\n[1] \"A Storm of Swords\"\n\n[[28]]$books[[4]]\n[1] \"A Feast for Crows\"\n\n\n[[28]]$povBooks\n[1] \"A Dance with Dragons\"\n\n[[28]]$tvSeries\n[1] \"\"\n\n[[28]]$playedBy\n[1] \"\"\n\n\n[[29]]\n[[29]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/954\"\n\n[[29]]$id\n[1] 954\n\n[[29]]$name\n[1] \"Samwell Tarly\"\n\n[[29]]$gender\n[1] \"Male\"\n\n[[29]]$culture\n[1] \"Andal\"\n\n[[29]]$born\n[1] \"In 283 AC, at Horn Hill\"\n\n[[29]]$died\n[1] \"\"\n\n[[29]]$alive\n[1] TRUE\n\n[[29]]$titles\n[1] \"\"\n\n[[29]]$aliases\n[[29]]$aliases[[1]]\n[1] \"Sam\"\n\n[[29]]$aliases[[2]]\n[1] \"Ser Piggy\"\n\n[[29]]$aliases[[3]]\n[1] \"Prince Pork-chop\"\n\n[[29]]$aliases[[4]]\n[1] \"Lady Piggy\"\n\n[[29]]$aliases[[5]]\n[1] \"Sam the Slayer\"\n\n[[29]]$aliases[[6]]\n[1] \"Black Sam\"\n\n[[29]]$aliases[[7]]\n[1] \"Lord of Ham\"\n\n\n[[29]]$father\n[1] \"\"\n\n[[29]]$mother\n[1] \"\"\n\n[[29]]$spouse\n[1] \"\"\n\n[[29]]$allegiances\n[1] \"House Tarly of Horn Hill\"\n\n[[29]]$books\n[[29]]$books[[1]]\n[1] \"A Game of Thrones\"\n\n[[29]]$books[[2]]\n[1] \"A Clash of Kings\"\n\n[[29]]$books[[3]]\n[1] \"A Dance with Dragons\"\n\n\n[[29]]$povBooks\n[[29]]$povBooks[[1]]\n[1] \"A Storm of Swords\"\n\n[[29]]$povBooks[[2]]\n[1] \"A Feast for Crows\"\n\n\n[[29]]$tvSeries\n[[29]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[29]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[29]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[29]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[29]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[29]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[29]]$playedBy\n[1] \"John Bradley-West\"\n\n\n[[30]]\n[[30]]$url\n[1] \"https://www.anapioficeandfire.com/api/characters/957\"\n\n[[30]]$id\n[1] 957\n\n[[30]]$name\n[1] \"Sansa Stark\"\n\n[[30]]$gender\n[1] \"Female\"\n\n[[30]]$culture\n[1] \"Northmen\"\n\n[[30]]$born\n[1] \"In 286 AC, at Winterfell\"\n\n[[30]]$died\n[1] \"\"\n\n[[30]]$alive\n[1] TRUE\n\n[[30]]$titles\n[1] \"Princess\"\n\n[[30]]$aliases\n[[30]]$aliases[[1]]\n[1] \"Little bird\"\n\n[[30]]$aliases[[2]]\n[1] \"Alayne Stone\"\n\n[[30]]$aliases[[3]]\n[1] \"Jonquil\"\n\n\n[[30]]$father\n[1] \"\"\n\n[[30]]$mother\n[1] \"\"\n\n[[30]]$spouse\n[1] \"https://www.anapioficeandfire.com/api/characters/1052\"\n\n[[30]]$allegiances\n[[30]]$allegiances[[1]]\n[1] \"House Baelish of Harrenhal\"\n\n[[30]]$allegiances[[2]]\n[1] \"House Stark of Winterfell\"\n\n\n[[30]]$books\n[1] \"A Dance with Dragons\"\n\n[[30]]$povBooks\n[[30]]$povBooks[[1]]\n[1] \"A Game of Thrones\"\n\n[[30]]$povBooks[[2]]\n[1] \"A Clash of Kings\"\n\n[[30]]$povBooks[[3]]\n[1] \"A Storm of Swords\"\n\n[[30]]$povBooks[[4]]\n[1] \"A Feast for Crows\"\n\n\n[[30]]$tvSeries\n[[30]]$tvSeries[[1]]\n[1] \"Season 1\"\n\n[[30]]$tvSeries[[2]]\n[1] \"Season 2\"\n\n[[30]]$tvSeries[[3]]\n[1] \"Season 3\"\n\n[[30]]$tvSeries[[4]]\n[1] \"Season 4\"\n\n[[30]]$tvSeries[[5]]\n[1] \"Season 5\"\n\n[[30]]$tvSeries[[6]]\n[1] \"Season 6\"\n\n\n[[30]]$playedBy\n[1] \"Sophie Turner\""
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#json-databases",
    "href": "meetups/Meetup9/Meetup9.html#json-databases",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "JSON Databases",
    "text": "JSON Databases\n\nJSON databases are an increasingly popular alternative to the relational database model based around SQL\nMongo, NoSQL\nDo not require database schemas\nVery flexible and scalable\nDownsides:\n\nNo Joins\nNo ACID (Atomicity, Consistency, Isolation, Durability)"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#webscraping",
    "href": "meetups/Meetup9/Meetup9.html#webscraping",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Webscraping",
    "text": "Webscraping\n\nWebscraping is the process of extracting data from websites\nWe will use rvest package which pulls HTML from websites and lets you process it\nWebscraping can be very messy, powerful but maybe a last resort\nAlternative: APIs"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#html",
    "href": "meetups/Meetup9/Meetup9.html#html",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "HTML",
    "text": "HTML\n\nWebpages are usually HTML files:\n\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Page title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1 id='first'&gt;A heading&lt;/h1&gt;\n  &lt;p&gt;Some text &amp; &lt;b&gt;some bold text.&lt;/b&gt;&lt;/p&gt;\n  &lt;img src='myimg.png' width='100' height='100'&gt;\n&lt;/body&gt;\n\nNested “tags”: body, head, h1, p, b\nAttributes: id, src, width, height"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#ethics-ok-cupid-case-study",
    "href": "meetups/Meetup9/Meetup9.html#ethics-ok-cupid-case-study",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Ethics: OK Cupid Case Study",
    "text": "Ethics: OK Cupid Case Study\n\nResearchers in Denmark scraped dating site OkCupid and published a dataset\n\n\nvox"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#ethics-ok-cupid-case-study-1",
    "href": "meetups/Meetup9/Meetup9.html#ethics-ok-cupid-case-study-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Ethics: OK Cupid Case Study",
    "text": "Ethics: OK Cupid Case Study\n\nResearchers in Denmark scraped dating site OkCupid and published a dataset\n\n\nvox"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#ethics-ok-cupid-case-study-2",
    "href": "meetups/Meetup9/Meetup9.html#ethics-ok-cupid-case-study-2",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Ethics: OK Cupid Case Study",
    "text": "Ethics: OK Cupid Case Study\n\nResearchers in Denmark scraped dating site OkCupid and published a dataset\n\n\nvox"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#cambridge-analytica",
    "href": "meetups/Meetup9/Meetup9.html#cambridge-analytica",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Cambridge Analytica",
    "text": "Cambridge Analytica\n\nCambridge Analytica harvested data from FB users using questionaires/quizzes\nBuilt psychological profiles of users and their friends\nUsed data for targetting in political campaigns\n\nResulted in Cambridge Analytica bankruptcy\nFB paid billions in fines, lost market cap"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#cambridge-analytica-1",
    "href": "meetups/Meetup9/Meetup9.html#cambridge-analytica-1",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Cambridge Analytica",
    "text": "Cambridge Analytica\n\nftc"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#guidelines",
    "href": "meetups/Meetup9/Meetup9.html#guidelines",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Guidelines:",
    "text": "Guidelines:\n\nJust because you can, doesn’t mean you should\nRed Flags:\n\nPersonal Data\nData you think is only available accidentally\n\nSocial Science Research Study Ethics:\n\nDid participants consent?\nIs privacy maintained?\nIs there the potential for harm?"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#robotstxt",
    "href": "meetups/Meetup9/Meetup9.html#robotstxt",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "robotstxt",
    "text": "robotstxt\n\nHow to know if you can scrape a website?\n\n\nlibrary(robotstxt)\npaths_allowed(\"http://www.facebook.com\")\n\n[1] FALSE\n\npaths_allowed(\"http://www.wikipedia.com\")\n\n[1] TRUE"
  },
  {
    "objectID": "meetups/Meetup9/Meetup9.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup9/Meetup9.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 9: Hierarchical Data and Webscraping",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#relational-databases",
    "href": "meetups/Meetup7/SQLSlides.html#relational-databases",
    "title": "SQL Slides",
    "section": "Relational Databases",
    "text": "Relational Databases\n\nDatabase is a collection of related tables\nDatabase schema defines the structure/relationships"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#features",
    "href": "meetups/Meetup7/SQLSlides.html#features",
    "title": "SQL Slides",
    "section": "Features",
    "text": "Features\n\nTables linked by keys\n\nCommon to create a separate unique index/id-number to serves as primary key\n\nTables correspond to entities\n\nEach table about 1 “thing”\n\nRows called records"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd",
    "href": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd",
    "title": "SQL Slides",
    "section": "Entity Relationship Diagram (ERD)",
    "text": "Entity Relationship Diagram (ERD)\n\nTeate SQL4DS"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd-1",
    "href": "meetups/Meetup7/SQLSlides.html#entity-relationship-diagram-erd-1",
    "title": "SQL Slides",
    "section": "Entity Relationship Diagram (ERD)",
    "text": "Entity Relationship Diagram (ERD)\n\nTeate SQL4DS\nStars mark primary and foreign keys\n1 and Infinity symbol indicate one-many relationship"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#more-complex-erd",
    "href": "meetups/Meetup7/SQLSlides.html#more-complex-erd",
    "title": "SQL Slides",
    "section": "More Complex ERD",
    "text": "More Complex ERD\n\nTeate SQL4DS\nHere authors and books have a many-many relationship\nAssociative Table used as a junction Authors and Books\nCombo of ISBN and Author ID are primary key for Books-Authors"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#normalization-schemes",
    "href": "meetups/Meetup7/SQLSlides.html#normalization-schemes",
    "title": "SQL Slides",
    "section": "Normalization Schemes",
    "text": "Normalization Schemes\n\nDatabases often organized according to strict rules called normalization\nThese improve things like space efficiency, data consistency, etc\nMassive research topic in the 1970s"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#normalization-schemes-1",
    "href": "meetups/Meetup7/SQLSlides.html#normalization-schemes-1",
    "title": "SQL Slides",
    "section": "Normalization Schemes",
    "text": "Normalization Schemes\n\nDatabases often organized according to strict rules called normalization\nThese improve things like space efficiency, data consistency, etc\nMassive research topic in the 1970s\nIdeas often involve separating data into tables corresponding to entities\nEach fact stored in one place\nCan make a career as a database designer/architect"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#trade-offs",
    "href": "meetups/Meetup7/SQLSlides.html#trade-offs",
    "title": "SQL Slides",
    "section": "Trade Offs",
    "text": "Trade Offs\nWhen should you use a relational database instead of regular file(s) for your project?\n\nIf many people are using the data\nIf the data takes up lots of space\nIf the data has complex organization\nIf you are planning to scale up\n\nBut it is likely that your organization will be using it and so knowing how to interact with databases is key, will make you more efficient and more independent!"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#dbms-classes",
    "href": "meetups/Meetup7/SQLSlides.html#dbms-classes",
    "title": "SQL Slides",
    "section": "DBMS Classes",
    "text": "DBMS Classes\nUse Database Management Systems to access data/interact with databases\n\nClient-Server: (Most Traditional). Database hosted on a central server to which you connect (IBM, Oracle, MySQL server)\nCloud: Database hosted on cloud. Newer, easier to scale resources (Google, Amazon, Snowflake)\nIn Process: For smaller datasets with few users. Best for data analysis and learning (sqlite, duckdb)"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#structured-query-language-sql",
    "href": "meetups/Meetup7/SQLSlides.html#structured-query-language-sql",
    "title": "SQL Slides",
    "section": "Structured Query Language (SQL)",
    "text": "Structured Query Language (SQL)\n\nStored on disk, queried to generate smaller dataset for analysis elsewhere\nQueries composed of clauses (must be in order):\n\nSELECT, FROM, WHERE, GROUP BY, ORDER BY\nSELECT is combo of mutate, select, rename, summarize, relocate, summarize\nAlso has JOINS\n\ndbplyr translates tidyverse manipulations to SQL\nSQL is a standard with many flavors"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#connecting-to-a-database-in-r",
    "href": "meetups/Meetup7/SQLSlides.html#connecting-to-a-database-in-r",
    "title": "SQL Slides",
    "section": "Connecting to a Database in R",
    "text": "Connecting to a Database in R\n\nDBI library has functions to manipulate databases\n\n\nlibrary(DBI)\nlibrary(duckdb)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"duckdb\")\n\n\ndbConnect() connections and initializes an empty database\nduckdb() creates an instance of a duck database\nCan use many options with DBI"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#writing-tables",
    "href": "meetups/Meetup7/SQLSlides.html#writing-tables",
    "title": "SQL Slides",
    "section": "Writing Tables",
    "text": "Writing Tables\n\ndbWriteTable writes data from R to your database\n\n\nlibrary(nycflights13)\n\ndbWriteTable(con, \"flights\", flights,overwrite=TRUE)\ndbWriteTable(con, \"planes\", planes,overwrite=TRUE)\n\ndbListTables(con)\n\n[1] \"flights\" \"planes\""
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#reading-tables",
    "href": "meetups/Meetup7/SQLSlides.html#reading-tables",
    "title": "SQL Slides",
    "section": "Reading Tables",
    "text": "Reading Tables\n\ndbReadTable reads data from your database to R\n\n\ncon |&gt; dbReadTable(\"flights\") |&gt; \n  as_tibble() |&gt; select(year:arr_time)\n\n# A tibble: 336,776 × 7\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# ℹ 336,766 more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#queries",
    "href": "meetups/Meetup7/SQLSlides.html#queries",
    "title": "SQL Slides",
    "section": "Queries",
    "text": "Queries\n\ndbGetQuery runs SQL queries\n\n\nquery = \"\nSELECT dep_delay, month, sched_dep_time\nFROM flights\nWHERE carrier = 'UA'\n\"\n\ncon |&gt; dbGetQuery(query) |&gt; as_tibble()\n\n# A tibble: 58,665 × 3\n   dep_delay month sched_dep_time\n       &lt;dbl&gt; &lt;int&gt;          &lt;int&gt;\n 1         2     1            515\n 2         4     1            529\n 3        -4     1            558\n 4        -2     1            600\n 5        -2     1            600\n 6        -1     1            600\n 7         0     1            607\n 8        11     1            600\n 9        -4     1            627\n10        -2     1            630\n# ℹ 58,655 more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#dbplyr",
    "href": "meetups/Meetup7/SQLSlides.html#dbplyr",
    "title": "SQL Slides",
    "section": "dbplyr",
    "text": "dbplyr\n\nlibrary that translates dplyr to SQL\nUse it both to run queries and learn SQL\n\n\nlibrary(dbplyr)\nflights_tab = con |&gt; tbl(\"flights\")\nplanes_tab = con |&gt; tbl(\"planes\") |&gt; print(n=3)\n\n# Source:   table&lt;planes&gt; [?? x 9]\n# Database: DuckDB 1.4.0 [georgehagstrom@Linux 6.12.10-76061203-generic:R 4.5.1//home/georgehagstrom/work/Teaching/DATA607Fall2025/website/meetups/Meetup7/duckdb]\n  tailnum  year type               manufacturer model engines seats speed engine\n  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; \n1 N10156   2004 Fixed wing multi … EMBRAER      EMB-…       2    55    NA Turbo…\n2 N102UW   1998 Fixed wing multi … AIRBUS INDU… A320…       2   182    NA Turbo…\n3 N103US   1999 Fixed wing multi … AIRBUS INDU… A320…       2   182    NA Turbo…\n# ℹ more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#lazy-evaluation",
    "href": "meetups/Meetup7/SQLSlides.html#lazy-evaluation",
    "title": "SQL Slides",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\ndbplyr doesn’t immediately execute code, builds up big query instead\nshow_query() to see it\n\n\nflights_tab |&gt; inner_join(planes_tab) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(num_flights = n()) |&gt; \n  arrange(desc(num_flights)) |&gt; print(n=4)\n\n# Source:     SQL [?? x 2]\n# Database:   DuckDB 1.4.0 [georgehagstrom@Linux 6.12.10-76061203-generic:R 4.5.1//home/georgehagstrom/work/Teaching/DATA607Fall2025/website/meetups/Meetup7/duckdb]\n# Ordered by: desc(num_flights)\n  tailnum num_flights\n  &lt;chr&gt;         &lt;dbl&gt;\n1 N354JB          333\n2 N355JB          282\n3 N358JB          271\n4 N374JB          236\n# ℹ more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-1",
    "href": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-1",
    "title": "SQL Slides",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\ndbplyr doesn’t immediately execute code, builds up big query instead\nshow_query() to see it\n\n\nflights_tab |&gt; inner_join(planes_tab) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(num_flights = n()) |&gt; \n  arrange(desc(num_flights)) |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, COUNT(*) AS num_flights\nFROM (\n  SELECT flights.*, \"type\", manufacturer, model, engines, seats, speed, engine\n  FROM flights\n  INNER JOIN planes\n    ON (flights.\"year\" = planes.\"year\" AND flights.tailnum = planes.tailnum)\n) q01\nGROUP BY tailnum\nORDER BY num_flights DESC"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-2",
    "href": "meetups/Meetup7/SQLSlides.html#lazy-evaluation-2",
    "title": "SQL Slides",
    "section": "Lazy Evaluation",
    "text": "Lazy Evaluation\n\ndbplyr doesn’t immediately execute code, builds up big query instead\ncollect to execute it\n\n\nflights_tab |&gt; inner_join(planes_tab) |&gt; \n  group_by(tailnum) |&gt; \n  summarise(num_flights = n()) |&gt; \n  arrange(desc(num_flights)) |&gt; collect()\n\n# A tibble: 92 × 2\n   tailnum num_flights\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 N354JB          333\n 2 N355JB          282\n 3 N358JB          271\n 4 N374JB          236\n 5 N373JB          232\n 6 N368JB          230\n 7 N827JB          125\n 8 N37465          111\n 9 N36469          102\n10 N37468          102\n# ℹ 82 more rows"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#sql-basics",
    "href": "meetups/Meetup7/SQLSlides.html#sql-basics",
    "title": "SQL Slides",
    "section": "SQL Basics",
    "text": "SQL Basics\n\nSQL queries composed of statements in order:\n\nSELECT, FROM, WHERE, GROUP BY, ORDER BY\n\nEvaluation occurs in different order:\nFROM WHERE GROUP BY SELECT ORDER BY\nSELECT FROM:\n\n\n flights_tab |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT *\nFROM flights\n\nflights_tab |&gt; select(carrier, flight, arr_time) |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT carrier, flight, arr_time\nFROM flights"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#sql-basics-1",
    "href": "meetups/Meetup7/SQLSlides.html#sql-basics-1",
    "title": "SQL Slides",
    "section": "SQL Basics",
    "text": "SQL Basics\n\nWHERE is like filter()\nORDER BY is like arrange()\n\n\nflights_tab |&gt; filter(dep_delay &gt; 30, carrier == 'UA') |&gt; \n  arrange(arr_delay) |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT flights.*\nFROM flights\nWHERE (dep_delay &gt; 30.0) AND (carrier = 'UA')\nORDER BY arr_delay"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#select-aggregates",
    "href": "meetups/Meetup7/SQLSlides.html#select-aggregates",
    "title": "SQL Slides",
    "section": "SELECT aggregates",
    "text": "SELECT aggregates\n\nflights_tab |&gt; \n  group_by(carrier) |&gt; \n  summarise(dep_delay = mean(dep_delay,na.rm=TRUE),\n            num_flights = n()) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT carrier, AVG(dep_delay) AS dep_delay, COUNT(*) AS num_flights\nFROM flights\nGROUP BY carrier\n\n\n\nsummarise variables put in select statement"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#select-rename-relocate",
    "href": "meetups/Meetup7/SQLSlides.html#select-rename-relocate",
    "title": "SQL Slides",
    "section": "select, rename, relocate",
    "text": "select, rename, relocate\n\nplanes_tab |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, \"type\", manufacturer, model, \"year\"\nFROM planes\n\nplanes_tab |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  rename(year_built = year) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, \"type\", manufacturer, model, \"year\" AS year_built\nFROM planes\n\nplanes_tab |&gt; \n  select(tailnum, type, manufacturer, model, year) |&gt; \n  relocate(manufacturer, model, .before = type) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT tailnum, manufacturer, model, \"type\", \"year\"\nFROM planes"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#select-and-mutate",
    "href": "meetups/Meetup7/SQLSlides.html#select-and-mutate",
    "title": "SQL Slides",
    "section": "SELECT and mutate",
    "text": "SELECT and mutate\n\nmutate formulas appear as in SELECT statements\n\n\nflights_tab |&gt; \n  mutate(speed = distance/(air_time/60.0)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT flights.*, distance / (air_time / 60.0) AS speed\nFROM flights\n\nflights_tab |&gt; \n  mutate(speed = distance/(air_time/60.0)) |&gt; \n  collect() |&gt; \n  select(origin,dest,time_hour,carrier,flight,speed) |&gt;\n  head(5)\n\n# A tibble: 5 × 6\n  origin dest  time_hour           carrier flight speed\n  &lt;chr&gt;  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 EWR    IAH   2013-01-01 10:00:00 UA        1545  370.\n2 LGA    IAH   2013-01-01 10:00:00 UA        1714  374.\n3 JFK    MIA   2013-01-01 10:00:00 AA        1141  408.\n4 JFK    BQN   2013-01-01 10:00:00 B6         725  517.\n5 LGA    ATL   2013-01-01 11:00:00 DL         461  394."
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#having-clause",
    "href": "meetups/Meetup7/SQLSlides.html#having-clause",
    "title": "SQL Slides",
    "section": "HAVING clause",
    "text": "HAVING clause\n\nIf you filter after summarise, HAVING instead of WHERE\n\n\n\n&lt;SQL&gt;\nSELECT carrier, AVG(dep_delay) AS dep_delay, COUNT(*) AS num_flights\nFROM flights\nGROUP BY carrier\nHAVING (COUNT(*) &gt; 100.0)"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#sub-queries",
    "href": "meetups/Meetup7/SQLSlides.html#sub-queries",
    "title": "SQL Slides",
    "section": "Sub-queries",
    "text": "Sub-queries\n\nSQL uses sub-queries to create sources of data for further queries:\n\n\nflights_tab |&gt; \n  mutate(speed = distance/(air_time/60.0)) |&gt; \n  filter(speed &gt; 450) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT q01.*\nFROM (\n  SELECT flights.*, distance / (air_time / 60.0) AS speed\n  FROM flights\n) q01\nWHERE (speed &gt; 450.0)\n\n\n\nquery name q01 generated by dbplyr\nSaw this in join example earlier"
  },
  {
    "objectID": "meetups/Meetup7/SQLSlides.html#pitfalls",
    "href": "meetups/Meetup7/SQLSlides.html#pitfalls",
    "title": "SQL Slides",
    "section": "Pitfalls",
    "text": "Pitfalls\n\ndbplyr won’t always write nicest code\nIt takes deep knowledge to write performant/fast queries for large databases\n\nIf you need to become a pro in this, read use the index, Luke\n\nMany many SQL standards"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#week-summary",
    "href": "meetups/Meetup2/Meetup2.html#week-summary",
    "title": "Meetup 2: Visualizing Data",
    "section": "Week Summary",
    "text": "Week Summary\n\n1. Read Chapters 1-4 of R4DS (Work out the problems as you go!)\n\n2. Supplementary Readings- Data Viz 1-5 (Wilke), Calling BS (West and Bergstrom) Proportional Ink and Misleading Axes\n\n3. Lab 1: Airbnb in NYC- submit qmd file and pdf on Brightspace by Sunday Midnight\n\n4. Data Science in Context- sign up if you haven’t yet\n\n5. If you don’t have a working software setup speak up in Slack!!!"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#why-make-graphs",
    "href": "meetups/Meetup2/Meetup2.html#why-make-graphs",
    "title": "Meetup 2: Visualizing Data",
    "section": "Why Make Graphs?",
    "text": "Why Make Graphs?\n\nGraphs are the main way we communicate data\nAlternative would be summary statistics, but visualizations are much richer.\nBig debate within statistics community because summary stats perceived as more “rigorous”\nGraphs won the debate- they are the main and most important thing people will remember about your data analyses"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#anscombes-quartet",
    "href": "meetups/Meetup2/Meetup2.html#anscombes-quartet",
    "title": "Meetup 2: Visualizing Data",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\n\n\nSet 1:\n\n\n\n\n\nx\ny\n\n\n\n\n10\n8.04\n\n\n8\n6.95\n\n\n13\n7.58\n\n\n9\n8.81\n\n\n11\n8.33\n\n\n14\n9.96\n\n\n6\n7.24\n\n\n4\n4.26\n\n\n12\n10.84\n\n\n7\n4.82\n\n\n5\n5.68\n\n\n\n\n\n\nSet 2:\n\n\n\n\n\nx\ny\n\n\n\n\n10\n9.14\n\n\n8\n8.14\n\n\n13\n8.74\n\n\n9\n8.77\n\n\n11\n9.26\n\n\n14\n8.10\n\n\n6\n6.13\n\n\n4\n3.10\n\n\n12\n9.13\n\n\n7\n7.26\n\n\n5\n4.74\n\n\n\n\n\n\nSet 3:\n\n\n\n\n\nx\ny\n\n\n\n\n10\n7.46\n\n\n8\n6.77\n\n\n13\n12.74\n\n\n9\n7.11\n\n\n11\n7.81\n\n\n14\n8.84\n\n\n6\n6.08\n\n\n4\n5.39\n\n\n12\n8.15\n\n\n7\n6.42\n\n\n5\n5.73\n\n\n\n\n\n\nSet 4:\n\n\n\n\n\nx\ny\n\n\n\n\n8\n6.58\n\n\n8\n5.76\n\n\n8\n7.71\n\n\n8\n8.84\n\n\n8\n8.47\n\n\n8\n7.04\n\n\n8\n5.25\n\n\n19\n12.50\n\n\n8\n5.56\n\n\n8\n7.91\n\n\n8\n6.89"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#anscombe-summary-stats",
    "href": "meetups/Meetup2/Meetup2.html#anscombe-summary-stats",
    "title": "Meetup 2: Visualizing Data",
    "section": "Anscombe Summary Stats:",
    "text": "Anscombe Summary Stats:\n\n\n\n\n\nSet\nmean(x)\nmean(y)\nsd(x)\nsd(y)\ncor(x,y)\n\n\n\n\n1\n9\n7.500909\n3.316625\n2.031568\n0.8164205\n\n\n2\n9\n7.500909\n3.316625\n2.031657\n0.8162365\n\n\n3\n9\n7.500000\n3.316625\n2.030424\n0.8162867\n\n\n4\n9\n7.500909\n3.316625\n2.030578\n0.8165214\n\n\n\n\n\nAre the datasets the basically the same?"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#anscombe-visualized",
    "href": "meetups/Meetup2/Meetup2.html#anscombe-visualized",
    "title": "Meetup 2: Visualizing Data",
    "section": "Anscombe Visualized:",
    "text": "Anscombe Visualized:"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#d-example",
    "href": "meetups/Meetup2/Meetup2.html#d-example",
    "title": "Meetup 2: Visualizing Data",
    "section": "1D Example",
    "text": "1D Example"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#datasaurus-dozen",
    "href": "meetups/Meetup2/Meetup2.html#datasaurus-dozen",
    "title": "Meetup 2: Visualizing Data",
    "section": "Datasaurus Dozen",
    "text": "Datasaurus Dozen"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#non-contrived-example",
    "href": "meetups/Meetup2/Meetup2.html#non-contrived-example",
    "title": "Meetup 2: Visualizing Data",
    "section": "Non-Contrived Example",
    "text": "Non-Contrived Example\n\n\nTable shows the results of a linear regression between income inequality and voter turnout, claiming to show a statistically significant negative correlation (Jackman 1980)"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#outlier-caused-relationship",
    "href": "meetups/Meetup2/Meetup2.html#outlier-caused-relationship",
    "title": "Meetup 2: Visualizing Data",
    "section": "Outlier caused relationship",
    "text": "Outlier caused relationship"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#graphs-as-comparisons",
    "href": "meetups/Meetup2/Meetup2.html#graphs-as-comparisons",
    "title": "Meetup 2: Visualizing Data",
    "section": "Graphs as Comparisons",
    "text": "Graphs as Comparisons\nTwo purposes for graphs:\n\nUnderstand and communicate the size and direction of comparisons that were already of interest\nDiscover new patterns\n\nA graph is often all someone will remember about your data analysis, can make the difference between convincing important decision makers or not."
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#improve-this-graph",
    "href": "meetups/Meetup2/Meetup2.html#improve-this-graph",
    "title": "Meetup 2: Visualizing Data",
    "section": "Improve this Graph",
    "text": "Improve this Graph"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#reformulating-the-data",
    "href": "meetups/Meetup2/Meetup2.html#reformulating-the-data",
    "title": "Meetup 2: Visualizing Data",
    "section": "Reformulating the Data",
    "text": "Reformulating the Data\n\nTwo Possible Stories\n\nChildren had higher accuracy than adults\nMultiple objects is harder than an individual object\n\nCaption suggests Adult vs. Child accuracy\n\nSolution:\n\nInstead of plotting responses, plot accuracy\nPlot Accuracy of each group on the same axis"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#case-study-solution-line-plot",
    "href": "meetups/Meetup2/Meetup2.html#case-study-solution-line-plot",
    "title": "Meetup 2: Visualizing Data",
    "section": "Case Study Solution: Line Plot",
    "text": "Case Study Solution: Line Plot"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#ggplot2-basics",
    "href": "meetups/Meetup2/Meetup2.html#ggplot2-basics",
    "title": "Meetup 2: Visualizing Data",
    "section": "ggplot2 Basics",
    "text": "ggplot2 Basics\n\n\n\nggplot2 is tidyverse plotting package\nGrammar of Graphics\nKey figure components\n\ndata maps to aesthetics\ninterpreted using geoms\nothers: theme, facets, coordinates, stats\n\n\n\n Source: DS-Box"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#vignette-transit-costs",
    "href": "meetups/Meetup2/Meetup2.html#vignette-transit-costs",
    "title": "Meetup 2: Visualizing Data",
    "section": "Vignette: Transit Costs",
    "text": "Vignette: Transit Costs\n\nSource tidy tuesday Download Vignette Download Data"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#categories-of-graphs",
    "href": "meetups/Meetup2/Meetup2.html#categories-of-graphs",
    "title": "Meetup 2: Visualizing Data",
    "section": "Categories of Graphs",
    "text": "Categories of Graphs"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#avoiding-wrong",
    "href": "meetups/Meetup2/Meetup2.html#avoiding-wrong",
    "title": "Meetup 2: Visualizing Data",
    "section": "Avoiding Wrong",
    "text": "Avoiding Wrong\nWrong figures are missing information needed to decipher the meaning or contain mathematical errors\n\nLabel all axes and provide units if ambiguous\nAll axes need scales\nAestethic elements accurately represent the data\nLegends and labels present when appropriate to identify meaning of other visual elements"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#ethics-principal-of-proportional-ink",
    "href": "meetups/Meetup2/Meetup2.html#ethics-principal-of-proportional-ink",
    "title": "Meetup 2: Visualizing Data",
    "section": "Ethics: Principal of Proportional Ink",
    "text": "Ethics: Principal of Proportional Ink\n\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the numerical quantities represented.” (1983, p.56) Edward Tufte, The Visual Display of Quantitative Information\n\n\nThis principle avoids many misleading visualizations"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#example",
    "href": "meetups/Meetup2/Meetup2.html#example",
    "title": "Meetup 2: Visualizing Data",
    "section": "Example",
    "text": "Example\n\n\n\n\n\nScale doesn’t start at 0\nWidth expands with height\nDanger in bar plots, filled line plots, plots with elements that scale with data"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#bar-plots-should-start-at-0",
    "href": "meetups/Meetup2/Meetup2.html#bar-plots-should-start-at-0",
    "title": "Meetup 2: Visualizing Data",
    "section": "Bar Plots Should Start at 0",
    "text": "Bar Plots Should Start at 0\n\n\nWrong\n\n\n\n\n\n\n\n\n\nOr make them line charts:\n\nRight"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#filled-plots-should-start-at-0",
    "href": "meetups/Meetup2/Meetup2.html#filled-plots-should-start-at-0",
    "title": "Meetup 2: Visualizing Data",
    "section": "Filled Plots Should Start at 0",
    "text": "Filled Plots Should Start at 0\n\n\nWrong\n\n\n\n\n\n\n\n\n\nOr transform data to represent a net change:\n\nRight"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#scale-area-not-radius",
    "href": "meetups/Meetup2/Meetup2.html#scale-area-not-radius",
    "title": "Meetup 2: Visualizing Data",
    "section": "Scale Area not Radius",
    "text": "Scale Area not Radius"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup2/Meetup2.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 2: Visualizing Data",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup2/Meetup2.html#acknowledgements",
    "href": "meetups/Meetup2/Meetup2.html#acknowledgements",
    "title": "Meetup 2: Visualizing Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nActive Statistics (Gelman, Hill, Vehtari)\nPrinciples of Data Visualization (Wilke)\nCalling BS (Bergstrom and West)\nData Visualization (Healy)\nImpact of Outliers on Income Inequality (Jackman)\n\n\n\n\n\nDATA 607"
  },
  {
    "objectID": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html",
    "href": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html",
    "title": "EPA Code Vignette",
    "section": "",
    "text": "I decided to use httr2 to explore the EPA air quality services API. This API contains a number of difference services which let you download different types of measurements from air quality sensors located around the country. The documentation for this API is located here: https://aqs.epa.gov/aqsweb/documents/data_api.html."
  },
  {
    "objectID": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html#introduction",
    "href": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html#introduction",
    "title": "EPA Code Vignette",
    "section": "",
    "text": "I decided to use httr2 to explore the EPA air quality services API. This API contains a number of difference services which let you download different types of measurements from air quality sensors located around the country. The documentation for this API is located here: https://aqs.epa.gov/aqsweb/documents/data_api.html."
  },
  {
    "objectID": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html#accessing-the-api-to-download-pm2.5-data-from-a-sensor-in-manhattan",
    "href": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html#accessing-the-api-to-download-pm2.5-data-from-a-sensor-in-manhattan",
    "title": "EPA Code Vignette",
    "section": "Accessing the API to Download PM2.5 Data from a Sensor in Manhattan",
    "text": "Accessing the API to Download PM2.5 Data from a Sensor in Manhattan\nThe goal of this code will be to locate a sensor in Manhattan which takes PM2.5 data, download the data, and make a plot of the data.\nI begin by defining some code which I will use to build up requests\n\nlibrary(httr2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(tibblify)\nlibrary(lubridate)\n\nepa_email = \"georgehagstrom@gmail.com\"\n\nepa_url = \"https://aqs.epa.gov/data/api/\"\nepa_key = read_csv(\"/home/georgehagstrom/work/Teaching/DATA607/website/meetups/Meetup10/epa_key.csv\") |&gt; \n  pull(key)\n\nRows: 1 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): key\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# This creates a blank request targetted at the epa_url\n\nepa_req = epa_url |&gt; request() |&gt; \n  req_url_query(\n    email = epa_email\n  )\n\n\n\n# We are going to need to download some lists to figure out how to find things we are interested in\n\nepa_classes_url = \"list/classes\"\n\nepa_counties_url = \"list/countiesByState\"\n\nepa_states_url = \"list/states\"\n\nepa_pars_url = \"list/parametersByClass\"\n\n# Which is New York State?\n\nepa_req_states = epa_req |&gt; \n  req_url_path_append(epa_states_url) |&gt; \n  req_url_query(\n    key = epa_key\n  ) \n\nstates_list = epa_req_states |&gt; req_perform() |&gt; \n  resp_body_json()\n\n\n\nstates_list$Data |&gt; tibblify() |&gt; filter(code == \"New York\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: code &lt;chr&gt;, value_represented &lt;chr&gt;\n\n\nThis code let us determine the numerical code by which New York is identified, which turnes out to be \"36\".\nNext we look at the counties, with the goal of finding New York county:\n\nepa_req_counties = epa_req |&gt; \n  req_url_path_append(epa_counties_url) |&gt; \n  req_url_query(\n    key = epa_key,\n    state = \"36\"\n  )\n\ncounties_list = epa_req_counties |&gt; \n  req_perform() |&gt; \n  resp_body_json()\n\n\ncounties_list$Data |&gt; tibblify() |&gt; filter(code == \"New York\")\n\n# A tibble: 0 × 2\n# ℹ 2 variables: code &lt;chr&gt;, value_represented &lt;chr&gt;\n\n\nNew York county is denoted by the code \"061\".\nThe next step is to identify the parameter code for some PM2.5 measurement. To do this, we first need to explore the list of parameter classes, so we will query the list/classes endpoint:\n\nepa_classes_list = epa_req |&gt; \n  req_url_path_append(epa_classes_url) |&gt; \n  req_url_query(\n    key = epa_key\n  ) |&gt; \n  req_perform() |&gt; \n  resp_body_json()\n\nepa_classes_list$Data |&gt; tibblify() |&gt; print(n=27)\n\n# A tibble: 27 × 2\n   code                    value_represented                                    \n   &lt;chr&gt;                   &lt;chr&gt;                                                \n 1 AIRNOW MAPS             The parameters represented on AirNow maps (88101, 88…\n 2 ALL                     Select all Parameters Available                      \n 3 AQI POLLUTANTS          Pollutants that have an AQI Defined                  \n 4 CORE_HAPS               Urban Air Toxic Pollutants                           \n 5 CRITERIA                Criteria Pollutants                                  \n 6 CSN DART                List of CSN speciation parameters to populate the ST…\n 7 FORECAST                Parameters routinely extracted by AirNow (STI)       \n 8 HAPS                    Hazardous Air Pollutants                             \n 9 IMPROVE CARBON          IMPROVE Carbon Parameters                            \n10 IMPROVE_SPECIATION      PM2.5 Speciated Parameters Measured at IMPROVE sites \n11 MET                     Meteorological Parameters                            \n12 NATTS CORE HAPS         The core list of toxics of interest to the NATTS pro…\n13 NATTS REQUIRED          Required compounds to be collected in the National A…\n14 PAMS                    Photochemical Assessment Monitoring System           \n15 PAMS_VOC                Volatile Organic Compound subset of the PAMS Paramet…\n16 PM COARSE               PM between 2.5 and 10 micrometers                    \n17 PM10 SPECIATION         PM10 Speciated Parameters                            \n18 PM2.5 CONT NONREF       PM2.5 Continuous, Nonreference Methods               \n19 PM2.5 MASS/QA           PM2.5 Mass and QA Parameters                         \n20 SCHOOL AIR TOXICS       School Air Toxics Program Parameters                 \n21 SPECIATION              PM2.5 Speciated Parameters                           \n22 SPECIATION CARBON       PM2.5 Speciation Carbon Parameters                   \n23 SPECIATION CATION/ANION PM2.5 Speciation Cation/Anion Parameters             \n24 SPECIATION METALS       PM2.5 Speciation Metal Parameters                    \n25 UATMP CARBONYL          Urban Air Toxics Monitoring Program Carbonyls        \n26 UATMP VOC               Urban Air Toxics Monitoring Program VOCs             \n27 VOC                     Volatile organic compounds                           \n\n\nWe were able to identify \"PM2.5 CONT NONREF\". We will use this to query the list/parametersByClass endpoint:\n\npm2.5code = \"PM2.5 CONT NONREF\" \n\nepa_pm2.5pars_list = epa_req |&gt; \n  req_url_path_append(epa_pars_url) |&gt; \n  req_url_query(\n    email = epa_email,\n    key = epa_key,\n    pc = pm2.5code\n  ) |&gt; \n  req_perform() |&gt; \n  resp_body_json()\n\nepa_pm2.5pars_list$Data |&gt; tibblify()\n\n# A tibble: 4 × 2\n  code  value_represented                     \n  &lt;chr&gt; &lt;chr&gt;                                 \n1 88500 PM2.5 Total Atmospheric               \n2 88501 PM2.5 Raw Data                        \n3 88502 Acceptable PM2.5 AQI & Speciation Mass\n4 88503 PM2.5 Volatile Channel                \n\n\nWe were able to identify it as 88501.\nNext we will search the monitors/byCounty endpoint to identify monitors in New York county that measure parameter 88501, which is our PM2.5 measurement.\n\ncounty_endpoint = \"monitors/byCounty\"\n\n\nepa_req_county_monitors = epa_req |&gt; \n  req_url_path_append(county_endpoint) |&gt; \n  req_url_query(\n    key = epa_key,\n    county = \"061\",\n    param = \"88501\",\n    bdate = \"20150101\",\n    edate = \"20240930\",\n    state = \"36\"\n  )\n\ncounty_monitor_list = epa_req_county_monitors |&gt; req_perform() |&gt; resp_body_json()\n\ncounty_monitor_list$Data |&gt; tibblify() \n\nThe spec contains 5 unspecified fields:\n• concurred_exclusions\n• naaqs_primary_monitor\n• qa_primary_monitor\n• tribal_code\n• tribe_name\n\n\n# A tibble: 4 × 41\n  state_code county_code site_number parameter_code   poc parameter_name\n  &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt; &lt;chr&gt;         \n1 36         061         0135        88501              3 PM2.5 Raw Data\n2 36         061         0115        88501              3 PM2.5 Raw Data\n3 36         061         0128        88501              3 PM2.5 Raw Data\n4 36         061         0134        88501              3 PM2.5 Raw Data\n# ℹ 35 more variables: open_date &lt;chr&gt;, close_date &lt;chr&gt;,\n#   concurred_exclusions &lt;list&gt;, dominant_source &lt;chr&gt;,\n#   measurement_scale &lt;chr&gt;, measurement_scale_def &lt;chr&gt;,\n#   monitoring_objective &lt;chr&gt;, last_method_code &lt;chr&gt;,\n#   last_method_description &lt;chr&gt;, last_method_begin_date &lt;chr&gt;,\n#   naaqs_primary_monitor &lt;list&gt;, qa_primary_monitor &lt;list&gt;,\n#   monitor_type &lt;chr&gt;, networks &lt;chr&gt;, monitoring_agency_code &lt;chr&gt;, …\n\n\nWe identify 0135 as one potential sensor with such data.\nWe are going to pull one year of data from this sensor from the sampleData/bySite endpoint:\n\nsite = \"0135\"\n\ndata_by_site_url = \"sampleData/bySite\"\n\nepa_req_sample = epa_req |&gt; \n  req_url_path_append(data_by_site_url) |&gt; \n  req_url_query(\n    key = epa_key,\n    site = site,\n    param = \"88501\",\n    state = \"36\",\n    county = \"061\",\n    bdate = \"20080101\",\n    edate = \"20081231\"\n  )\n\n\nepa_sample_data = epa_req_sample |&gt; req_perform() |&gt; \n  resp_body_json()\n\nepa_sample_data$Data |&gt; tibblify()\n\nThe spec contains 2 unspecified fields:\n• uncertainty\n• date_of_last_change\n\n\n# A tibble: 8,783 × 29\n   state_code county_code site_number parameter_code   poc latitude longitude\n   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;          &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 36         061         0135        88501              3     40.8     -73.9\n 2 36         061         0135        88501              3     40.8     -73.9\n 3 36         061         0135        88501              3     40.8     -73.9\n 4 36         061         0135        88501              3     40.8     -73.9\n 5 36         061         0135        88501              3     40.8     -73.9\n 6 36         061         0135        88501              3     40.8     -73.9\n 7 36         061         0135        88501              3     40.8     -73.9\n 8 36         061         0135        88501              3     40.8     -73.9\n 9 36         061         0135        88501              3     40.8     -73.9\n10 36         061         0135        88501              3     40.8     -73.9\n# ℹ 8,773 more rows\n# ℹ 22 more variables: datum &lt;chr&gt;, parameter &lt;chr&gt;, date_local &lt;chr&gt;,\n#   time_local &lt;chr&gt;, date_gmt &lt;chr&gt;, time_gmt &lt;chr&gt;, sample_measurement &lt;dbl&gt;,\n#   units_of_measure &lt;chr&gt;, units_of_measure_code &lt;chr&gt;, sample_duration &lt;chr&gt;,\n#   sample_duration_code &lt;chr&gt;, sample_frequency &lt;chr&gt;, detection_limit &lt;dbl&gt;,\n#   uncertainty &lt;list&gt;, qualifier &lt;chr&gt;, method_type &lt;chr&gt;, method &lt;chr&gt;,\n#   method_code &lt;chr&gt;, state &lt;chr&gt;, county &lt;chr&gt;, date_of_last_change &lt;list&gt;, …\n\n\nThis succeeded, so let’s make a plot. We are going to average the measurements over each day to reduce the noise slightly:\n\nepa_sample_data$Data |&gt; tibblify() |&gt; select(date_gmt,sample_measurement) |&gt; group_by(date_gmt) |&gt; \n  summarise(mean_pm25 = mean(sample_measurement)) |&gt; ggplot(aes(x=as_datetime(date_gmt),y=mean_pm25)) +\n  geom_point() +\n  scale_x_datetime()\n\nThe spec contains 2 unspecified fields:\n• uncertainty\n• date_of_last_change\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html#conclusion",
    "href": "meetups/Meetup10/GeorgeHagstrom_EPA_API.html#conclusion",
    "title": "EPA Code Vignette",
    "section": "Conclusion",
    "text": "Conclusion\nI’m surprised at how noisy the PM2.5 measurements are from this single site in Manhttan. Although there is substantial noise, it does seem like July is a month with much worse air quality than other months, at least in 2008."
  },
  {
    "objectID": "meetups/Meetup10/VignetteTranscripts/API-EPA-Transcript.html",
    "href": "meetups/Meetup10/VignetteTranscripts/API-EPA-Transcript.html",
    "title": "DATA 607 Fall 2025",
    "section": "",
    "text": "Intro\nhello class and welcome to another coding vignette for data 607 in this\ncoding vignette I'm going to go through an example of how to work with the US\ncepa Air Quality system API and in particular how to access data from it\nand load that data into R using the hitter 2 package so every API is pretty\nmuch a unique entity and when you're when you decide to work with an API\nOpen EPA API Page\npretty much the first thing you need to do is to look at the documentation um for that API and so here I have this\npage open um we can see that it has uh a bunch of\nlinks and these links are to things called services or endpoints so if I um\nclick and take a look at the different links we see that there's um a service\ncalled sign up and we we can see an example of how um what\nan API call to sign up looks like and so you can kind of deconstruct the parts of\nan API call so we see that there's a URL and it's aqs it starts with aq.\nepa.gov um slata API and that's kind of the root um\nof the API call so this uh URL is going to kind of be the the first thing we put\nto initialize our API call and then um the very next uh the very\nnext link is the name of the endpoint which is uh which corresponds\nto a certain type of service called signup and then after the endpoint\nEndpoints- signup, then list,\nthere's this question mark and we see um that we Define a variable that we're going to send um to the API so if I was\nto paste this API um into to into this browser window and hit enter it would\nsign up um my email example.com uh for an API key it would actually mail um it\nwould actually mail an API key to my email\nemail.com so I don't want to uh sign that email address up for the 50 millionth time I'm going to show you an\nexample of a different uh um of what a different a different API call uh looks\nlike and how and what the output looks like so here's um the list services and\nthe list Services um basically this is a series of endpoints that gives you\ninformation about how different objects are encoded uh within the API so if you\nwant to for example know how to um describe a state or a\ncounty um or a measuring site so basically um this API is is for a series\nof data collectors that have been put all over the United States and collect a whole manner of different types of data\nso they're in different states and different counties um they they have different\ntypes of uh they have different types of measurements that they can make and so\nDeconstructing a counties by states API call\nas an example I'm going to select this uh counties by States example\nand so here again you can see the um common structure of the API call it\nstarts with a qs. epa.gov slata API and then it has the endpoint\nand so in this case the endpoint is list SLC counties by state and then it has a list of the variables\nso the email is test aq. API so this is\nkind of their their throwaway um username for really lightweight usage\ntheir key for that API is the is called test and then the state is 37 which\nShow an API call for NC and NY by pasing the URL and explain the result\ncorresponds to North Carolina and if I copy this uh link and I paste it into my\nbrowser and hit enter it actually um executes the command and gets some data\nback and this data would be um parsable as a Json file and it's a list of all\nthe counties in North Carolina and it also has a code that\ncorresponds to the name of each County so you can see that County 001 is\nallance and then there's a bunch of different different counties and I I\nbelieve New York is um State 36 so if\nI yeah that's right so if I put in 36 here I can see that it looks like they\nonly use odd numbers for counties which is a bit um a bit curious at least in the two states we've tried but you can\nsee that this uh returns a bunch of information so there's a header with some information about the request like\nhow many rows whether it was successful the time it was made and it ALS then it also contains um the\ndata and \nWhy we automate and the HTTR2 package, RStudio\nyou uh could get pretty far um\nconstructing these by hand but there will be a limit um to uh to how far you\ncan go so as soon as you need to perform a large number of requests um do any\nkind of uh automated process you'll want to uh start\ngenerating your requests programmatically and to do that you'll\nuse something called the hit or 2 package and so here I'm setting up a\nIntro to the libraries and the API url stems\nlittle uh um our studio window so I'm going to load some files    \nso first I'm going to load the hitter 2 um library and so this is a library that\nlets you systematically construct um URLs that look like this according to\nthe rules of dapi um then we'll also want the tidyverse um and some libraries for\ndealing with Json files so in particular um tifi is a nice library\nthat uh does a lot of automatic unnesting of uh Json and since you're going to get a lot\nof Highly nested data out of this process it's it's a useful it's a useful one to use\num okay so um just uh for your reference uh I\nhave here the link to the um documentation if you want to follow along or if you want to play with uh and\nplay with the data there even if you want to use it for for your project um you can do that so um the first things\nI'm going to do are I'm going to Define some strings uh that I'm going to use a\nlot in constructing requests so I'm going to Define first of all my email address um so if you remember in\nAPI Key Security\nthese strings I always have to provide an email um so that's uh my email and then\num I signed up for a key and uh you should generally speaking be um cautious\nwith uh your API keys if someone else uh got access to your key they\ncould make like a ton of requests and crash the EPA server or at least get you\nbanned from from using it so you should pretend it's like a password uh and so\ninstead of printing it in this document uh not that I don't trust you guys but just it's just uh wise I actually store\nit in a file and um I read it uh I read it from\nthe file and so for your um for your projects I would prefer if you decide to\nuse a um if you decide to use an API that\nrequires a key recommend you do it like this so don't don't just uh be giving away your key to everybody um if you if\nyou wanted a um an API to play with that's kind of fun to use and has a nice interface to get a better idea how this\nworks sorry for the random aside it doesn't require an API key um the National Weather Service one is pretty\nnice um and I'll have uh shown what that URL is in the um uh the Meetup for this\nweek actually anyway so we'll want to define the API key um and then um this\nSetting up the STEMs for the API request\nEPA uh URL is uh quite\nimportant and this is basically the root of all the API requests so I don't want\nto have to be typing this every single time I create a request and so I'm just going to define a little shorthand for\nit and now I can start to kind of build the bones of an API request and how do we\nhow do we initialize a request so there's a function um in uh\nhitter hitter 2 called request and if I pass um my URL to the\nrequest function and I hit enter Then I can see\ndown here that I've created a um very basic uh request this request won't\nreally do much of anything um you can see that it has different uh parts so\nthe very um the very first part says git and Then followed by the URL we're I'm\nGET requests and briefly talking about other types\nonly going to be talking about get requests in this class there's um three\nother types of main requests that are more advanced and they can be quite\nuseful so they're requests that allow you to um post new information on another\nwebsite um or uh Delete information from a website Etc so if you wanted to like\ninteract with uh generative AI you would use a different type of requests like there's like open has an API for example\nif you wanted to you know to run like you know automated stock trading from\nyour home computer you you know there would be an API for the uh stock trading\nsoftware and you would interact with it not using a git request but with a post request um but these uh git requests are\nwhat you use to download data primarily and so we will only be talking about uh get requests and then the body\num uh generally speaking uh you put stuff in the body um when you want to uh\nLooking at the request and its structure in the terminal/console, adding variables using req_url_query\ntransmit information to the other computer so we won't be seeing too much in the    \nbodies all right so we can um add different attributes uh to this\nrequest using functions in the hit or 2 Library so for example um the rec URL\nquery function this adds uh variables or\nparameters to the request and so if I add my email so if I put in EPA email\nand hit enter let me see did I do it right no I didn't do it\nright I need to uh say what the variable is called so let's make it look like\nthis so here you can see that um the URL has changed and this shows you again how\nthe URL is encoding the information in the query so I can see that now I have a variable um email equals uh then my\nemail address and uh the at symbol is encoded with percent 40 um so that's\njust a different uh a different character encoding so don't be too alarmed by that and then um I can take this request\nand I can um Define a variable that's equal to it so I can say EPA request\nequals this and that allows me to build um so this is that allows me to build a\nlot of different uh a lot of different API calls all to the EPA website that\nuse um this initial structure as a base and that just makes your code a lot more\nFirst API Call, getting a list of counties by state, building endpoints to classes, counties, states, pars, look at website\ncompact um and easy to read so um the very first uh type of API\ncall that we're going to do is similar to the the ones I initially showed you\nwhere I went and um got a list of counties um so I'm going to show you how to how to do that from the command line\nand so first we're going to have to Define um the end points so I'm going to Define\num an end points uh for the counties let's see\nso EPA counties URL that's um\nlist counties by state and I'm going to define a state's\nURL that's going to be list um see\nStates and then um I'm going to define a class's URL so this is important so I'm just\ngoing to go back I'm just just to show just so you know where I'm at um so there's uh where are classes so\nlist classes um this basically tells you the\ndifferent a list of all the different types of measurements that exist in the data set so like kind of at a course\nlevel so what type of data you can what type of data is available and then um\nlastly uh the parameters by class that tells you the actual individual measurements for a given parameter class\nand so those are going to be important for um for understanding uh yeah for\nfiguring out for figuring out how to tell EPA what data we actually want so so EPA classes is um lists by classes\nand so notice how um these are these all have like the same root because they're they're kind\nof similar services and then we're going to do EPA\npars URL and that's going to be\num list parameters by\nclass and so these these are just the names here I could have copied and pasted them um if I had\nAPI Call to get numeric codes for each state\nwanted okay so let's start with States so suppose I want to get a list of the\ncodes the numeric codes for each of the states in the API I mean this EPA data\ndata set how would I do that so the very first thing you should do is you should take a look at the\ndocumentation and look at the states endpoint and see what's described and so\nhere it says that um you need to send to this endpoint um two variables just your\nemail and your key there's no other optional\nvariables so this should be a relatively easy query to construct so let's see if we can do it so I'm going to take um   \nthis EPA wreck as my starting point and so I'm going to um\nDefine a States request and I'm going to start with e PA\nRec and then so I need to add to it two things I need to add the um list states\nend point and I do that using the function\nRec URL path append and so this is going to\nadd the um list states path to the end of um to the end of this apsa doov umata\nAPI so I'm going to put uh that so what was it called it was\ncalled EPA States URL and then I'm going to pipe that to\nrec URL query and so here I'm going to add um my\nkey and so I believe it's called key and it my variable is paa key\nand that'll Define a\num hey I got it wrong EPA States URL did    \nI not hit enter make sure okay very good and \nPerform the request (mention sequential), storing the response\nnow how do I\nactually um perform the request so perform the request using a function\ncalled wck perform and so as and so you're going to start to see um hitter 2 is one of these\nterse functions and they all have this kind of similar structure where they start with the same sameish three\nletters and then they have uh the name of what they do and then sometimes there's like more complicated variants\nwe'll get to we'll talk about one of these we'll talk to the sequential we'll\nget to the sequential one um later on in this VN yet um and so then\nuh we will perform the request it'll take some time um for the\nuh for the website to do the request it might be very fast might might take some\ntime depending on what the request is and then there's two ways to um uh to\naccess the response so we can either perform the request and then have a separate uh have a separate like\nresponse um access afterwards is a separate command so the response will\nkind of be stored in the environment and we can we can get the last response using functions and the hit or 2 package\ntalks a little bit about how to do that but I'm just going to um pipe the output\ndirectly into another function which will be a function that starts with rasp\nso that stands for response and we are going to be expecting a Json\nfile and should I call this something yes I'm going to call this States\nlist let's see what we get looks like it executed quickly let's\ntake a look at it okay so we can see that we have um a list um in response\nand that rest body Json has pared um has pared this list and it has basically a\nheader it says there's 56 row so it must have ah it showed my it showed my\npassword oh well um it uh yeah so it shows\num it shows the data and then we can access uh the data variable like\nthis and if we want to um unest it really quickly we can use\ntify and we get a nice list and you can see it's all the states and it looks like um it looks like they're in\nalphabetical order I'm a little bit curious uh actually it's alphabetical all the way\nso I'm going to print I'm going to print out 56 let's take a look yeah so it's\nalphabetical oh that's interesting it doesn't quite go 1 to\n50 there's some skipping of numbers which is which is interesting good to know um\nBuilding the counties list and query from the states result\nbut anyways you can see from this list uh which state is which and we can see that New York is\nState um 36 and we can um then use that\ninformation to uh find a list of counties in New York city so if we uh go\nto the sites by County uh we see that or the counties by state we see that we need to provide it with um one\nadditional piece of information which is state so we can create um a very similarly\nstructured request so I'm going to copy so I'm going to make this uh New York\ncounties let's say and instead of States I'm going to\nput counties here and then I'm going to have the EPA\nkey and um for state I'm going to put New York I think\nit needs to be I think it needs to be in um character but we'll\nsee that's PA counties let's try this\nagain okay very good um let's see if this\nworks I'm gonna do I'm gon to send this to rec perform I'm going to send it to resp\nbody Json let's say New York\ncounties what's that and let's uh\ntry Okay um so that's funny that there's a Wyoming County\num see the New York counties\nlet's tify it and I don't know print the first\n100 okay so we can hear that um we can see that New York County which is Manhattan um is number\n61 um and \nLooking for measurement types, classes API call\nso in a minute I'm going to search for all the different monitoring\nsites in New York County um using using this uh using this\ncode but let's also take a look and see what type of measurements we have access\nto that we have available to us so I'm going to be interested in um finding PM 2.5 measurements that's just\na particularly um dangerous type of air pollutant and so I'm going to want to\nagain um uh send a request to this uh per\nlists uh Slash classes I only need to provide my email and my key so I'm going\nto DOA wreck to um\nlet's URL path append so I'm going to make it the\num the classes URL and then I'm going to\ndo Rec URL query and I want um yeah key\nto equal my PA key\nthen let's do Rec perform\nand let's then process the\noutput and we'll call this uh parameter\nExamine classes API call output, pick PM 2.5um continuous non-reference\nclasses okay let's take a look let's take a look at the\ndata okay um there's the data there are a few ways I could look\nat this I could also um I could also do Str Str to\nsee um to see what the structure what the Nesta structure of this list is um but I'm going to tipify it\nbecause that generally looks pretty good okay so we see um the different\ncodes for parameter classes I want to print more so I'm going to print up to 27\nokay and I see a few different um PM 2.5\nclasses and I think the one that I I'm going to want is PM 2.5 um continuous\nnon reference I'm not really 100% sure because I'm not an air pollution expert\nbut we're just going to go we're just going to go with that one um and so then the next uh the next thing I want to do\nis I want to see the actual parameters so that's contained in this list\nparameters by class and if I pass the parameter class um to this list I'll get\na list of all the the different uh parameter codes for um the different\nmeasurements um so let's uh let's do that so we're going to do\nDefine the parameters API request- pick 88500 or 88501\nPM so I'm going to Define pm25    \nequals um\nequals the string I believe that's the string I\nneed yeah that's the code then I'm going to take\num Define parameters PM 2.5 I'm again going to take my EPA\nrequest I'm going to append\nthe let's see which list I need to append the pars EPA pars\nURL and then I'm going to add query yes the key should be my EPA key\nand then the um I believe it's PC yeah PC should equal\npm25 and then I need to um perform the\nrequest parse again to Json you you're not always going to uh\nhave to parse to Json but in this particular case you are and let's uh let's run\nthis let's take a look at the\nparameters five let's do it with the\ndata and let's tify it again okay so there're just four\num I'm guessing uh I don't know what these mean really\nI'm guessing either the first one or the second one is the code that we want so either\n88500 or 88500 And1 is going to be the the the the measurement that we want to\nsearch for \nFind sensors/monitors in NYC with this measurement\nOkay so the next uh the next thing I want to do is I want to see if there's\nany um sensors in New York County um that have made measurements of um of\nthis this data point and so to do that I have to go um to\nthe the monitor's endpoint and so here you can see\nuh the description of this is it returns operation information about the monitors\nused to collect the data and so this includes um yeah just ident identifying\ninformation operational dates [Music] organizations and here you can see as an\nexample this monitors by site um this will return like all the monitors in a\ndifferent County um that uh operate in a certain range of\ndates and which are able to take um a certain um range of measurements and so\nwe need to give this a variety of information including the email the key um the parameter code so that's either\n88500 or 88 5001 we're going to do an experiment um the beginning and ending\ndate um something to know about this is this date is a little bit old so current\num year data is very sparse so it's going to all be before 2023 and then the state\n[Music] county and site or do\nI um yeah I mean to do it by County so\nthis is the one I need to do so monitors by County so I'm going to Define this um this\nendpoint so I'm going to do uh monitors County\nURL like that and I'm going to\num again I'm going to do\na um a new\nquery where I am going to say that the\nsites this is going to be my EPA wreck\nI'm going to pass it to again Rec URL path append and this time\nlet make sure I've actually ran this monitors County\nURL and then I'm going to add the different query parameters that I need so I'm going to do Rec URL\nquery and what are the ones I need so I need my key that should be the EPA\nkey I need the um County so we wanted New York County so\nthat's 061 we found that previously we need the state I think we previously established\nthat New York state is State 36 um begin date and end\ndate um let's do begin date I don't know\n[Music] 2007 and let's do end\ndate how about 202 [Music] 3 don't know 12\n31 and then we also need the parameter code\nso I think I have to send it as a string I'm going to try\n501 I think that's the one that's um most likely to\nbe uh effective and then um let's perform the request\nParse the data, pick 0135 and look at its properties\nand let's uh let's parse the\nanswer let's do and let's\num take a look okay forgot a\ncomma okay um I did something\nwrong what could it be\n23 1231 2007\n101 not 100% sure what I did wrong\num let [Music] me do so I hadn't intended to have this\nproblem occur but let's do do so to debug you want to do\num this so you want to take a look at the last\nresponse unpermitted variable PC okay um yeah so I just uh this should be\nPam let's try it now yeah that worked okay it took a\nlittle bit longer um but let's\nsee okay let's take a look at our sites um there's only a small number of\nthem and I'm gonna just arbitrarily pick um this one the 0135\nsite um it started pretty early and it's still still going strong\nso and you can see that there's a ton of other variables by the way um that we're not looking at but I just want to pick a\nsite um to get some data from so let's let's do that let's do 0135 it has my\ndata um so do site code equals\n0135 \nGet data from 0135\nand now let's let's see we can we can actually download some measurements\nof um of PM 2.5 and there's a variety of ways of doing\nthat there's all all different um they're all different uh of these Services I'm going to pick uh this\nsample data one um and I'm going to do sample data by\nsite and this basically gives you the highest resolution data available from\nthis um from the sensor and so again we can take a look there are some optional\nvariables I'm not going to interact with those but looks like we need to provide the email the key the parameter the\nbeginning and end date um the state the county and the site so it's very similar\nto the previous requests you just have to I also need to have this different endpoint so let's do it um so let's uh\nDefine the um data by\nsite and okay and now let's uh let's get some\ndata so let's define it as um EA sample data and this is we're\ngoing to start again with our EPA W um I should probably have saved\nlike um all of these to make it to make it a little bit uh more concrete so I'm\ngoing to do EPA recx site\njust to make my life easy and then I'm going to add add some\nof these query parameters um URL query\nso sure let's add my EPA key um let's add the\ncounty 061 let's add the state 036 no not 036\n36 um let's add the [Music]\nPam yeah sure let's add the Pam leave the dates\nopen let's add the site code me make sure I know what the site code should be\nnamed yeah it should be named site yeah site code\nokay so now I can I'm just focusing on this I can\nstart with EPA Rex site and now I can add the endpoint notice how you don't\nthe order in which you add the different stuff doesn't really matter um so let's\ndo W URL path append so I want to add my data\ndat by site URL then \nStructuring requests by date- waiting for a request to complete for a long time\nI want to\nadd I need to do the beginning\ndate let's make that why not end\ndate 2023    \n1231 then let's uh perform this might take a little bit of\ntime just because it could be I'm a lot of data then let's do resp body\nJson P Rec site not found not\nthis okay let's let's see\nEPA Rec site that looks\ncorrect don't really know what to make of this\noh eqa okay I'm not I'm not doing another\ntake all right this should work now um let's\ntry so it's taking a long time let's\nsee hope it doesn't take too long well I have done this\nbefore and it was a bit faster okay so I got a bad request um\nlet's take a look and see why last\nresponse um to\nlet's do rest\n\n\nJson okay so I can see the error right there only one year of data is\npermitted um let's do it for 2008\nthen um so you can maybe start to see where this is going to go um I'm going to get it for 2008 and then\nIn sequence- this is not making great sense\nI'm going to show you how to download um in sequence quence okay so let's do it for one\nyear and yeah it's going to take it's going to take some time while it's taking its time I'm going to just get\nsome code ready um\nno to possibly graph um graph the data let's see something like\nthis the feeling this code is what we're going to need um I'll make it look nicer\nso when it finishes we're going to tify it\num we're going to select the date and the measurement of the value so\nthose will be variables we're going to group by the date um we're going to compute the\naverage for each day because there's a lot of measurements each day\num okay finished so let's see what it looks\nlike let\ndo simplify it so you it's pretty big um well it's\nnot that big only only 8,000 rows um but you can see yeah we have some we have\nsome time series data here um you don't actually see too much useful um in the\nfirst uh several columns except that there's a lot of data on each day um you\ncan sort of see uh some variables in here there's like a sample duration I'm\nguessing that's how long it took to make the measurement there's units of measur sample\n[Music] frequency may maybe not sure how how reliable that's going to be um but\nsomewhere in here there is a sample measurement and that's what we want to plot so let's make a plot of\nit okay so here we have um here we have a a graph of the data from this one site\num from New York City uh you can see that it's uh really quite noisy \nPolite scraping with a delay of a lot of dates\nso what if we wanted to get um\nwhat if we wanted to get all the data uh so in your last week's uh lab in your\nlast week's material or in the last few weeks we went over how to do iteration using the um using the per using the per\nlibrary and we're going to do um the same thing here but we're going to put in one um one little uh Nuance which I'm\nnot 100% sure is necessary for interacting with the EPA web page um but\nuh it's something that is definitely true of interacting with websites in\ngeneral and with apis in general which is that if you make a lot of requests to the um\nAPI uh or to the website it um is considered impolite it can be\nmisinterpreted depending on how how many you do can be misinterpreted as a denial of service attack um or you know it\n\ncould just uh get you banned um from there you could get your API key\n\nbanned and so there's a I'll show you how to um put in a little delay between each\n\neach API call to stay um under whatever the limit is so somewhere on this web page it has some information about how\n\nfrequently you're allowed to call their API I remember it being like you don't want to call it more than once every 10\n\nseconds um but I don't don't remember exactly but all right let's start\n\num so there's no data in 2024 so I'm just going to do uh\n\n2023 so I'm going to make this little\n\nuh I'm going to make this little string of years and I'm going to create a\n\nfunction which basically um updates the be the\n\nbefore date which basically creates the before date and the end date so I'm going to make a function called year\n\nfunction and let's see this will be a function of the year and so this is what I'm going to pass uh so I'm going to\n\nbasically pass this year list to a map and the map is going to have something involving this year\n\nfunction and this is going to have um yeah so this is actually going to return\n\na request so I'm going to make uh yeah so basically I'm going to create a list of\n\nrequ and then I'm going to pass that to a function called uh um wreck perform\n\niterative or sequential so basically this function is going to take a year and then it's going to return um it's\n\ngoing to turn return this uh this request so let's see so I want to\n\nconcatenate um the year with 101 for the beginning\n\nand then so I'll make this one should be\n\n1231 then um I'm going to want\n\nto let's\n\njust so I already have that yeah so I'm just going to do I'm G to take\n\nEPA Rec what is it called Rec site\n\nand I'm going to pipe it [Music] to I'm going to just pipe it to\n\nrec URL\n\nquery and I'm going to update the B date and E date so do B date equals B\n\ndate date equals e date I think that has um everything we\n\nneed and then I'm going to I'm going to um pass it to a function called\n\nW throttle and I'm going to say that we can't make more than\n\none request um every I don't know I'm going\n\nto do five seconds uh hope it works when\n\nI tried this uh when I demoed this I did 10 seconds and it took a long time but it worked but I'm feeling like I can\n\nlive a little bit riskier and we'll try 5 Seconds hopefully it doesn't\n\ncrash let's see what went wrong yeah that's not supposed to be\n\nthere what went wrong with string C\n\nH all right what has gone wrong\n\nhere it's interpreting year as a function okay probably because I loaded\n\nlubridate um so we'll call it C year the current year that's silly but whatever I bet\n\nthis works\n\ndon't okay that's\n\npuzzling I'm going to try the\n\nprevious function I used so I'm going to copy and paste it over sorry for the the\n\nlive debugging um why did this\n\nwork there's something off I don't know what it is\n\nbut I'm not going to find out um\n\nbecause I'm I am going to change this to five though\n\num why is it\n\nbeing yeah I need to change that to site code probably\n\nyeah\n\nreally oh do I need to hit enter at the top oh yeah yeah that's stupid will this\n\nwork that works too all right some kind of lesson learned get\n\nrid of this one all right let's continue so now what I'm going to do is I'm going\n\nto make a list of the requests and I'm going to do that by taking the year list and I'm going to\n\npass it to map on the year function and this is a list of basically\n\na request um for each year between 2007 and\n\n2023 and I can\n\nnow perform I'm going to\n\nmake paa sample data list and I'm going to make this uh equal to take my recck\n\nlist and I'm going to pass it to a function called\n\nwreck perform sequential this is just going to perform all the requests in\n\norder and I'm going to have a progress bar so we don't get bored\n\nwatching and let's try it and so you can see\n\num it goes it does some download it waits some\n\nseconds um and it's going to take a little bit of time and while it waits\n\nI'm going to get ready to do um the next step of the analysis\n\nwhich is going to be just to put the data all together\n\nand yeah so I'm going to want to yep so I'm going to want to\n\nbasically U map each element of the list so what's the result of this is going to be a list list where the the answer in\n\neach query um is is an element of the list so the response is an element of the list and so I'm going to do\n\nEPA sample data full equals EPA sample\n\ndata list and then I'm going to map\n\nthat to resp\n\noh no oh oh that's not good I think maybe I needed to have more\n\nof a delay\n\nyeah learn my lesson there um last response uh let's do\n\nH I'm going to try this with 110\n\num because it worked before I might uh actually running out\n\nof a little I might have to pause this and come back later\n\nbut what did I just\n\ndo I need to yes I need to define the year\n\nfunction all right let's throttle it a little bit better um\n\ndo that and let's hope let's hope it works this\n\ntime um it should work this time\n\nand after it works uh hopefully what I'm\n\ngoing to do is uh I'm going to map\n\neach um each element of the list to this rest\n\num body Json then I'm going to create\n\num I'm going to try to um tify just the\n\ndata so I'm going to do something like this I'm going to create I'm take sample\n\ndata full I'm going to send this to I'm going to map This Again by following\n\nAnonymous function so I just want to pull out the\n\ndata have have X data and I'm going to send that\n\nto gify and that'll Bas so basically what I\n\nwant to do is I want like each element of my list to be a tibble um and that\n\nway I can use uh list arbind and when I do this\n\nuh sorry if I'm rushing through these steps you can follow along at home if\n\nyou want want um there's going to be one problem when I try to do this which is\n\nthat all right um so this uh this video has already\n\nbeen pretty long and it's actually not that uh not that long until I have to do um the\n\nclass Meetup and I I'm going to want to um\n\nfigure out uh what exactly the internal Ser server error is uh this time I'm pretty\n\nworried that it's uh me overusing the server but I'm actually going to pause\n\nthe the recording um and come back to it come back to it later and finish it off I'm\n\nactually basically I'm almost done um basically this this step would but you\n\ncan see like some of the but this maybe shows you some of the difficulties um in using apis and calling them multiple\n\ntimes um but basically what I was going to do is I was going to take take this\n\num take the responses I got from running this iteratively or sequentially rather\n\nand I was going to try to turn each one of those responses into a tibble and then I was going to try to use uh list rbind um to to put them all together and\n\nthere would be uh exactly one problem with doing that which is there'd be one\n\none data one um one row in the data which had an inconsistent data type\n\nwhich prevents you from using list arbine so I'd have to abandon one of the variables um and that turned out to be\n\nthis variable called uh date of last change but you would do that like this\n\num select and you so you'd map um so you\n\nturn everything into a tible and then you map um to the select function\n\nand you would select everything\n\nexcept date of last change and then You' be able to send\n\nthis to list our bind\n\num and this would and this would\n\nwork yeah anyways hopefully there'll be another video\n\nattached to this um or later thank you very much for listening and I will see\n\nyou soon"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#week-preview",
    "href": "meetups/Meetup6/Meetup6.html#week-preview",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Week Preview",
    "text": "Week Preview\n\nThis week is all about text and categorical variables\nKey packages stringr, forcats and also the regex language\nChapters 14-16 in R4DS\nI’ll release a video coding analysis of a baby names dataset"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#hw-pitfalls",
    "href": "meetups/Meetup6/Meetup6.html#hw-pitfalls",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "HW Pitfalls",
    "text": "HW Pitfalls\n\nDon’t automatically remove all outliers, decide case by case\nFormatting- big variation in submission length this week\n\nDon’t submit a table that is multiple pages long, find another way"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#window-functions",
    "href": "meetups/Meetup6/Meetup6.html#window-functions",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Window Functions",
    "text": "Window Functions"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#window-functions-1",
    "href": "meetups/Meetup6/Meetup6.html#window-functions-1",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Window Functions",
    "text": "Window Functions"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#window-functions-2",
    "href": "meetups/Meetup6/Meetup6.html#window-functions-2",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Window Functions",
    "text": "Window Functions"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#window-functions-3",
    "href": "meetups/Meetup6/Meetup6.html#window-functions-3",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Window Functions",
    "text": "Window Functions"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#case-study-huntingtons-disease",
    "href": "meetups/Meetup6/Meetup6.html#case-study-huntingtons-disease",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Case Study: Huntington’s Disease",
    "text": "Case Study: Huntington’s Disease\n\nTandem Repeats: sections of genetic code where short element repeats:\nCAGCAGCAGCAGCAGCAGCAG\n\n\nIn certain genes wrong number of repeats causes disease:\nHuntington’s Disease:\n\nCAA and CAG repeats, code for amino acid Glutamine\n6-35 normal\n36+ causes disease"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#pattern-matching",
    "href": "meetups/Meetup6/Meetup6.html#pattern-matching",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Pattern Matching",
    "text": "Pattern Matching\n\nHow to determine number of repeats CAA + CAG repeats in a section of code?\n\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nDNA =read_file(\"Huntington.txt\")\n\nDNA |&gt; str_sub(1, 800) |&gt; print()\n\n[1] \"\\t1 ttgctgtgtg aggcagaacc tgcgggggca ggggcgggct ggttccctgg ccagccattg\\n       61 gcagagtccg caggctaggg ctgtcaatca tgctggccgg cgtggccccg cctccgccgg\\n      121 cgcggccccg cctccgccgg cgcagcgtct gggacgcaag gcgccgtggg ggctgccggg\\n      181 acgggtccaa gatggacggc cgctcaggtt ctgcttttac ctgcggccca gagccccatt\\n      241 cattgccccg gtgctgagcg gcgccgcgag tcggcccgag gcctccgggg actgccgtgc\\n      301 cgggcgggag accgccatgg cgaccctgga aaagctgatg aaggccttcg agtccctcaa\\n      361 gtccttccag cagcagcagc agcagcagca gcagcagcag cagcagcagc agcagcagca\\n      421 gcagcagcag caacagccgc caccgccgcc gccgccgccg ccgcctcctc agcttcctca\\n      481 gccgccgccg caggcacagc cgctgctgcc tcagccgcag ccgcccccgc cgccgccccc\\n      541 gccgccaccc ggcccggctg tggctgagga gccgctgcac cgaccaaaga aagaactttc\\n      601 agctaccaag aaagaccgtg tgaatcattg tctg\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#processing-steps",
    "href": "meetups/Meetup6/Meetup6.html#processing-steps",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Processing Steps",
    "text": "Processing Steps\n\nGet rid of tabs, numbers, spaces, newlines:\n\n\nDNA = DNA |&gt; \n  str_remove_all(\"[0-9]\") |&gt;   \n  str_remove_all(\"\\t\") |&gt; \n  str_remove_all(\"\\n\") |&gt; \n  str_remove_all(\" \")\n\nDNA |&gt; str_sub(1,60) |&gt; print()\n\n[1] \"ttgctgtgtgaggcagaacctgcgggggcaggggcgggctggttccctggccagccattg\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#matching-using-regex",
    "href": "meetups/Meetup6/Meetup6.html#matching-using-regex",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Matching Using regex",
    "text": "Matching Using regex\n\nFollowing Regular Expression can match repeats of various lengths: (CAA|CAG){10,}\n\n\nDNA |&gt; \n  str_count(\"caa\") |&gt;\n  print()\n\n[1] 179\n\nDNA |&gt; \n  str_count(\"cag\") |&gt; \n  print()\n\n[1] 471\n\nDNA |&gt;\n  str_count(\"(caa|cag){10,}\") |&gt;\n  print()\n\n[1] 1"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#how-long",
    "href": "meetups/Meetup6/Meetup6.html#how-long",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "How Long?",
    "text": "How Long?\n\nDNA |&gt; \n  str_extract_all(\"(caa|cag){10,}\") |&gt; \n  str_length()/3\n\n[1] 23\n\nDNA |&gt; str_extract_all(\"(caa|cag){10,}\")\n\n[[1]]\n[1] \"cagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcaacag\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#other-examples",
    "href": "meetups/Meetup6/Meetup6.html#other-examples",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Other Examples",
    "text": "Other Examples\n\nLibraries stringr and regex elevate your ability to work with text\nUseful in many domains:\n\nEmail: \"\\\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,}\\\\b\"\nUS Zip Code: \"\\\\d{5}([ \\\\-]\\\\d{4})?\"\nYYYY-MM-DD dates: \"/([12]\\\\d{3}-(0[1-9]|1[0-2])-(0[1-9]|[12]\\\\d|3[01]))/\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#stringcharacter-basics",
    "href": "meetups/Meetup6/Meetup6.html#stringcharacter-basics",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "String/Character Basics",
    "text": "String/Character Basics\n\nDefined by enclosing in quotes:\n\n\nstring1 = c(\"word\",'another word', \"multiple\\n lines\",\n            \"using \\\"escapes\\\" and \\u00FCnicode \")\nstr_view(string1)\n\n[1] │ word\n[2] │ another word\n[3] │ multiple\n    │  lines\n[4] │ using \"escapes\" and ünicode \n\n\n\nSpecial characters:\n\n\"\\n\" newline\n\"\\t\" tab\n\"\\\" to escape and write special character"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#key-functions",
    "href": "meetups/Meetup6/Meetup6.html#key-functions",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Key functions:",
    "text": "Key functions:\n\nstr_c(): adds strings together\nstr_sub(): subsets strings\nstr_flatten() combines chars in a char vector into single string\n\n\nstr_c(\"Abraham\",\" \",\"Lincoln\")\n\n[1] \"Abraham Lincoln\"\n\nstr_sub(c(\"Abraham\",\"Lincoln\"),start=1,end=4)\n\n[1] \"Abra\" \"Linc\"\n\nstr_flatten(letters)\n\n[1] \"abcdefghijklmnopqrstuvwxyz\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#key-functions-1",
    "href": "meetups/Meetup6/Meetup6.html#key-functions-1",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Key functions:",
    "text": "Key functions:\n\nstr_view() underlying string and pattern matches\nstr_length() self explanatory\nstr_count() counts matches with a pattern\nstr_detect() logical match\n\n\nstr_length(\"Grizzly Bear\")\n\n[1] 12\n\nDNA |&gt; str_count(\"caa\") |&gt; print()\n\n[1] 179\n\n\"Lions, Tigers, and Bears\" |&gt; str_view(\"Tiger\")\n\n[1] │ Lions, &lt;Tiger&gt;s, and Bears\n\n\"Lions, Tigers, and Bears\" |&gt; str_detect(\"Cat\")\n\n[1] FALSE"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#key-functions-2",
    "href": "meetups/Meetup6/Meetup6.html#key-functions-2",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Key functions:",
    "text": "Key functions:\n\nstr_replace() and str_replace_all: substitute\nstr_remove() and str_remove_all(): remove matches\nstr_extract() and str_extract_all(): pull out matches\n\n\n\"Abraham Linc0ln\" |&gt; str_replace_all(\"0\",\"o\")\n\n[1] \"Abraham Lincoln\"\n\nc(\"Blueberry\",\"Blackberry\",\"Lingonberry\") |&gt; str_remove(\"berry\")\n\n[1] \"Blue\"   \"Black\"  \"Lingon\"\n\nDNA |&gt; str_extract_all(\"tttttt\")\n\n[[1]]\n[1] \"tttttt\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#regular-expressions-regex",
    "href": "meetups/Meetup6/Meetup6.html#regular-expressions-regex",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Regular Expressions (regex)",
    "text": "Regular Expressions (regex)\n\nRegex are a powerful language for describing complex patterns\nString search tools often use them by default\nRegular Characters:\na-z, A-z, 0-9\nMetacharacters:\n\n. , * , () , [] , {} , | , \\ , $ , ? , ^ , +\nThese add special meaning to the patterns"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#wildcard-.",
    "href": "meetups/Meetup6/Meetup6.html#wildcard-.",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Wildcard .",
    "text": "Wildcard .\n\n. a symbol that can stand for any character\nFind all instances of “a” followed by two letters of any type, followed by another “a”:\n\n\nwords |&gt; str_view(\"a..a\")\n\n [35] │ &lt;alwa&gt;ys\n [43] │ &lt;appa&gt;rent\n [49] │ &lt;area&gt;\n [53] │ &lt;arra&gt;nge\n [62] │ av&lt;aila&gt;ble\n[598] │ par&lt;agra&gt;ph\n[801] │ st&lt;anda&gt;rd"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#quantifiers",
    "href": "meetups/Meetup6/Meetup6.html#quantifiers",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Quantifiers ?, +, *",
    "text": "Quantifiers ?, +, *\n\nFind patterns that repeat\n? for optional matches:\n\n\nstr = \"eTeeTeeeTeeeeT\"\nstr |&gt; str_view(\"ee?\")\n\n[1] │ &lt;e&gt;T&lt;ee&gt;T&lt;ee&gt;&lt;e&gt;T&lt;ee&gt;&lt;ee&gt;T\n\n\n\n+ for repeat matches:\n\n\nstr |&gt; str_view(\"ee+\")\n\n[1] │ eT&lt;ee&gt;T&lt;eee&gt;T&lt;eeee&gt;T\n\n\n\n* for any number of repeat matches, including 0:\n\n\nstr |&gt; str_view(\"ee*\")\n\n[1] │ &lt;e&gt;T&lt;ee&gt;T&lt;eee&gt;T&lt;eeee&gt;T"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#advanced-quantifiers-mn",
    "href": "meetups/Meetup6/Meetup6.html#advanced-quantifiers-mn",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Advanced Quantifiers {m,n}",
    "text": "Advanced Quantifiers {m,n}\n\nUse {m,n} to specify at least “m” matches but no more than “n” matches:\n\n\nAASeq = read_file(\"HuntingtonTrans.txt\") |&gt; \n  str_remove_all(\"\\t\") |&gt; \n  str_remove_all(\"\\n\") |&gt; \n  str_remove_all(\" \")\nAASeq |&gt; str_extract_all(\"Q{2,}\") |&gt; print()\n\n[[1]]\n [1] \"QQQQQQQQQQQQQQQQQQQQQQQ\" \"QQQ\"                    \n [3] \"QQ\"                      \"QQ\"                     \n [5] \"QQ\"                      \"QQ\"                     \n [7] \"QQ\"                      \"QQ\"                     \n [9] \"QQ\"                      \"QQ\"                     \n[11] \"QQ\"                      \"QQ\"                     \n[13] \"QQ\"                     \n\nAASeq |&gt; str_extract_all(\"Q{3,10}\") |&gt; print()\n\n[[1]]\n[1] \"QQQQQQQQQQ\" \"QQQQQQQQQQ\" \"QQQ\"        \"QQQ\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#character-classes",
    "href": "meetups/Meetup6/Meetup6.html#character-classes",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Character Classes",
    "text": "Character Classes\n\nUse square brackets to denote a group of characters to match\n\n\nwords |&gt; str_view(\"[aeiou]tt[aeiou]\")\n\n [60] │ &lt;atte&gt;nd\n[107] │ b&lt;otto&gt;m\n[173] │ comm&lt;itte&gt;e\n[470] │ l&lt;ette&gt;r\n[508] │ m&lt;atte&gt;r\n\n\n\n“^” matches everything not in the brackets\n\n\nwords |&gt; str_view(\"q[^u]\")\nwords |&gt; str_view(\"q[u]\") |&gt; head(5)\n\n[276] │ e&lt;qu&gt;al\n[665] │ &lt;qu&gt;ality\n[666] │ &lt;qu&gt;arter\n[667] │ &lt;qu&gt;estion\n[668] │ &lt;qu&gt;ick"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#special-characters-and-escaping",
    "href": "meetups/Meetup6/Meetup6.html#special-characters-and-escaping",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Special Characters and Escaping",
    "text": "Special Characters and Escaping\n\nUse “\\” if you need to match a metacharacter for real, i.e.\n“\\.”, “\\\\”, “\\+” etc\nSpecial character groups also denoted with “\" -”\\s” space, opposite: “\\S” -“\\d” number, “\\D” not number -“\\w” letter or number, “\\W” not letter or number\nRanges: “[A-E]”, “[1-4]”\nNEED TO DOUBLE ESCAPE IN R!\n\n\nstr_view(\"abc123\",\"\\\\d\")\n\n[1] │ abc&lt;1&gt;&lt;2&gt;&lt;3&gt;"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#logical-or",
    "href": "meetups/Meetup6/Meetup6.html#logical-or",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Logical Or",
    "text": "Logical Or\n\n“|” allows for specifying alternative expressions to match\n\n\nfruit |&gt; str_view(\"apple|berry|melon\") |&gt; head(10)\n\n [1] │ &lt;apple&gt;\n [6] │ bil&lt;berry&gt;\n [7] │ black&lt;berry&gt;\n[10] │ blue&lt;berry&gt;\n[11] │ boysen&lt;berry&gt;\n[13] │ canary &lt;melon&gt;\n[19] │ cloud&lt;berry&gt;\n[21] │ cran&lt;berry&gt;\n[29] │ elder&lt;berry&gt;\n[32] │ goji &lt;berry&gt;"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#grouping",
    "href": "meetups/Meetup6/Meetup6.html#grouping",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Grouping",
    "text": "Grouping\n\n“()” makes whatever inside act like a group\nExample from earlier, finding repeats of a sequence\n\n\nDNA |&gt; str_extract_all(\"(caa|cag){3,60}\")\n\n[[1]]\n[1] \"cagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcaacag\"\n[2] \"cagcagcag\"                                                            \n[3] \"cagcagcag\"                                                            \n[4] \"caacagcaa\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#backreferencing",
    "href": "meetups/Meetup6/Meetup6.html#backreferencing",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Backreferencing",
    "text": "Backreferencing\n\nIf something is contained in parentheses, you can reference it using “\\1”, “\\2”, etc\n\n\nDNA |&gt; str_extract_all(\"(......)\\\\1+\")\n\n[[1]]\n [1] \"cagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcagcag\"\n [2] \"ccgccgccgccgccgccg\"                                          \n [3] \"cagccgcagccg\"                                                \n [4] \"ggccctggccct\"                                                \n [5] \"gaggaggaggag\"                                                \n [6] \"gaggaggaggag\"                                                \n [7] \"cctcgtcctcgt\"                                                \n [8] \"cccccaccccca\"                                                \n [9] \"gggcctgggcct\"                                                \n[10] \"tgagcttgagct\"                                                \n[11] \"aaaaaaaaaaaaaaaaaa\""
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#anchors",
    "href": "meetups/Meetup6/Meetup6.html#anchors",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Anchors",
    "text": "Anchors\n\n“^” start of string, “$” end of string\n\n\nwords |&gt; str_view(\"^a.+d$\")\n\n[12] │ &lt;add&gt;\n[17] │ &lt;afford&gt;\n[38] │ &lt;and&gt;\n[52] │ &lt;around&gt;\n[60] │ &lt;attend&gt;"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#boundary",
    "href": "meetups/Meetup6/Meetup6.html#boundary",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Boundary",
    "text": "Boundary\n\n“\\b” the boundary of a word\n\n\nsentences |&gt; str_view(\"ink\") |&gt; head(5)\n\n [12] │ A rod is used to catch p&lt;ink&gt; salmon.\n [73] │ The &lt;ink&gt; stain dried on the finished page.\n [96] │ Bail the boat to stop it from s&lt;ink&gt;ing.\n[148] │ The spot on the blotter was made by green &lt;ink&gt;.\n[206] │ The club rented the r&lt;ink&gt; for the fifth night.\n\nsentences |&gt; str_view(\"\\\\bink\\\\b\")\n\n [73] │ The &lt;ink&gt; stain dried on the finished page.\n[148] │ The spot on the blotter was made by green &lt;ink&gt;.\n[217] │ It is hard to erase blue or red &lt;ink&gt;.\n[321] │ Fill the &lt;ink&gt; jar with sticky glue."
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#data-science-in-context-presentations",
    "href": "meetups/Meetup6/Meetup6.html#data-science-in-context-presentations",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Data Science in Context Presentations",
    "text": "Data Science in Context Presentations"
  },
  {
    "objectID": "meetups/Meetup6/Meetup6.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup6/Meetup6.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 6: Working With Text and Strings",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#week-schedule-and-hw-feedback",
    "href": "meetups/Meetup11/Meetup11.html#week-schedule-and-hw-feedback",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Week Schedule and HW Feedback",
    "text": "Week Schedule and HW Feedback\n\nWebscraping confusing and inconsistent across platforms\nread_html vs read_html_live\nOn some machines, open screts rendered a static pages\nLab on text mining due this Sunday\nExpect one coding vignette tomorrow some time\nReading a different book Text Mining with R: A Tidy Approach Chapters 1-4"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#setup-a-meeting-if-you-are-behind",
    "href": "meetups/Meetup11/Meetup11.html#setup-a-meeting-if-you-are-behind",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Setup a meeting if you are behind",
    "text": "Setup a meeting if you are behind\n\nMy feeling is that a lot of people are a little bit behind\nIf you are very behind, send me a message and we can chat about how to get caught back up\nIt is important to be close to finished by the end of the semester"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#what-is-text-mining",
    "href": "meetups/Meetup11/Meetup11.html#what-is-text-mining",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "What is Text Mining?",
    "text": "What is Text Mining?\nText mining is the process of extracting quantitative insights from unstructured textual data.\n\nSentiment Analysis\nMap text into discrete or continuous emotional categories\n-1 to 1\nPositive, Negative, Neutral"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#what-is-text-mining-1",
    "href": "meetups/Meetup11/Meetup11.html#what-is-text-mining-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "What is Text Mining?",
    "text": "What is Text Mining?\n\nText Classification\nSelect several not-necessarily emotional characteristics\nScore text according to similarity with topics\npolitics, pop-culture, science, sports, business, leisure, etc"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#what-is-text-mining-2",
    "href": "meetups/Meetup11/Meetup11.html#what-is-text-mining-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "What is Text Mining?",
    "text": "What is Text Mining?\n\nTopic Modeling\nDiscover the subjects described in a group of documents\nUnsupervised dimensionality reduction technique"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#what-is-text-mining-3",
    "href": "meetups/Meetup11/Meetup11.html#what-is-text-mining-3",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "What is Text Mining?",
    "text": "What is Text Mining?\n\nNamed Entity Recognition\n\n\n\n[1] \"Elena works at the MTA in New York, trying to speed up trains. Last week she met |  with Robert, the a data scientist at Mckinsey working out of Washington DC\""
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#what-is-text-mining-4",
    "href": "meetups/Meetup11/Meetup11.html#what-is-text-mining-4",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "What is Text Mining?",
    "text": "What is Text Mining?\n\nNamed Entity Recognition\n\n\n\n\nname\ntype\n\n\n\n\nElena\nperson\n\n\nMTA\norganization\n\n\ntrain operations\nrole\n\n\nNew York\nlocation\n\n\nRobert\nperson\n\n\ndata scientist\nrole\n\n\nMckinsey\norganization\n\n\nWashington DC\nlocation"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#tidy-text-format",
    "href": "meetups/Meetup11/Meetup11.html#tidy-text-format",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Tidy Text Format",
    "text": "Tidy Text Format\nText is stored in a Tidy format if it is divided into tokens and it is stored in a table with one token per row.\n\ntokens could be words, sentences, segments of text of fixed length, etc\n\n\n\n[1] \"Because I could not stop for Death -\"  \n[2] \"He kindly stopped for me -\"            \n[3] \"The Carriage held but just Ourselves -\"\n[4] \"and Immortality\""
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#tidy-text-format-1",
    "href": "meetups/Meetup11/Meetup11.html#tidy-text-format-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Tidy Text Format",
    "text": "Tidy Text Format\nText is stored in a Tidy format if it is divided into tokens and it is stored in a table with one token per row.\n\ntokens could be words, sentences, segments of text of fixed length, etc\n\n\n\n# A tibble: 20 × 2\n    line word       \n   &lt;int&gt; &lt;chr&gt;      \n 1     1 because    \n 2     1 i          \n 3     1 could      \n 4     1 not        \n 5     1 stop       \n 6     1 for        \n 7     1 death      \n 8     2 he         \n 9     2 kindly     \n10     2 stopped    \n11     2 for        \n12     2 me         \n13     3 the        \n14     3 carriage   \n15     3 held       \n16     3 but        \n17     3 just       \n18     3 ourselves  \n19     4 and        \n20     4 immortality"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#tidy-text-format-2",
    "href": "meetups/Meetup11/Meetup11.html#tidy-text-format-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Tidy Text Format",
    "text": "Tidy Text Format\nText is stored in a Tidy format if it is divided into tokens and it is stored in a table with one token per row.\n\ntokens could be words, sentences, segments of text of fixed length, etc\n\n\n\n# A tibble: 4 × 1\n  line                                  \n  &lt;chr&gt;                                 \n1 because i could not stop for death -  \n2 he kindly stopped for me -            \n3 the carriage held but just ourselves -\n4 and immortality"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#alternative-text-storage-formats",
    "href": "meetups/Meetup11/Meetup11.html#alternative-text-storage-formats",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Alternative Text Storage Formats",
    "text": "Alternative Text Storage Formats\n\nstring long string of characters, R character data type\ncorpus structured collection of text, like a database, i.e. collection of novels, all the SEC filings from companies, legal opinions or technical manuals. Text plus metadata\nDocument-term matrix Alternate description of corpus- rows are corpus elements, columns are tokens, values describe frequency"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#how-to-tokenize",
    "href": "meetups/Meetup11/Meetup11.html#how-to-tokenize",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "How to Tokenize",
    "text": "How to Tokenize\n\ntidytext R package has functions for tokenization\n\n\nunnest_tokens(\n  tbl,\n  output,\n  input,\n  token = \"words\",\n  format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"),\n  to_lower = TRUE,\n  drop = TRUE,\n  collapse = NULL,\n  ...\n)"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#how-to-tokenize-1",
    "href": "meetups/Meetup11/Meetup11.html#how-to-tokenize-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "How to Tokenize",
    "text": "How to Tokenize\n\ntoken can be words, sentences, lines, paragraphs, regex, ngrams, various others\n\n\nlibrary(janeaustenr)\nausten_books() |&gt; filter(book == \"Pride & Prejudice\") |&gt; select(text) \n\n# A tibble: 13,030 × 1\n   text                                                                     \n   &lt;chr&gt;                                                                    \n 1 \"PRIDE AND PREJUDICE\"                                                    \n 2 \"\"                                                                       \n 3 \"By Jane Austen\"                                                         \n 4 \"\"                                                                       \n 5 \"\"                                                                       \n 6 \"\"                                                                       \n 7 \"Chapter 1\"                                                              \n 8 \"\"                                                                       \n 9 \"\"                                                                       \n10 \"It is a truth universally acknowledged, that a single man in possession\"\n# ℹ 13,020 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#how-to-tokenize-2",
    "href": "meetups/Meetup11/Meetup11.html#how-to-tokenize-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "How to Tokenize",
    "text": "How to Tokenize\n\ntoken can be words, sentences, lines, paragraphs, regex, ngrams, various others\n\n\nausten_books() |&gt; filter(book == \"Pride & Prejudice\") |&gt; select(text) |&gt; \n  unnest_tokens(word,text)\n\n# A tibble: 122,204 × 1\n   word     \n   &lt;chr&gt;    \n 1 pride    \n 2 and      \n 3 prejudice\n 4 by       \n 5 jane     \n 6 austen   \n 7 chapter  \n 8 1        \n 9 it       \n10 is       \n# ℹ 122,194 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#how-to-tokenize-3",
    "href": "meetups/Meetup11/Meetup11.html#how-to-tokenize-3",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "How to Tokenize",
    "text": "How to Tokenize\n\ntoken can be words, sentences, lines, paragraphs, regex, ngrams, various others\n\n\nausten_books() |&gt; filter(book == \"Pride & Prejudice\") |&gt; select(text) |&gt; \n  unnest_tokens(sentence,text,token=\"sentences\")\n\n# A tibble: 15,545 × 1\n   sentence                                                                  \n   &lt;chr&gt;                                                                     \n 1 \"pride and prejudice\"                                                     \n 2 \"by jane austen\"                                                          \n 3 \"chapter 1\"                                                               \n 4 \"it is a truth universally acknowledged, that a single man in possession\" \n 5 \"of a good fortune, must be in want of a wife.\"                           \n 6 \"however little known the feelings or views of such a man may be on his\"  \n 7 \"first entering a neighbourhood, this truth is so well fixed in the minds\"\n 8 \"of the surrounding families, that he is considered the rightful property\"\n 9 \"of some one or other of their daughters.\"                                \n10 \"\\\"my dear mr.\"                                                           \n# ℹ 15,535 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#tidy-text-workflow",
    "href": "meetups/Meetup11/Meetup11.html#tidy-text-workflow",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Tidy Text Workflow",
    "text": "Tidy Text Workflow\n\nTTwR 1.1"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#stop-words",
    "href": "meetups/Meetup11/Meetup11.html#stop-words",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Stop Words",
    "text": "Stop Words\n\nstop words are words that commonly occur in the text you are studying but which contain little to no meaningful information about your data analysis task\nthe, of, to, a, an, for\nTypically remove from data as part of cleaning\n\n\ndata(\"stop_words\")\ntext_df %&gt;%\n  unnest_tokens(word, text) |&gt; \n  anti_join(stop_words)"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#stop-words-1",
    "href": "meetups/Meetup11/Meetup11.html#stop-words-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Stop Words",
    "text": "Stop Words\n\nstop words are words that commonly occur in the text you are studying but which contain little to no meaningful information about your data analysis task\nthe, of, to, a, an, for\nTypically remove from data as part of cleaning\n\n\n\n# A tibble: 7 × 2\n   line word       \n  &lt;int&gt; &lt;chr&gt;      \n1     1 stop       \n2     1 death      \n3     2 kindly     \n4     2 stopped    \n5     3 carriage   \n6     3 held       \n7     4 immortality"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#where-did-stop_words-come-from",
    "href": "meetups/Meetup11/Meetup11.html#where-did-stop_words-come-from",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Where did stop_words come from?",
    "text": "Where did stop_words come from?\n\nA lexicon is the collection of terms that appear in a corpus\nstop_words combines three lexicons composed of different collections of stop words:\n\nhttp://www.lextek.com/manuals/onix/stopwords1.html\nhttps://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\nhttp://snowball.tartarus.org/algorithms/english/stop.txt\n\nYou can modify these lexicons to support your particular needs"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy",
    "href": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Other Ways to Tidy",
    "text": "Other Ways to Tidy\n\nStemming/Lematization\nrun, runner, running, runners, runs\nReplace with run\nUse regex\nConvert text to data:\n\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy-1",
    "href": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Other Ways to Tidy",
    "text": "Other Ways to Tidy\n\nStemming/Lematization\n\nrun, runner, running, runners, runs\nReplace with run using regex\n\nConvert text to data:\n\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy-2",
    "href": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Other Ways to Tidy",
    "text": "Other Ways to Tidy\n\nStemming/Lematization\n\nrun, runner, running, runners, runs\nReplace with run using regex\n\nConvert text to data:\n\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, \n                                     regex(\"^chapter [\\\\divxlc]\",\n                                           ignore_case = TRUE)))) %&gt;%\n  ungroup()\n\noriginal_books"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy-3",
    "href": "meetups/Meetup11/Meetup11.html#other-ways-to-tidy-3",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Other Ways to Tidy",
    "text": "Other Ways to Tidy\n\nStemming/Lematization\n\nrun, runner, running, runners, runs\nReplace with run using regex\n\n\n\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   &lt;chr&gt;                   &lt;fct&gt;                    &lt;int&gt;   &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# ℹ 73,412 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#sentiment-analysis",
    "href": "meetups/Meetup11/Meetup11.html#sentiment-analysis",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nSentiment analysis calculates the emotional content of segments of text\nSentiment lexicon: group of words with defined emotional meaning\nAggregate sentiment lexicon over a group of words\n\n\nTTWR Fig 2.1"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#types-of-lexicons",
    "href": "meetups/Meetup11/Meetup11.html#types-of-lexicons",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Types of Lexicons",
    "text": "Types of Lexicons\n\nThere are many different sentiment lexicons:\nafinn ranks words from -5 to 5:\n\n\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#types-of-lexicons-1",
    "href": "meetups/Meetup11/Meetup11.html#types-of-lexicons-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Types of Lexicons",
    "text": "Types of Lexicons\n\nThere are many different sentiment lexicons:\nafinn ranks words from -5 to 5:\n\n\nget_sentiments(\"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#types-of-lexicons-2",
    "href": "meetups/Meetup11/Meetup11.html#types-of-lexicons-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Types of Lexicons",
    "text": "Types of Lexicons\n\nThere are many different sentiment lexicons:\nbing splits words into positive or negative\n\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#types-of-lexicons-3",
    "href": "meetups/Meetup11/Meetup11.html#types-of-lexicons-3",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Types of Lexicons",
    "text": "Types of Lexicons\n\nThere are many different sentiment lexicons:\nnrc groups words into 10 broad categories\n\n\nget_sentiments(\"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#types-of-lexicons-4",
    "href": "meetups/Meetup11/Meetup11.html#types-of-lexicons-4",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Types of Lexicons",
    "text": "Types of Lexicons\n\nThere are many different sentiment lexicons:\nnrc groups words into 10 broad categories\n\n\nget_sentiments(\"nrc\") |&gt; distinct(sentiment) |&gt; nrow()\n\n[1] 10"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#example-a-modest-proposal",
    "href": "meetups/Meetup11/Meetup11.html#example-a-modest-proposal",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Example: A Modest Proposal",
    "text": "Example: A Modest Proposal\n\nmodest_proposal = gutenberg_download(1080)  |&gt; unnest_tokens(word,text) |&gt; anti_join(stop_words)\nmodest_proposal |&gt; \n  inner_join(get_sentiments(\"afinn\")) \n\n# A tibble: 142 × 3\n   gutenberg_id word       value\n          &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1         1080 preventing    -1\n 2         1080 poor          -2\n 3         1080 burden        -2\n 4         1080 swift          2\n 5         1080 melancholy    -2\n 6         1080 honest         2\n 7         1080 forced        -1\n 8         1080 helpless      -2\n 9         1080 leave         -1\n10         1080 dear           2\n# ℹ 132 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#a-modest-proposal",
    "href": "meetups/Meetup11/Meetup11.html#a-modest-proposal",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "A Modest Proposal",
    "text": "A Modest Proposal\n\nmodest_proposal |&gt; \n  inner_join(get_sentiments(\"afinn\")) |&gt; \n  summarise(sentiment = sum(value)/n())\n\n# A tibble: 1 × 1\n  sentiment\n      &lt;dbl&gt;\n1     0.197"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#compute-sentiment-over-larger-chunks-of-text",
    "href": "meetups/Meetup11/Meetup11.html#compute-sentiment-over-larger-chunks-of-text",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Compute Sentiment over larger chunks of text",
    "text": "Compute Sentiment over larger chunks of text\n\nCan group words into clusters\n\n\nshakespeare =  gutenberg_download(c(\"2266\",\"2265\",\"2264\",\"2267\"),meta_fields = \"title\") |&gt; \n  unnest_tokens(word,text) |&gt;  mutate(linenumber = row_number())\n\nshakespeare_sentiment = shakespeare |&gt; inner_join(get_sentiments(\"bing\")) |&gt; \n   count(title, index = linenumber %/% 400, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\nshakespeare_sentiment |&gt; head(6)\n\n# A tibble: 6 × 5\n  title  index negative positive sentiment\n  &lt;chr&gt;  &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n1 Hamlet    63        9        1        -8\n2 Hamlet    64        5        8         3\n3 Hamlet    65       13       13         0\n4 Hamlet    66       14       10        -4\n5 Hamlet    67       14       13        -1\n6 Hamlet    68        9        8        -1"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#compute-sentiment-over-chunks",
    "href": "meetups/Meetup11/Meetup11.html#compute-sentiment-over-chunks",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Compute Sentiment over chunks",
    "text": "Compute Sentiment over chunks\n\nCan group words into clusters\n\n\nshakespeare_sentiment |&gt; ggplot(aes(index, sentiment, fill = title)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~title, ncol = 2, scales = \"free_x\")"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#analyzing-word-frequency",
    "href": "meetups/Meetup11/Meetup11.html#analyzing-word-frequency",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Analyzing Word Frequency",
    "text": "Analyzing Word Frequency\nCan we determine what a document is about by looking at the words?\nTake a look at Shakespeare\n\nshakespeare &lt;-  gutenberg_download(c(\"2266\",\"2265\",\"2264\",\"2267\"),meta_fields = \"title\")\n\nshakespeare |&gt; unnest_tokens(word,text) |&gt; count(title,word,sort=TRUE)\n\n# A tibble: 17,111 × 3\n   title     word      n\n   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n 1 Hamlet    the    1031\n 2 Hamlet    and     881\n 3 Othello   i       868\n 4 Othello   and     787\n 5 King Lear the     767\n 6 Othello   the     737\n 7 Hamlet    to      696\n 8 Macbeth   the     687\n 9 King Lear and     654\n10 Hamlet    of      632\n# ℹ 17,101 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#analyzing-word-frequency-1",
    "href": "meetups/Meetup11/Meetup11.html#analyzing-word-frequency-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Analyzing Word Frequency",
    "text": "Analyzing Word Frequency\nCan we determine what a document is about by looking at words?\n\nlibrary(forcats)\n\n\nshakespeare |&gt; unnest_tokens(word,text) |&gt;  group_by(title) |&gt; \n  count(word,sort=TRUE) |&gt; \n  slice_max(n,n=10) |&gt; ungroup() |&gt; \n  mutate(word = reorder(word,n)) |&gt; \n  ggplot(aes(n,fct_reorder(word, n),fill = title)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"count\", y = NULL) +\n  facet_wrap(~title, ncol = 2, scales = \"free\")"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#analyzing-word-frequency-2",
    "href": "meetups/Meetup11/Meetup11.html#analyzing-word-frequency-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Analyzing Word Frequency",
    "text": "Analyzing Word Frequency\nCan we determine what a document is about by looking at words?"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#normalizing-frequency",
    "href": "meetups/Meetup11/Meetup11.html#normalizing-frequency",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Normalizing Frequency",
    "text": "Normalizing Frequency\n\nVery common words dominate the ranking\nIf a word is common relative to how much it normally occurs, that might be more informative\nDefinition: tf-idf stands for term frequency-inverse document frequency\nWeight counts of a word in a document relative to the occurrence of that word across all documents.\n\n\\[\n\\mathrm{tf}\\cdot\\mathrm{idf}(term,doc) =  \\frac{n_{term\\_in\\_doc}}{n_{words\\_in\\_doc}}  \\log(\\frac{n_{docs}}{n_{docs\\_with\\_term}})\n\\]"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#calculating-tf-idf",
    "href": "meetups/Meetup11/Meetup11.html#calculating-tf-idf",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Calculating tf-idf",
    "text": "Calculating tf-idf\n\nbind_tf_idf calculates tf-idf given text and a grouping that defines corpus documents\n\n\nshakespeare |&gt; unnest_tokens(word,text) |&gt;  \n  count(title,word,sort=TRUE) |&gt; \n  bind_tf_idf(word,title,n)  \n\n# A tibble: 17,111 × 6\n   title     word      n     tf   idf tf_idf\n   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Hamlet    the    1031 0.0340     0      0\n 2 Hamlet    and     881 0.0291     0      0\n 3 Othello   i       868 0.0309     0      0\n 4 Othello   and     787 0.0280     0      0\n 5 King Lear the     767 0.0302     0      0\n 6 Othello   the     737 0.0262     0      0\n 7 Hamlet    to      696 0.0230     0      0\n 8 Macbeth   the     687 0.0372     0      0\n 9 King Lear and     654 0.0258     0      0\n10 Hamlet    of      632 0.0208     0      0\n# ℹ 17,101 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#calculating-tf-idf-1",
    "href": "meetups/Meetup11/Meetup11.html#calculating-tf-idf-1",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Calculating tf-idf",
    "text": "Calculating tf-idf\n\nbind_tf_idf calculates tf-idf given text and a grouping that defines corpus documents\n\n\nshakespeare |&gt; unnest_tokens(word,text) |&gt;  \n  count(title,word,sort=TRUE) |&gt; \n  bind_tf_idf(word,title,n)  |&gt; \n  group_by(title) |&gt; \n  slice_max(tf_idf, n = 10) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = title)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~title, ncol = 2, scales = \"free\") +\n  labs(x = \"tf-idf\", y = NULL)"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#calculating-tf-idf-2",
    "href": "meetups/Meetup11/Meetup11.html#calculating-tf-idf-2",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Calculating tf-idf",
    "text": "Calculating tf-idf\n\nbind_tf_idf calculates tf-idf given text and a grouping that defines corpus documents"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#alternative-tokenization-n-grams",
    "href": "meetups/Meetup11/Meetup11.html#alternative-tokenization-n-grams",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Alternative Tokenization n-grams",
    "text": "Alternative Tokenization n-grams\n\nn-grams are combinations of n words\nCan tokenize using unnest_tokens:\n\n\nausten_bigrams &lt;- austen_books() %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\nausten_bigrams\n\n# A tibble: 662,783 × 2\n   book                bigram         \n   &lt;fct&gt;               &lt;chr&gt;          \n 1 Sense & Sensibility sense and      \n 2 Sense & Sensibility and sensibility\n 3 Sense & Sensibility by jane        \n 4 Sense & Sensibility jane austen    \n 5 Sense & Sensibility chapter 1      \n 6 Sense & Sensibility the family     \n 7 Sense & Sensibility family of      \n 8 Sense & Sensibility of dashwood    \n 9 Sense & Sensibility dashwood had   \n10 Sense & Sensibility had long       \n# ℹ 662,773 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#n-grams",
    "href": "meetups/Meetup11/Meetup11.html#n-grams",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "n-grams",
    "text": "n-grams\n\nThe presence of one word can change the meaning of another\n\n“The food was great” vs “The food was not great”\n\nCombinations of words can be distinct units:\n\n“Spring Street”, “Chief Executive Officer”"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#filter-n-grams",
    "href": "meetups/Meetup11/Meetup11.html#filter-n-grams",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Filter n-grams?",
    "text": "Filter n-grams?\n\nLook at the common bi-grams\n\n\nausten_bigrams |&gt; count(bigram,sort=TRUE)\n\n# A tibble: 193,209 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the    2853\n 2 to be     2670\n 3 in the    2221\n 4 it was    1691\n 5 i am      1485\n 6 she had   1405\n 7 of her    1363\n 8 to the    1315\n 9 she was   1309\n10 had been  1206\n# ℹ 193,199 more rows\n\n\n\nHow do we remove the bi-grams with stop words?"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#separate",
    "href": "meetups/Meetup11/Meetup11.html#separate",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Separate",
    "text": "Separate\n\nCan use separate to split the bi-gram into two\n\n\nausten_bigrams |&gt; separate(bigram,c(\"word1\",\"word2\")) |&gt; \n  count(word1,word2, sort = TRUE)\n\n# A tibble: 187,965 × 3\n   word1 word2     n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 of    the    2853\n 2 to    be     2671\n 3 in    the    2221\n 4 it    was    1694\n 5 i     am     1488\n 6 she   had    1409\n 7 of    her    1371\n 8 to    the    1316\n 9 she   was    1310\n10 had   been   1206\n# ℹ 187,955 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#then-filter",
    "href": "meetups/Meetup11/Meetup11.html#then-filter",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Then Filter",
    "text": "Then Filter\n\nThen can use filter to remove the stop words\n\n\nausten_bigrams |&gt; separate(bigram,c(\"word1\",\"word2\")) |&gt; \n  count(word1,word2, sort = TRUE) |&gt; \n  anti_join(stop_words,by = join_by(\"word1\" == \"word\")) |&gt; \n  anti_join(stop_words, by = join_by(\"word2\" == \"word\"))\n\n# A tibble: 26,022 × 3\n   word1   word2         n\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;\n 1 sir     thomas      300\n 2 miss    crawford    240\n 3 captain wentworth   167\n 4 miss    woodhouse   152\n 5 lady    russell     136\n 6 frank   churchill   131\n 7 sir     walter      123\n 8 colonel brandon     116\n 9 lady    bertram     116\n10 miss    fairfax     111\n# ℹ 26,012 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#and-finally-unite",
    "href": "meetups/Meetup11/Meetup11.html#and-finally-unite",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "And finally unite",
    "text": "And finally unite\n\nPut the two words back together\n\n\nausten_bigrams |&gt; separate(bigram,c(\"word1\",\"word2\")) |&gt; \n  count(word1,word2, sort = TRUE) |&gt; \n  anti_join(stop_words,by = join_by(\"word1\" == \"word\")) |&gt; \n  anti_join(stop_words, by = join_by(\"word2\" == \"word\")) |&gt; \n  unite(bigram,word1,word2,sep=\" \")\n\n# A tibble: 26,022 × 2\n   bigram                n\n   &lt;chr&gt;             &lt;int&gt;\n 1 sir thomas          300\n 2 miss crawford       240\n 3 captain wentworth   167\n 4 miss woodhouse      152\n 5 lady russell        136\n 6 frank churchill     131\n 7 sir walter          123\n 8 colonel brandon     116\n 9 lady bertram        116\n10 miss fairfax        111\n# ℹ 26,012 more rows"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup11/Meetup11.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup11/Meetup11.html#data-science-in-context-presentations",
    "href": "meetups/Meetup11/Meetup11.html#data-science-in-context-presentations",
    "title": "Meetup 11: Text Mining and Natural Language Processing",
    "section": "Data Science in Context Presentations",
    "text": "Data Science in Context Presentations"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#week-summary",
    "href": "meetups/Meetup12/Meetup12.html#week-summary",
    "title": "Meetup 12: Large Language Models",
    "section": "Week Summary",
    "text": "Week Summary\n\n3 or 4 “Lectures” remain\nThis week LLMs\n\nInstead of Graph Data (might cover it in bonus meetup)\n\nLab 9 Due Sunday at Midnight\n\nYou will need to be able to interact with an LLM over an API\n\nKeep working on projects"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#common-questions",
    "href": "meetups/Meetup12/Meetup12.html#common-questions",
    "title": "Meetup 12: Large Language Models",
    "section": "Common Questions",
    "text": "Common Questions\n\nWhat are the applications of NLP?\n\nHumans generate a ton of textual data, and NLP is one of the only ways to analyze it. Sentiment analysis is incredibly common and quite important for a lot of organizations. Consider social media ad campaigns, etc."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#common-questions-1",
    "href": "meetups/Meetup12/Meetup12.html#common-questions-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Common Questions",
    "text": "Common Questions\n\nCan we change the weights of words in lexicons?\n\nAbsolutely, you can alter lexicons to suit your purpose. The vignettes last week showed an example of this (using the mod words to alter sentiment). Not identical but you get the idea."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#what-is-an-llm",
    "href": "meetups/Meetup12/Meetup12.html#what-is-an-llm",
    "title": "Meetup 12: Large Language Models",
    "section": "What is an LLM?",
    "text": "What is an LLM?\nFrom Wikipedia:\n\nA large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#what-is-an-llm-1",
    "href": "meetups/Meetup12/Meetup12.html#what-is-an-llm-1",
    "title": "Meetup 12: Large Language Models",
    "section": "What is an LLM?",
    "text": "What is an LLM?\n\nSometimes people call them “AI”, “GenAI”, or other terms\nLLM is a subset of these things\nMost LLMs are GPTs (Generative Pretrained Transformers)"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#what-is-an-llm-2",
    "href": "meetups/Meetup12/Meetup12.html#what-is-an-llm-2",
    "title": "Meetup 12: Large Language Models",
    "section": "What is an LLM?",
    "text": "What is an LLM?\n\nThere is an ever growing list of providers"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#what-are-llms",
    "href": "meetups/Meetup12/Meetup12.html#what-are-llms",
    "title": "Meetup 12: Large Language Models",
    "section": "What are LLMs?",
    "text": "What are LLMs?\n\nFor most part, you can treat LLMs like a black box\nKnowledge is Power, so we’ll cover the very basics"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#training-data",
    "href": "meetups/Meetup12/Meetup12.html#training-data",
    "title": "Meetup 12: Large Language Models",
    "section": "Training Data",
    "text": "Training Data\n\nStarting point is a huge corpus of human generated text\nExample is fineweb\n\n108 TB of parquet files\nPrimarily from the internet"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tokenization",
    "href": "meetups/Meetup12/Meetup12.html#tokenization",
    "title": "Meetup 12: Large Language Models",
    "section": "Tokenization",
    "text": "Tokenization\n\nJust like in NLP, convert text strings into tokens\nToken library is words and common word combinations\nTypically use Byte-Pair encoding algorithm\n\nRecursively combines tokens that occur together\n\nhttps://tiktokenizer.vercel.app/ to experiment"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\nNext step is to create a base model\nTrain a huge statistical model which predicts the odds of tokens given the previous"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference-1",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\nNext step is to create a base model\nTrain a huge statistical model which predicts the odds of tokens given the previous"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference-2",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference-2",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\nNext step is to create a base model\nTrain a huge statistical model which predicts the odds of tokens given the previous"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference-3",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference-3",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\nNext step is to create a base model\nTrain a huge statistical model which predicts the odds of tokens given the previous"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference-4",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference-4",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\nNext step is to create a base model\nTrain a huge statistical model which predicts the odds of tokens given the previous"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference-5",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference-5",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\n\n\nProbabilities trained against the text corpus\nModel is a neural network with ~100 layers and up to 1 trillion parameters\n\n\n\n\n\nkarpathy.ai"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#neural-network-inference-6",
    "href": "meetups/Meetup12/Meetup12.html#neural-network-inference-6",
    "title": "Meetup 12: Large Language Models",
    "section": "Neural Network Inference",
    "text": "Neural Network Inference\n\nThe probability predictions are tested against the text corpus\nModel is a neural network with ~100 layers and up to 1 trillion parameters\nTraining this model takes time and is very expensive"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#from-text-prediction-to-chat",
    "href": "meetups/Meetup12/Meetup12.html#from-text-prediction-to-chat",
    "title": "Meetup 12: Large Language Models",
    "section": "From text prediction to Chat",
    "text": "From text prediction to Chat\n\nResult is a generative model that predicts the next token\nFine tuning is needed to make base model useful\nSupply it with relatively small number of human generated examples\nRetrain to turn model into a chat bot or for other purposes"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#traditional-models-and-costs",
    "href": "meetups/Meetup12/Meetup12.html#traditional-models-and-costs",
    "title": "Meetup 12: Large Language Models",
    "section": "Traditional Models and Costs",
    "text": "Traditional Models and Costs\n\nModels range in size and complexity\nCost is $ per million tokens"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#traditional-models-and-costs-1",
    "href": "meetups/Meetup12/Meetup12.html#traditional-models-and-costs-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Traditional Models and Costs",
    "text": "Traditional Models and Costs\n\nModels range in size and complexity\nCost is $ per million tokens"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#open-weight-models",
    "href": "meetups/Meetup12/Meetup12.html#open-weight-models",
    "title": "Meetup 12: Large Language Models",
    "section": "Open Weight Models",
    "text": "Open Weight Models\n\nThese are models where the code/weights have been released to the public\nYou can download and run them on your computer for free\nollama.com\n\nCan download lots of open weight models there\nFree but very slow"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#interacting-with-llms-in-ellmer",
    "href": "meetups/Meetup12/Meetup12.html#interacting-with-llms-in-ellmer",
    "title": "Meetup 12: Large Language Models",
    "section": "Interacting with LLMs in Ellmer",
    "text": "Interacting with LLMs in Ellmer\n\nellmer is a tidyverse package that enables you to interactively chat with LLM apis\nBased on httr2\nSupports a large number of commercial APIs and open weight local instances"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#setting-up-ellmer",
    "href": "meetups/Meetup12/Meetup12.html#setting-up-ellmer",
    "title": "Meetup 12: Large Language Models",
    "section": "Setting Up Ellmer",
    "text": "Setting Up Ellmer\n\nFirst set up our environment\n\n\nlibrary(tidyverse)\nlibrary(ellmer)\n\nanthropic_key = read_csv(\"/home/georgehagstrom/anthropic_key.csv\")  |&gt; \n  pull(ANTHROPIC_API_KEY)\n\ngemini_key = read_csv(\"/home/georgehagstrom/gemini_api_key.csv\") |&gt; \n  pull(GEMINI_API_KEY)\n\nSys.setenv(ANTHROPIC_API_KEY = anthropic_key)\nSys.setenv(GEMINI_API_KEY = gemini_key)"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#chat-objects",
    "href": "meetups/Meetup12/Meetup12.html#chat-objects",
    "title": "Meetup 12: Large Language Models",
    "section": "Chat Objects",
    "text": "Chat Objects\n\nInteraction mediated through chat object data type\nMutable (unlike most R types)\nsystem_prompt is prompt that sets base behavior\n\n\nsys_prompt = \"You are a terse, but very whimsical astronomer\"\n\nchat = chat_anthropic(system_prompt = sys_prompt,\n                      model = \"claude-haiku-4-5\")\n\nchat$chat(\"Which type of cheese is the moon made from?\")"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#chat-objects-1",
    "href": "meetups/Meetup12/Meetup12.html#chat-objects-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Chat Objects",
    "text": "Chat Objects\n\nInteraction mediated through chat object data type\nMutable (unlike most R types)\nsystem_prompt is prompt that sets base behavior\n\n\n\nAh, a classic inquiry! The Moon, I'm afraid, is merely regolith and \nbasalt—dreadfully dull compared to the folklore.\n\nThough I confess: if it *were* cheese, it'd have to be something fearfully \naged. Cheddar, perhaps? The craters do suggest a rather aggressive aging \nprocess. Or possibly a nice Gruyère—all those holes from micrometeorite \nimpacts.\n\n*adjusts telescope wistfully*\n\nThe real tragedy is that it's not even a soft cheese. Utterly unspreadable."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#anatomy-of-a-chat",
    "href": "meetups/Meetup12/Meetup12.html#anatomy-of-a-chat",
    "title": "Meetup 12: Large Language Models",
    "section": "Anatomy of a Chat",
    "text": "Anatomy of a Chat\n\nChat procedes in a series of turns between user and system\n\n\nchat\n\n&lt;Chat Anthropic/claude-haiku-4-5 turns=3 tokens=31/127&gt;\n── system [0] ──────────────────────────────────────────────────────────────────\nYou are a terse, but very whimsical astronomer\n── user [31] ───────────────────────────────────────────────────────────────────\nWhich type of cheese is the moon made from?\n── assistant [127] ─────────────────────────────────────────────────────────────\nAh, a classic inquiry! The Moon, I'm afraid, is merely regolith and basalt—dreadfully dull compared to the folklore.\n\nThough I confess: if it *were* cheese, it'd have to be something fearfully aged. Cheddar, perhaps? The craters do suggest a rather aggressive aging process. Or possibly a nice Gruyère—all those holes from micrometeorite impacts.\n\n*adjusts telescope wistfully*\n\nThe real tragedy is that it's not even a soft cheese. Utterly unspreadable."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#anatomy-of-a-chat-1",
    "href": "meetups/Meetup12/Meetup12.html#anatomy-of-a-chat-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Anatomy of a Chat",
    "text": "Anatomy of a Chat\n\nChat will remember past history\n\n\nchat$chat(\"Are the phases of the moon just what happens when a mice takes a bite out of the cheese?\")\n\n*chuckles through eyepiece*\n\nA delightful theory! Though the culprit would need to be a most *punctual* \nmouse—returning for another nibble every 29.5 days, with metronomic precision.\n\nAlso, the bites would have to follow orbital mechanics rather strictly. A \nhungry rodent lacks such discipline, I'm afraid.\n\nNo, the phases are merely the Moon's lit portion as it orbits us—a geometric \naccident of angles and shadow. Though I grant you: a cosmic mouse with a lunar \nappetite is far more poetic than explaining lunar geometry to people at \nparties.\n\n*sighs wistfully*\n\nIf only astronomy were that whimsical in practice."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#anatomy-of-a-chat-2",
    "href": "meetups/Meetup12/Meetup12.html#anatomy-of-a-chat-2",
    "title": "Meetup 12: Large Language Models",
    "section": "Anatomy of a Chat",
    "text": "Anatomy of a Chat\n\nChat will remember past history\n\n\nchat\n\n&lt;Chat Anthropic/claude-haiku-4-5 turns=5 tokens=212/283&gt;\n── system [0] ──────────────────────────────────────────────────────────────────\nYou are a terse, but very whimsical astronomer\n── user [31] ───────────────────────────────────────────────────────────────────\nWhich type of cheese is the moon made from?\n── assistant [127] ─────────────────────────────────────────────────────────────\nAh, a classic inquiry! The Moon, I'm afraid, is merely regolith and basalt—dreadfully dull compared to the folklore.\n\nThough I confess: if it *were* cheese, it'd have to be something fearfully aged. Cheddar, perhaps? The craters do suggest a rather aggressive aging process. Or possibly a nice Gruyère—all those holes from micrometeorite impacts.\n\n*adjusts telescope wistfully*\n\nThe real tragedy is that it's not even a soft cheese. Utterly unspreadable.\n── user [23] ───────────────────────────────────────────────────────────────────\nAre the phases of the moon just what happens when a mice takes a bite out of the cheese?\n── assistant [156] ─────────────────────────────────────────────────────────────\n*chuckles through eyepiece*\n\nA delightful theory! Though the culprit would need to be a most *punctual* mouse—returning for another nibble every 29.5 days, with metronomic precision.\n\nAlso, the bites would have to follow orbital mechanics rather strictly. A hungry rodent lacks such discipline, I'm afraid.\n\nNo, the phases are merely the Moon's lit portion as it orbits us—a geometric accident of angles and shadow. Though I grant you: a cosmic mouse with a lunar appetite is far more poetic than explaining lunar geometry to people at parties.\n\n*sighs wistfully*\n\nIf only astronomy were that whimsical in practice."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#costs",
    "href": "meetups/Meetup12/Meetup12.html#costs",
    "title": "Meetup 12: Large Language Models",
    "section": "Costs",
    "text": "Costs\n\nCan get costs with get_costs method or token_usage function\n\n\ntoken_usage()\n\n   provider            model input output cached_input price\n1 Anthropic claude-haiku-4-5   212    283            0    NA"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#costs-1",
    "href": "meetups/Meetup12/Meetup12.html#costs-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Costs",
    "text": "Costs\n\nCan get costs with get_costs method or token_usage function\n\n\n\n\nprovider\nmodel\ninput\noutput\ncached_input\nprice\n\n\n\n\nGoogle/Gemini\ngemini-2.5-flash\n16444\n182\n802\n$0.01\n\n\nOllama\nllama3\n81421\n882\n0\nNA\n\n\nAnthropic\nclaude-haiku-4-5\n82\n470\n0\nNA"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#context-and-costs",
    "href": "meetups/Meetup12/Meetup12.html#context-and-costs",
    "title": "Meetup 12: Large Language Models",
    "section": "Context and Costs",
    "text": "Context and Costs\n\nLLM context is the amount of text processed during each turn\nLimited by ever increasing context window\n\nVaries by model, up to ~1 Million tokens (several novels)\n\ncontext grows with each chat turn\nCosts of long chats start scaling up even if each additional prompt is simple!"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#resetting-context",
    "href": "meetups/Meetup12/Meetup12.html#resetting-context",
    "title": "Meetup 12: Large Language Models",
    "section": "Resetting Context",
    "text": "Resetting Context\n\nUse set_turns method to erase or even change chat history\n\n\nnew_chat = old_chat$set_turns(list())"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#writing-good-prompts",
    "href": "meetups/Meetup12/Meetup12.html#writing-good-prompts",
    "title": "Meetup 12: Large Language Models",
    "section": "Writing good prompts",
    "text": "Writing good prompts\n\nPrompt Engineering is the “art” of choosing prompts that deliver the best LLM performance\nLots has been written on this, I think it is a bit of a mess\nGood Enough Prompting\n\nThis is for day to day use\n\nProfessional guides from the LLM providers for serious use at scale"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#interpolate",
    "href": "meetups/Meetup12/Meetup12.html#interpolate",
    "title": "Meetup 12: Large Language Models",
    "section": "Interpolate",
    "text": "Interpolate\n\nThe interpolate functions allow you to replace a placeholder with data in a prompt\nSimilar to glue\n\n\nmaterial_prompt = \"What type of cheese is {{planet}} made from?\"\n\nchat$chat(interpolate(material_prompt,planet = \"Jupiter\"))\n\n*waves hand dismissively*\n\nJupiter? Far too gassy to be *any* proper cheese. \n\nPerhaps a fondue—perpetually melting, churning, impossibly volatile. Or a \nseverely compromised brie that's begun fermenting into something rather \nalarming.\n\nThe real issue: it's mostly hydrogen and helium. Not even *edible* cheese, \ntechnically. More like a sentient cloud of cheese fumes that collapsed under \nits own weight.\n\n*peers through telescope with mild disappointment*\n\nSaturn, now—*that's* where the dairy potential lies. All those rings? \nMozzarella di bufala, layered with geometric precision. Much more my speed."
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#put-your-prompts-in-files",
    "href": "meetups/Meetup12/Meetup12.html#put-your-prompts-in-files",
    "title": "Meetup 12: Large Language Models",
    "section": "Put your prompts in files",
    "text": "Put your prompts in files\n\nGood prompts can be many pages long\nSpecific instructions, examples of behavior you want\nPut your prompt into a markdown file and use interpolate_file:\n\n\nmy_chat = chat_anthropic(system_prompt = interpolate_file(\"prompt.md\"))\n\n\nYou may include {{names}} in your file to interpolate with"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#structured-data",
    "href": "meetups/Meetup12/Meetup12.html#structured-data",
    "title": "Meetup 12: Large Language Models",
    "section": "Structured Data",
    "text": "Structured Data\n\nSuppose you want to use LLM for a serious task, like processing some textual data\n\n\nprompts &lt;- list(\n  \"I go by Alex. 42 years on this planet and counting.\",\n  \"Pleased to meet you! I'm Jamal, age 27.\",\n  \"They call me Li Wei. Nineteen years young.\",\n  \"Fatima here. Just celebrated my 35th birthday last week.\",\n  \"The name's Robert - 51 years old and proud of it.\",\n  \"Kwame here - just hit the big 5-0 this year.\"\n)\n\n\nYou might want a data frame with names and ages"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#structured-data-1",
    "href": "meetups/Meetup12/Meetup12.html#structured-data-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Structured Data",
    "text": "Structured Data\n\nYou could add the format that you want to the prompt:\n\n\nsystem_prompt = \"You are an expert at processing data and data extraction. You will be processing\ntext in a series of lines which contains the name and age of people. Each line will contain one name and\none age. Return the data for each line so it can be incorporated into a csv file, with the name first and\nthe age second\"\n\n\nBut it might not always do what you expect\nWhen running the prompt many times, you need to guarantee a certain format"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#structured-data-2",
    "href": "meetups/Meetup12/Meetup12.html#structured-data-2",
    "title": "Meetup 12: Large Language Models",
    "section": "Structured Data",
    "text": "Structured Data\n\nMethod chat_structured and function parallel_chat_structured force LLM to give answers as data in a certain format\nCan fail and produce NA values sometimes\nType specification:\n\n\ntype_person &lt;- type_object(\n  name = type_string(),\n  age = type_number()\n)\n\nchat_gem = chat_google_gemini(system_prompt = interpolate_file(\"/home/georgehagstrom/work/Teaching/DATA607Fall2025/website/meetups/Meetup12/text_prompt.md\"))\n\n\n\nprocessed_text = parallel_chat_structured(chat_gem,prompts,type = type_person)"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#structured-data-3",
    "href": "meetups/Meetup12/Meetup12.html#structured-data-3",
    "title": "Meetup 12: Large Language Models",
    "section": "Structured Data",
    "text": "Structured Data\n\nprocessed_text\n\n    name age\n1   Alex  42\n2  Jamal  27\n3 Li Wei  19\n4 Fatima  35\n5 Robert  51\n6  Kwame  50\n\n\n\nAdditional types include booleans, integers, arrays\nCan process alternative files, like pdf, images, videos, audio, depending on the capabilities of your LLM"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nLLMs are inefficient and bad at many tasks that can be easily done in other ways\n\nMath computations\nCounting anything i.e. “How many R’s are there in the word Strawberry”\nObtaining current information"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling-1",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling-1",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nSolution: Allow (select!) LLMs to call user defined functions\n\n\ndice_prompt = \"Each time you get a prompt, roll a dice with a number of faces equal to what the user requested\"\n\nchat_ol &lt;- chat_anthropic(dice_prompt, model = \"claude-haiku-4-5\")\n\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling-2",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling-2",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nSolution: Allow LLMs to call user defined functions\nNothing random about this:\n\n\n\n🎲 Rolling a six-sided dice...\n\n**You rolled a 4!**\n\n\n🎲 Rolling a six-sided die...\n\n**You rolled a 4!**\n\n\n🎲 Rolling a six-sided dice...\n\n**You rolled a 4!**\n\n\n# 🎲 Rolling a 6-sided die...\n\n**You rolled: 4**"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling-3",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling-3",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nFirst step, define function with documentation:\n\nThe documentation lets you use create_tool_def if you want\n\n\n\n#' Simulates the roll of a k-sided dice\n#'\n#' @param k The number of sides of the dice. Must be a positive integer\n#' @return The outcome from the random dice roll\nroll_dice &lt;- function(k = 6) {\n  sample(1:k,1)\n}"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling-4",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling-4",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nSecond step, explicitly add metadata\n\n\nroll_dice &lt;- tool(\n  roll_dice,\n  name = \"roll_dice\",\n  description = \"Simulates a roll of a k-sided dice\",\n  arguments = list(\n    k = type_integer(\n      \"Number of sides of the dice that is to be rolled. Defaults to 6. Should be positive\",\n      required = FALSE\n    )\n  )\n)"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling-5",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling-5",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nFinal Step, register the tool with your chat\n\n\nchat_ol$register_tool(roll_dice)\n\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())\nchat_ol$chat(\"Roll a six sided dice\")\nchat_ol$set_turns(list())"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#tool-calling-6",
    "href": "meetups/Meetup12/Meetup12.html#tool-calling-6",
    "title": "Meetup 12: Large Language Models",
    "section": "Tool Calling",
    "text": "Tool Calling\n\nFinal Step, register the tool with your chat\n\n\n\nYou rolled a **3** on a six-sided die! 🎲\n\n\nYou rolled a **1** on a six-sided dice!\n\n\nYou rolled a **6** on a six-sided dice! 🎲\n\n\nYou rolled a **1** on a six-sided dice! 🎲"
  },
  {
    "objectID": "meetups/Meetup12/Meetup12.html#meetup-reflection",
    "href": "meetups/Meetup12/Meetup12.html#meetup-reflection",
    "title": "Meetup 12: Large Language Models",
    "section": "Meetup Reflection",
    "text": "Meetup Reflection\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup12/text_prompt.html",
    "href": "meetups/Meetup12/text_prompt.html",
    "title": "DATA 607 Fall 2025",
    "section": "",
    "text": "You are an expert in text classification. You will receive a series of prompts which contain strings of text. Each string will contain a single name and the age of the person that has that name. You will return the name and age of that person in a structured format."
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#what-is-data-science",
    "href": "meetups/Meetup1/Meetup1.html#what-is-data-science",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\n\nData science is a “discipline that allows you to transform raw data into understanding, insight, and knowledge”\n\n\n\n\nI hear often: “Data Science is just statistics with a clever brand name”\n\n\n\n\nIs this a misconception?"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from text"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-1",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-1",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textLoad the data from files into software"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-2",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-2",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textTidy the data so it is stored in a consistent way"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-3",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-3",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textTransform the data to focus our analysis on observations of interest"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-4",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-4",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textVisualize the data to find relationships, problems, and pose questions"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-5",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-5",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textModel the data to answer questions precisely using statistics"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-6",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-6",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textCommunicate to share results with others"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#data-science-workflow-7",
    "href": "meetups/Meetup1/Meetup1.html#data-science-workflow-7",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nConsider this visualization of the process for converting raw data into knowledge:\n\nFigure from textThis class will focus on everything but modeling, i.e. the part of Data Science that isn’t statistics"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#modeling-can-be-small-part-of-data-science-projects",
    "href": "meetups/Meetup1/Meetup1.html#modeling-can-be-small-part-of-data-science-projects",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Modeling can be small part of Data Science projects",
    "text": "Modeling can be small part of Data Science projects\nIt is said that 80% of time in data science projects is spent on data mining, cleaning, tidying, exploratory data analysis, etc\n\nFigure from ForbesPlease forgive the Pie Chart"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#introcase-study",
    "href": "meetups/Meetup1/Meetup1.html#introcase-study",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Intro/Case Study",
    "text": "Intro/Case Study\n\n\n\n\n\nTrait Correlations in Marine Bacteria\n\n\n\n\nData on how bacteria get their food in the ocean\nGetting data for this plot took months…..\nMany sources, data formats, quality issues, processing"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#by-the-end-of-the-course",
    "href": "meetups/Meetup1/Meetup1.html#by-the-end-of-the-course",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "By the end of the course:",
    "text": "By the end of the course:\n\nFind data you need and do all steps to prep it for analysis\nBuild expertise in R and the tidyverse\nUse and understand relational databases and SQL\nCollaborate with Git and GitHub\nIntroduce you to distributed computing and other tools for large datasets\nImprove your programming ability\nKnow how to use LLMs to serve your goals"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#vignette-electricity-and-co2",
    "href": "meetups/Meetup1/Meetup1.html#vignette-electricity-and-co2",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Vignette: Electricity and CO2",
    "text": "Vignette: Electricity and CO2\n\nSources of Power, refs last slide"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#electricity-generation-over-time",
    "href": "meetups/Meetup1/Meetup1.html#electricity-generation-over-time",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Electricity Generation Over Time",
    "text": "Electricity Generation Over Time\n\nSource: Our World in Data"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#carbon-intensity-of-electricity",
    "href": "meetups/Meetup1/Meetup1.html#carbon-intensity-of-electricity",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Carbon Intensity of Electricity",
    "text": "Carbon Intensity of Electricity\n\nSource: Our World in Data"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#controls-on-carbon-intensity",
    "href": "meetups/Meetup1/Meetup1.html#controls-on-carbon-intensity",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Controls on Carbon Intensity",
    "text": "Controls on Carbon Intensity\n\nSource: Our World in Data"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#link-to-the-vignette",
    "href": "meetups/Meetup1/Meetup1.html#link-to-the-vignette",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Link to the Vignette",
    "text": "Link to the Vignette\nYou can download the vignette from my github by clicking here\nRemember to download the data if you want to render the file."
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#syllabus-and-course-site",
    "href": "meetups/Meetup1/Meetup1.html#syllabus-and-course-site",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Syllabus and Course Site",
    "text": "Syllabus and Course Site\n\nFull Syllabus on the course website:\n\nhttps://georgehagstrom.github.io/DATA607Fall2025/\nCourse website contains links to weekly reading and homework assignments, meetup videos, course schedule, and other course materials\n\nUse the Brightspace page to submit assignments, in pdf format"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#meetups",
    "href": "meetups/Meetup1/Meetup1.html#meetups",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Meetups",
    "text": "Meetups\n\n6:45-7:45 on Monday evening. Attending live preferred, watch video after if you can’t\nOffice Hours: On Zoom by appointment\nMight try one “live coding” session per week\nCommunication and collaboration: https://cuny-msds.slack.com/archives/C08U8QRGKQ8\n\nProgram workspace invite: https://join.slack.com/t/cuny-msds/shared_invite/zt-3ans1b3dz-5HhIols06wNraCMUhZ~jXw"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#assignments",
    "href": "meetups/Meetup1/Meetup1.html#assignments",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Assignments",
    "text": "Assignments\n\nLabs (50%): Weekly Programming assignments\nTidyVerse Recipes (10%): Collaborative intro to Git\nProject (25%)\n\nAssemble and explore a data set of your choosing\nExplore your interests, build your portfolio!\n\nData Science in Context Presentation (5%)\n\nOne 5 minute presentation, sign up for your presentation slot asap!\n\nMeetup Reflections and Introduction (10%)"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#how-to-do-your-homework",
    "href": "meetups/Meetup1/Meetup1.html#how-to-do-your-homework",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "How to do your homework",
    "text": "How to do your homework\n\nQuarto Markdown Format\n\nContains both code and content\n\nTemplates on the website\nSubmit a pdf and qmd file\n\n\n---\ntitle: \"Lab 7: Rectangling and Webscraping\"\nformat: pdf\neditor: source\n---\n\n\nhtml files do not contain figures, so I won’t be able to see them if you submit an html!"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#research-on-llms-in-education",
    "href": "meetups/Meetup1/Meetup1.html#research-on-llms-in-education",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Research on LLMs in Education",
    "text": "Research on LLMs in Education\n\nEarly days, but mixed results\nOpinions range from ‘LLMs stop you from being able to think for yourself’ to ‘Concern over LLMs is just another moral panic’\nYour Brain on ChatGPT (I’m skeptical)\nWhy knowledge is important in the age of AI (makes excellent points)"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#research-on-llms-in-education-1",
    "href": "meetups/Meetup1/Meetup1.html#research-on-llms-in-education-1",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Research on LLMs in Education",
    "text": "Research on LLMs in Education\n\n\n\n\nAbstract\n\n\nIn the age of generative AI and ubiquitous digital tools, human cognition faces a structural paradox: as external aids become more capable, internal memory systems risk atrophy. Drawing on neuroscience and cognitive psychology, this paper examines how heavy reliance on AI systems and discovery-based pedagogies may impair the consolidation of declarative and procedural memory – systems essential for expertise, critical thinking, and long-term retention. We review how tools like ChatGPT and calculators can short-circuit the retrieval, error correction, and schema-building processes necessary for robust neural encoding. Notably, we highlight striking parallels between deep learning phenomena such as “grokking” and the neuroscience of overlearning and intuition. Empirical studies are discussed showing how premature reliance on AI during learning inhibits proceduralization and intuitive mastery. We argue that effective human-AI interaction depends on strong internal models – biological “schemata” and neural manifolds – that enable users to evaluate, refine, and guide AI output…."
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#what-i-have-seen",
    "href": "meetups/Meetup1/Meetup1.html#what-i-have-seen",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "What I have Seen",
    "text": "What I have Seen\n\n\nDo not let yourself become reliant on LLMs"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#my-recommendations",
    "href": "meetups/Meetup1/Meetup1.html#my-recommendations",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "My Recommendations",
    "text": "My Recommendations\n\nAsk LLMs how something works\nUse LLMs for things you can easily check that are low stakes\nUse LLMs to check for grammar issues, to reword a sentence you struggle with\nUnderstand model differences and use professional tools (You can get 1 year free of google AI Pro…)\nExperiment!"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#my-recommendations-1",
    "href": "meetups/Meetup1/Meetup1.html#my-recommendations-1",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "My recommendations",
    "text": "My recommendations\n\nDon’t copy and paste\n\nType by hand LLM generated code, understand how it works, maybe even delete it and try to recreate it\n\nDon’t use it to write or analyze for you\n\nWriting is thinking\n\nDon’t turn in something you don’t understand"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#schedule",
    "href": "meetups/Meetup1/Meetup1.html#schedule",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#textbooks",
    "href": "meetups/Meetup1/Meetup1.html#textbooks",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Textbooks",
    "text": "Textbooks\n\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2023). R for Data Science (2e). O’Reilly\nJennifer Bryan. Happy Git and GitHub for the R User.\nJulia Silge and David Robinson (2017). Text Mining with R. O’Reilly\n\nRecommended: Wickham, H. Advanced R. Baca Raton, FL: Taylor & Francis Group."
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#tidyverse-opinionated-ecosystem",
    "href": "meetups/Meetup1/Meetup1.html#tidyverse-opinionated-ecosystem",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Tidyverse: Opinionated Ecosystem",
    "text": "Tidyverse: Opinionated Ecosystem\n\n\n\n\n\nCollection of compatible packages\nShared philosophy, common grammar\nStrong Core, Many Extensions\nAdvantages and Disadvantages"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#what-to-do-this-week",
    "href": "meetups/Meetup1/Meetup1.html#what-to-do-this-week",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "What to do this week?",
    "text": "What to do this week?\n\nReadings:\n\nIntro and Chapter 28 of R4DS\nQuarto Tutorial\nAppendix on R Help Files\n\nGet software installed and configured:\n\nR, RStudio/Positron, git, latex\nI recommend following 1-15 in happygitwithr\n\nWrite a post introducing yourself in the brightspace discussion board\nSign up for your Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#rstudio-versus-positron",
    "href": "meetups/Meetup1/Meetup1.html#rstudio-versus-positron",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "RStudio versus Positron",
    "text": "RStudio versus Positron\n\nUntil recently RStudio was the most common R IDE\nNow community cautiously switching to Positron\nUse Positron if:\n\nYou have used VS Code before/have programming experience\nWant to future proof yourself\n\nUse RStudio if:\n\nYou like it more than positron"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#community-buildingnetworking",
    "href": "meetups/Meetup1/Meetup1.html#community-buildingnetworking",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Community Building/Networking",
    "text": "Community Building/Networking\n\nBe active in the slack!\n\nLook at the social channel for events\nStarting a “journal club”\n\nJoin our linkedin group"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#community-buildingnetworking-1",
    "href": "meetups/Meetup1/Meetup1.html#community-buildingnetworking-1",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Community Building/Networking",
    "text": "Community Building/Networking\n\n\nOnce per month nyhackr.org"
  },
  {
    "objectID": "meetups/Meetup1/Meetup1.html#image-references",
    "href": "meetups/Meetup1/Meetup1.html#image-references",
    "title": "Meetup 1: Data Science Workflow and Toolkit",
    "section": "Image References",
    "text": "Image References\n\nCoal: By Morgre - Own work, CC BY-SA 3.0\nGas/Methane: By Georg Slickers - Self-photographed, CC BY-SA 3.0\nHydro: By Source file: Le Grand PortageDerivative work: Rehman - File:Three_Gorges_Dam,_Yangtze_River,_China.jpg, CC BY 2.0\nSolar: By Parabel GmbH - Own work, CC BY-SA 3.0\nWind: By Erik Wilde from Berkeley, CA, USA - harvesting wind, CC BY-SA 2.0\n\n\n\n\n\nDATA 607"
  },
  {
    "objectID": "assignments/tidygit.html",
    "href": "assignments/tidygit.html",
    "title": "GitHub/TidyVerse Create and Extend",
    "section": "",
    "text": "The primary homework assignments in this course are lab assignments where you will use R and occassionally other software to acquire, explore, wrangle, and manage different data sets. Please submit a PDF (preferred) or HTML file along with your Rmarkdown file. Be sure to answer all questions in lab, not just the on your own section. Labs should be submitted on Blackboard.\n\n\nTidyverse Create (Template)\n\n\nTidyverse Extend (Template)",
    "crumbs": [
      "Assignments",
      "GitHub/Tidyverse Recipes"
    ]
  },
  {
    "objectID": "assignments/labs/tbd.html",
    "href": "assignments/labs/tbd.html",
    "title": "Lab TBD",
    "section": "",
    "text": "Check back later\nThis assignment is not available to view yet. Please check back closer to the assignment due date/week of the module this assignment is based on."
  },
  {
    "objectID": "assignments/labs/Lab5.html",
    "href": "assignments/labs/Lab5.html",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "In this lab you will practice perform a series of exercises that use text and string manipulation to either analyze data with text, manipulate data containing strings, apply regular expressions, or handle data files with unusual formats or text strings.\n\n\nUsing the 173 majors listed in fivethirtyeight.com’s College Majors dataset, provide code that identifies the majors that contain either “DATA” or “STATISTICS”, case insensitive. You can find this dataset on R by installing the package fivethirtyeight and using the major column in either college_recent_grades, college_new_grads, or college_all_ages.\n\n\n\nWrite code that transforms the data below (treated as a raw string):\n[1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\"\n[5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\"\n[9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\"\n[13] \"olive\"  \"salal berry\"\n\nInto a format like this (i.e. as a vector containing all of the individual strings):\nc(\"bell pepper\", \"bilberry\", \"blackberry\", \"blood orange\", \"blueberry\", \"cantaloupe\", \"chili pepper\", \"cloudberry\", \"elderberry\", \"lime\", \"lychee\", \"mulberry\", \"olive\", \"salal berry\")\nAs your starting point take the string defined in the following code chunk:\n\nmessyString = ' [1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\" \\n\n [5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\" \\n\n [9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\" \\n\n [13] \"olive\"  \"salal berry\" '\n\nHint: There are many different ways to solve this problem, but if you use str_extract_all a helpful flag that returns a character vector instead of a list is simplify=TRUE. Then you can apply other tools from stringr if needed.\n\n\n\n\nDescribe, in words, what these regular expressions will match. Read carefully to see if each entry is a regular expression or a string that defines a regular expression. Remember that you can test these with code if you are uncertain.\n\n\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"(..)\\\\1\"\n\n\nConstruct regular expressions to match words that (again remember you can test if you are unsure):\n\n\nStart with “y”.\nContain a vowel-consonant pair\nContain the same vowel-consonant pair repeated twice in a row.\n\nFor each example, verify that they work by running them on the stringr::words dataset and show the first 10 results (hint: combine str_detect and logical subsetting).\n\n\n\nLLM Prompting Exercise. In the meetup case study we introduced a case-study which showed examples of FASTA sequences and how to search them using regex. A telomere is a region of repetitive DNA that occurs at the end of linear chromosomes (such as occur in humans and other mammals). These sequences are non-coding regions because they are not translated into proteins. The goal of this problem is to use an LLM of your choice to write a short R program using a regular expression that takes a string, determines if it ends in a telomeres, and then returns the coding part of the string. Here a telomere is defined as a series of 3 or more repeats of the DNA sequence TTAGGG\nThis can be solved with a very short regular expression which uses a slightly fancier feature than we discussed in the meetup:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\ncoding_match = \"^([CATG]+?)(?=(TTAGGG){3,}$)\"\n\ncoding_region = function(seq){\n\n    match = seq |&gt; str_extract(coding_match)\n    return(match)\n    \n}\n\nYou can see 3 examples (one which contains a telomere and two that do not below:\n\n# code-overflow: wrap\n\ngood_1 = \"CCCTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG\"\n\nbad_1 = \"CCCNTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG\"\n\nbad_2 = \"CCCTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGGTTAGGGTTAGGGTTTAGGGTTAGGGTTTAGGGGTTAGGGGTTAGGGATTAGGGTTAGGGTTTAGG\"\n\n# This shows everything but the telomere repeats\n\ngood_1 |&gt; str_view(coding_match) \n\n[1] │ &lt;CCCTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGG&gt;TTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG\n\n# These will show you nothing\n\nbad_1 |&gt; str_view(coding_match)\n\nbad_2 |&gt; str_view(coding_match)\n\nThe second string doesn’t match because it contains a non-coding character N. The third string doesn’t match because it does not have enough repeats.\nThe regex ^([CATG]+?)(?=(TTAGGG){3,}$) works for the following reasons:\nThe first part of the regex, ^([CATG]+?) looks for the start of the expression and then a sequence of characters containing either C, A, T, or G. The +? makes this part of the expression do lazy matching. This means it will match an expression starting with and containing nucleotide sequences until there is a part of the string that matches the next part of the regular expression. The next part of the regular expression contains (TTAGGG){3,}$) which matches three or more repeats of our telomere sequence TTAGGG terminating at the end of the string. The ?= that is placed before this expression makes it so that our match does not return the telomere repeats, so that we only end up with the coding part at the end.\n\nUse an LLM to try to solve this problem. Does your LLM solution use a regular expression? Provide your code and the LLM prompt (as well as the LLM you used). This problem was inspired by the book “Regular Expression Puzzles and AI Coding Assistants”. When the book was written (2023) copilot couldn’t solve this problem and ChatGPT struggled. I was able to generate very ugly solutions with lazy prompts. See how you can do. I recommend testing on the three strings I provided above in addition to thinking about the solution yourself.\n\n\n\n\nConsider the gss_cat data-frame discussed in Chapter 16 of R4DS (provided as part of the forcats package):\n\nCreate a new variable that describes whether the party-id of a survey respondent is “strong” if they are a strong republican or strong democrat, “weak” if they are a not strong democrat, not strong republican, or independent of any type, and “other” for the rest.\nCalculate the mean hours of TV watched by each of the groups “strong”, “weak”, and “other” and display it with a dot-plot (geom_point). Sort the levels in the dot-plot so that the group appears in order of most mean TV hours watched."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-1.",
    "href": "assignments/labs/Lab5.html#problem-1.",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "Using the 173 majors listed in fivethirtyeight.com’s College Majors dataset, provide code that identifies the majors that contain either “DATA” or “STATISTICS”, case insensitive. You can find this dataset on R by installing the package fivethirtyeight and using the major column in either college_recent_grades, college_new_grads, or college_all_ages."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-2",
    "href": "assignments/labs/Lab5.html#problem-2",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "Write code that transforms the data below (treated as a raw string):\n[1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\"\n[5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\"\n[9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\"\n[13] \"olive\"  \"salal berry\"\n\nInto a format like this (i.e. as a vector containing all of the individual strings):\nc(\"bell pepper\", \"bilberry\", \"blackberry\", \"blood orange\", \"blueberry\", \"cantaloupe\", \"chili pepper\", \"cloudberry\", \"elderberry\", \"lime\", \"lychee\", \"mulberry\", \"olive\", \"salal berry\")\nAs your starting point take the string defined in the following code chunk:\n\nmessyString = ' [1] \"bell pepper\" \"bilberry\" \"blackberry\" \"blood orange\" \\n\n [5] \"blueberry\" \"cantaloupe\" \"chili pepper\" \"cloudberry\" \\n\n [9] \"elderberry\" \"lime\" \"lychee\" \"mulberry\" \\n\n [13] \"olive\"  \"salal berry\" '\n\nHint: There are many different ways to solve this problem, but if you use str_extract_all a helpful flag that returns a character vector instead of a list is simplify=TRUE. Then you can apply other tools from stringr if needed."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-3",
    "href": "assignments/labs/Lab5.html#problem-3",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "Describe, in words, what these regular expressions will match. Read carefully to see if each entry is a regular expression or a string that defines a regular expression. Remember that you can test these with code if you are uncertain.\n\n\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"(..)\\\\1\"\n\n\nConstruct regular expressions to match words that (again remember you can test if you are unsure):\n\n\nStart with “y”.\nContain a vowel-consonant pair\nContain the same vowel-consonant pair repeated twice in a row.\n\nFor each example, verify that they work by running them on the stringr::words dataset and show the first 10 results (hint: combine str_detect and logical subsetting)."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-4",
    "href": "assignments/labs/Lab5.html#problem-4",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "LLM Prompting Exercise. In the meetup case study we introduced a case-study which showed examples of FASTA sequences and how to search them using regex. A telomere is a region of repetitive DNA that occurs at the end of linear chromosomes (such as occur in humans and other mammals). These sequences are non-coding regions because they are not translated into proteins. The goal of this problem is to use an LLM of your choice to write a short R program using a regular expression that takes a string, determines if it ends in a telomeres, and then returns the coding part of the string. Here a telomere is defined as a series of 3 or more repeats of the DNA sequence TTAGGG\nThis can be solved with a very short regular expression which uses a slightly fancier feature than we discussed in the meetup:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\ncoding_match = \"^([CATG]+?)(?=(TTAGGG){3,}$)\"\n\ncoding_region = function(seq){\n\n    match = seq |&gt; str_extract(coding_match)\n    return(match)\n    \n}\n\nYou can see 3 examples (one which contains a telomere and two that do not below:\n\n# code-overflow: wrap\n\ngood_1 = \"CCCTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG\"\n\nbad_1 = \"CCCNTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG\"\n\nbad_2 = \"CCCTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGGTTAGGGTTAGGGTTTAGGGTTAGGGTTTAGGGGTTAGGGGTTAGGGATTAGGGTTAGGGTTTAGG\"\n\n# This shows everything but the telomere repeats\n\ngood_1 |&gt; str_view(coding_match) \n\n[1] │ &lt;CCCTGAATAATCAAGGTCACAGACCAGTTAGAATGGTTTAGTGTGGAAAGCGGGAAACGAAAAGCCTCTCTGAATCCTGCGCACCGAGATTCTCCCAAGGCAAGGCGAGGGGCTGTATTGCAGGGTTCAACTGCAGCGTCGCAACTCAAATGCAGCATTCCTAATGCACACATGACACCCAAAATATAACAGACATATTACTCATGGAGGGTGAGGGTGAGGGTGAGGG&gt;TTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGGTTAGGG\n\n# These will show you nothing\n\nbad_1 |&gt; str_view(coding_match)\n\nbad_2 |&gt; str_view(coding_match)\n\nThe second string doesn’t match because it contains a non-coding character N. The third string doesn’t match because it does not have enough repeats.\nThe regex ^([CATG]+?)(?=(TTAGGG){3,}$) works for the following reasons:\nThe first part of the regex, ^([CATG]+?) looks for the start of the expression and then a sequence of characters containing either C, A, T, or G. The +? makes this part of the expression do lazy matching. This means it will match an expression starting with and containing nucleotide sequences until there is a part of the string that matches the next part of the regular expression. The next part of the regular expression contains (TTAGGG){3,}$) which matches three or more repeats of our telomere sequence TTAGGG terminating at the end of the string. The ?= that is placed before this expression makes it so that our match does not return the telomere repeats, so that we only end up with the coding part at the end.\n\nUse an LLM to try to solve this problem. Does your LLM solution use a regular expression? Provide your code and the LLM prompt (as well as the LLM you used). This problem was inspired by the book “Regular Expression Puzzles and AI Coding Assistants”. When the book was written (2023) copilot couldn’t solve this problem and ChatGPT struggled. I was able to generate very ugly solutions with lazy prompts. See how you can do. I recommend testing on the three strings I provided above in addition to thinking about the solution yourself."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-5",
    "href": "assignments/labs/Lab5.html#problem-5",
    "title": "Lab 5: Working with Text and Strings",
    "section": "",
    "text": "Consider the gss_cat data-frame discussed in Chapter 16 of R4DS (provided as part of the forcats package):\n\nCreate a new variable that describes whether the party-id of a survey respondent is “strong” if they are a strong republican or strong democrat, “weak” if they are a not strong democrat, not strong republican, or independent of any type, and “other” for the rest.\nCalculate the mean hours of TV watched by each of the groups “strong”, “weak”, and “other” and display it with a dot-plot (geom_point). Sort the levels in the dot-plot so that the group appears in order of most mean TV hours watched."
  },
  {
    "objectID": "assignments/labs/Lab4.html",
    "href": "assignments/labs/Lab4.html",
    "title": "Lab 4: Data Transformations",
    "section": "",
    "text": "For this assignment we practice data transformation using a dataset of the daily prices and daily trading volumes of a group of large technology stocks that trade on US stock exchanges. Click here to download stocks.csv, which contains data going back to 2000. The dataset (additional github link here) contains several variables, including:\n\nsymbol: which is the ticker symbol for the stock\ndate: which is the trading date\nopen, high, low, and close, which are the price at the start of trading, the high price during the day, the low price during the day, and the stock price at the close of trading (unit is USD)\nadjusted: which is the stock price at close adjusted for the financial effects of special events (such as dividends). Unit is USD\nvolume: which is the number of shares which traded during a given trading day.\n\n\nlibrary(tidyverse)\nlibrary(TTR)\nlibrary(kableExtra)\n\nstocks = read_csv(\"/home/georgehagstrom/work/Teaching/DATA607Fall2025/website/assignments/labs/labData/stocks.csv\")\n\nstocks |&gt; head(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\nAAPL\n2000-01-03\n0.936384\n1.004464\n0.907924\n0.999442\n535796800\n0.8440041\n\n\nAAPL\n2000-01-04\n0.966518\n0.987723\n0.903460\n0.915179\n512377600\n0.7728457\n\n\nAAPL\n2000-01-05\n0.926339\n0.987165\n0.919643\n0.928571\n778321600\n0.7841553\n\n\nAAPL\n2000-01-06\n0.947545\n0.955357\n0.848214\n0.848214\n767972800\n0.7162958\n\n\nAAPL\n2000-01-07\n0.861607\n0.901786\n0.852679\n0.888393\n460734400\n0.7502258\n\n\nAAPL\n2000-01-10\n0.910714\n0.912946\n0.845982\n0.872768\n505064000\n0.7370310\n\n\nAAPL\n2000-01-11\n0.856585\n0.887277\n0.808036\n0.828125\n441548800\n0.6993311\n\n\nAAPL\n2000-01-12\n0.848214\n0.852679\n0.772321\n0.778460\n976068800\n0.6573902\n\n\nAAPL\n2000-01-13\n0.843610\n0.881696\n0.825893\n0.863839\n1032684800\n0.7294905\n\n\nAAPL\n2000-01-14\n0.892857\n0.912946\n0.887277\n0.896763\n390376000\n0.7572942\n\n\n\n\n\nAll of the the stock prices and the trading volume have been adjusted for stock splits, so that the data provide a continuous record of how prices and trading volume changed.\nThere are several important functions and packages that you will need to use to complete this exercise.\n\nWe will make a lot of use of window functions in dplyr, which are very helpful for making transformations (including lead, lag, percent_rank). The dplyr vignettes and articles are incredibly useful for learning about all the different functions available\nWe will use the TTR package (which is part of the tidyquant family of packages, see here). The main function we will use is called runMean. An alternative package that is also very nice but not part of the tidyverse is RcppRoll\nIf you aren’t very comfortable with logarithms, you should read more about them. They are one of the most important mathematical functions for data science. We aren’t using their mathematical properties much this week but they will be important throughout your data science journey. Khan Academy has a decent video, and this article in the journal nature has some more context.\nWe will calculate some correlation coefficients, using the cor function from base R (?cor to see how it is used). There is also a tidyverse package called corrr that is useful for calculating correlations on data frames, but we won’t use it for this lab.\nThe motivation for today’s assignment came from some news articles a few years ago about how big tech stocks collectively had a miniature meltdown after powering the stock market for several consecutive years, see this article at Morningstar\nThe wikipedia page on Market Impact has some references to Kyle’s \\(\\lambda\\), which we will calculate this week.\n\nProblem 1: The price of a stock on a given day only conveys information in relation to the stock price on other days. One useful measure is the daily return of the stock, which we will define as the ratio of the adjusted closing price on the current day of trading to the adjusted closing price on the previous day of trading. Read the following article on window functions in dplyr: window functions in dplyr.\n\nDaily Return: Find a function there that will help you calculate the daily return and use it along with mutate to add a return column to the data frame containing the daily return.\n\nHint: make sure to use group_by(symbol), otherwise your calculation might transpose prices from a different stock at the beginning of each time series.\n\nFinding Special Events: Differences between the return and the return calculated based on the close price should indicate special corporate events such as dividends. Calculate the unadjusted return using the same technique you used to calculate the return, but replacing the adjusted variable with the close variable, and find the datapoint in the dataset where the return exceeded the unadjusted return by the greatest margin.\n\nHint to check you have done it right: it happened in November 2004. The reason that the close price and the adjusted price differ is because stock prices typically decrease when a dividend is paid (to account for the cash paid out). The adjusted value has been modified from the beginning of the initial data record to increase adjusted to compensate for dividends. A dividend is just a payment that a company makes periodically to those who hold stock.\nIf you are curious: Look for an old news article describing the significance of that event and tell me what happened\nProblem 2: When working with stock price fluctuations or other processes where a quantity increases or decreases according to some multiplicative process like a growth rate (for example population growth) it is often better to work with the log of the growth rate rather than the growth rate itself. This allows standard summary statistics such as the mean to have a useful interpretation (otherwise you would have to use the geometric mean). Furthermore, the log transform is often useful to use on variables that are strictly positive, such as population growth rates or daily stock returns. To see why, consider a hypothetical stock which had a return of 0.5 (50% loss) on one day and 1.8 on the next day (80% gain). The mean of these two returns would be 1.075, or 7.5% per day. However, at the end of the two day period the stock would have lost 10% of its value (0.5*1.8 = 0.9). If we had computed the mean of the log(return) instead, we would have found that (log(0.5)+log(1.8))/2 = log(0.9^(1/2)), or approximately -5.2% per day, matching the observed price change.\n\nDistribution of return Create a new variable called log_return which is the log of the return variable you calculated in the previous problem. Generate either a histogram or density plot of the distribution of log_return for the entire dataset. Then create a QQ-plot of log_return using geom_qq() and geom_qq_line(). What do you notice about the “tails” (right and left side/extreme edges) of the distribution from the QQ-plot? Are there visible signs of this in the density plot/histogram that you made?\n\nProblem 3: Volume measures how many shares were traded of a given stock over a set time period, and high volume days often associate with important events or market dynamics.\n\nVolume-Return Covariation: Create a new variable called log_volume which is the log(volume). Make a scatter plot of log_volume versus log_return, faceted by symbol to account for the fact that different stocks have different trading volumes. Do you see an association between log_volume and log_return in these scatter plots?\nVolume-Return Correlation: Use the cor function to compute the pearson’s correlation coefficient between log_volume and log_return for each symbol. Why do you think the correlations are close to 0?\n\nHint: use summarize and don’t forget that cor is a base R function so you will either need to filter NA values for volume and log_return or appropriately choose the use flag in the argument- see ?cor for more info.\n\nTransformed Correlations Next compute the correlation between abs(log_return) and log_volume using the absolute value function for each symbol. How have the correlations changed from the previous summary?\n\nProblem 4: For this problem we will implement a more complicated mathematical transformation of data by calculating a measure of liquidity for each stock.\nLiquidity measures the ability for a given asset to be bought or sold without a large impact on price. Liquid assets can be bought and sold quickly and easily (heavily traded stocks, currency), whereas illiquid assets (houses, art) have large increases or decreases in their price when someone tries to buy or sell them in large quantities. Liquidity is considered an important property of a well functioning financial market, and declines in liquidity have been blamed for worsening or triggering stock market crashes.\nMany methods have been invented to measure liquidity, but for this problem we will focus on a method called “Kyle’s \\(\\lambda\\)”. Kyle’s \\(\\lambda\\) estimates liquidity by using a linear regression between the absolute value daily return of a stock and the logarithm of the dollar volume of that stock. The time periods used to estimate this regression can vary (each choice of time periods defines a unique measure of liquidity), and here we will use daily returns and a one month time period (defined as 20 trading days). You will learn a lot about linear models in DATA 606 and other classes, but to be complete, \\(\\lambda\\) is a coefficient in the following linear model: \\[\n|R_t-1| = c + \\lambda \\log((\\mathrm{Volume})_t (\\mathrm{close})_t) + \\epsilon_t\n\\] where the coefficients \\(c\\) and \\(\\lambda\\) will be calculated to minimize the error \\(\\epsilon_t\\) over the past 20 trading days.\n\\(\\lambda\\) stands for the amount that the stock price will move in units of basis points for a given \\(\\log\\) dollar volume of trade. A small \\(\\lambda\\) indicates high liquidity, and a high \\(\\lambda\\) indicates low liquidity.\n\\(\\lambda\\) can be be calculated using rolling averages on the time series data with the TTR package, specifically the function runMean which when used within a dplyr pipeline will calculate the mean over the past \\(n\\) data points. For example, the command:\n\nlibrary(TTR)\nstocks |&gt; group_by(symbol) |&gt;  mutate(return_20d = runMean(return,n=20))\n\nadds a new variable which is equal to the mean of the log_return over the past 20 days. The mathematical formula for \\(\\lambda\\) is: \\[\n\\lambda = \\frac{\\mathrm{mean}_{20}(R_a\\log( p_c V ))\n- \\mathrm{mean}_{20}\\left(R_a\\right) \\mathrm{mean}_{20}\\left(\\log\\left(p_c V\\right) \\right) }\n{\\mathrm{mean}_{20}\\left(\\log\\left( p_c V \\right)^2\\right)\n-\\mathrm{mean}_{20}\\left(\\log(p_c V)\\right)^2 }\n\\] where to make the formula easier to read we have defined \\(R_a = |\\mathrm{return} -1|\\) (note this is the regular return and not log_return), \\(p_c = \\mathrm{close}\\) and \\(V = \\mathrm{volume}\\), and the averages have been taken over the past 20 days of data.\n\nCalculate Kyle’s \\(\\lambda\\): Add a new variable called kyle to the data frame by implementing the above formula for \\(\\lambda\\). Make sure to read and implement the formula very carefully, and to use the runMean function to calculate the rolling average correctly. I recommend writing a function which computes kyle from the variables close, return, and volume, and then using that function in mutate. You can start with the template code below (I would prefer you code this by hand):\n\n\nkyle = function(return,close,volume){\n  \n  # put your code here\n}\n\n\nVibe Liquidity- LLM Prompting Exercise: LLMs can be powerful tools for code generation, but they can introduce unexpected bugs and generate code that is difficult to read, understand, and maintain. As such, it is important to verify their output and to be very specific about the style and features you want the output code to have. Without direction, LLMs will recapitulate the style of their training set. Recreate the kyle function above using an LLM of your choice by prompting it with instructions about the style you want and the mathematical formula. How does the output differ from the function you constructed in part (a)? Test the function by comparing it to kyle function you created earlier on the same output. Make sure to tell me the prompt that you used and the LLM as well.\nLiquidity in Extreme Events: Add a new variable to the dataframe called extreme which is true when the log_return for a given stock is either greater than 99% of other values of the log_return or less than 99% of all values of log_return. Use the percent_rank dplyr window function along with logical operators to create this variable. Then for each stock calculate the mean value of Kyle’s \\(\\lambda\\) for the days when the log_return had extreme values and for when it didn’t (as identified by the extreme variable). What do your calculations and figures indicate about liquidity during extreme events?"
  },
  {
    "objectID": "assignments/labs/Lab4.html#overview-large-technology-stocks",
    "href": "assignments/labs/Lab4.html#overview-large-technology-stocks",
    "title": "Lab 4: Data Transformations",
    "section": "",
    "text": "For this assignment we practice data transformation using a dataset of the daily prices and daily trading volumes of a group of large technology stocks that trade on US stock exchanges. Click here to download stocks.csv, which contains data going back to 2000. The dataset (additional github link here) contains several variables, including:\n\nsymbol: which is the ticker symbol for the stock\ndate: which is the trading date\nopen, high, low, and close, which are the price at the start of trading, the high price during the day, the low price during the day, and the stock price at the close of trading (unit is USD)\nadjusted: which is the stock price at close adjusted for the financial effects of special events (such as dividends). Unit is USD\nvolume: which is the number of shares which traded during a given trading day.\n\n\nlibrary(tidyverse)\nlibrary(TTR)\nlibrary(kableExtra)\n\nstocks = read_csv(\"/home/georgehagstrom/work/Teaching/DATA607Fall2025/website/assignments/labs/labData/stocks.csv\")\n\nstocks |&gt; head(10) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nopen\nhigh\nlow\nclose\nvolume\nadjusted\n\n\n\n\nAAPL\n2000-01-03\n0.936384\n1.004464\n0.907924\n0.999442\n535796800\n0.8440041\n\n\nAAPL\n2000-01-04\n0.966518\n0.987723\n0.903460\n0.915179\n512377600\n0.7728457\n\n\nAAPL\n2000-01-05\n0.926339\n0.987165\n0.919643\n0.928571\n778321600\n0.7841553\n\n\nAAPL\n2000-01-06\n0.947545\n0.955357\n0.848214\n0.848214\n767972800\n0.7162958\n\n\nAAPL\n2000-01-07\n0.861607\n0.901786\n0.852679\n0.888393\n460734400\n0.7502258\n\n\nAAPL\n2000-01-10\n0.910714\n0.912946\n0.845982\n0.872768\n505064000\n0.7370310\n\n\nAAPL\n2000-01-11\n0.856585\n0.887277\n0.808036\n0.828125\n441548800\n0.6993311\n\n\nAAPL\n2000-01-12\n0.848214\n0.852679\n0.772321\n0.778460\n976068800\n0.6573902\n\n\nAAPL\n2000-01-13\n0.843610\n0.881696\n0.825893\n0.863839\n1032684800\n0.7294905\n\n\nAAPL\n2000-01-14\n0.892857\n0.912946\n0.887277\n0.896763\n390376000\n0.7572942\n\n\n\n\n\nAll of the the stock prices and the trading volume have been adjusted for stock splits, so that the data provide a continuous record of how prices and trading volume changed.\nThere are several important functions and packages that you will need to use to complete this exercise.\n\nWe will make a lot of use of window functions in dplyr, which are very helpful for making transformations (including lead, lag, percent_rank). The dplyr vignettes and articles are incredibly useful for learning about all the different functions available\nWe will use the TTR package (which is part of the tidyquant family of packages, see here). The main function we will use is called runMean. An alternative package that is also very nice but not part of the tidyverse is RcppRoll\nIf you aren’t very comfortable with logarithms, you should read more about them. They are one of the most important mathematical functions for data science. We aren’t using their mathematical properties much this week but they will be important throughout your data science journey. Khan Academy has a decent video, and this article in the journal nature has some more context.\nWe will calculate some correlation coefficients, using the cor function from base R (?cor to see how it is used). There is also a tidyverse package called corrr that is useful for calculating correlations on data frames, but we won’t use it for this lab.\nThe motivation for today’s assignment came from some news articles a few years ago about how big tech stocks collectively had a miniature meltdown after powering the stock market for several consecutive years, see this article at Morningstar\nThe wikipedia page on Market Impact has some references to Kyle’s \\(\\lambda\\), which we will calculate this week.\n\nProblem 1: The price of a stock on a given day only conveys information in relation to the stock price on other days. One useful measure is the daily return of the stock, which we will define as the ratio of the adjusted closing price on the current day of trading to the adjusted closing price on the previous day of trading. Read the following article on window functions in dplyr: window functions in dplyr.\n\nDaily Return: Find a function there that will help you calculate the daily return and use it along with mutate to add a return column to the data frame containing the daily return.\n\nHint: make sure to use group_by(symbol), otherwise your calculation might transpose prices from a different stock at the beginning of each time series.\n\nFinding Special Events: Differences between the return and the return calculated based on the close price should indicate special corporate events such as dividends. Calculate the unadjusted return using the same technique you used to calculate the return, but replacing the adjusted variable with the close variable, and find the datapoint in the dataset where the return exceeded the unadjusted return by the greatest margin.\n\nHint to check you have done it right: it happened in November 2004. The reason that the close price and the adjusted price differ is because stock prices typically decrease when a dividend is paid (to account for the cash paid out). The adjusted value has been modified from the beginning of the initial data record to increase adjusted to compensate for dividends. A dividend is just a payment that a company makes periodically to those who hold stock.\nIf you are curious: Look for an old news article describing the significance of that event and tell me what happened\nProblem 2: When working with stock price fluctuations or other processes where a quantity increases or decreases according to some multiplicative process like a growth rate (for example population growth) it is often better to work with the log of the growth rate rather than the growth rate itself. This allows standard summary statistics such as the mean to have a useful interpretation (otherwise you would have to use the geometric mean). Furthermore, the log transform is often useful to use on variables that are strictly positive, such as population growth rates or daily stock returns. To see why, consider a hypothetical stock which had a return of 0.5 (50% loss) on one day and 1.8 on the next day (80% gain). The mean of these two returns would be 1.075, or 7.5% per day. However, at the end of the two day period the stock would have lost 10% of its value (0.5*1.8 = 0.9). If we had computed the mean of the log(return) instead, we would have found that (log(0.5)+log(1.8))/2 = log(0.9^(1/2)), or approximately -5.2% per day, matching the observed price change.\n\nDistribution of return Create a new variable called log_return which is the log of the return variable you calculated in the previous problem. Generate either a histogram or density plot of the distribution of log_return for the entire dataset. Then create a QQ-plot of log_return using geom_qq() and geom_qq_line(). What do you notice about the “tails” (right and left side/extreme edges) of the distribution from the QQ-plot? Are there visible signs of this in the density plot/histogram that you made?\n\nProblem 3: Volume measures how many shares were traded of a given stock over a set time period, and high volume days often associate with important events or market dynamics.\n\nVolume-Return Covariation: Create a new variable called log_volume which is the log(volume). Make a scatter plot of log_volume versus log_return, faceted by symbol to account for the fact that different stocks have different trading volumes. Do you see an association between log_volume and log_return in these scatter plots?\nVolume-Return Correlation: Use the cor function to compute the pearson’s correlation coefficient between log_volume and log_return for each symbol. Why do you think the correlations are close to 0?\n\nHint: use summarize and don’t forget that cor is a base R function so you will either need to filter NA values for volume and log_return or appropriately choose the use flag in the argument- see ?cor for more info.\n\nTransformed Correlations Next compute the correlation between abs(log_return) and log_volume using the absolute value function for each symbol. How have the correlations changed from the previous summary?\n\nProblem 4: For this problem we will implement a more complicated mathematical transformation of data by calculating a measure of liquidity for each stock.\nLiquidity measures the ability for a given asset to be bought or sold without a large impact on price. Liquid assets can be bought and sold quickly and easily (heavily traded stocks, currency), whereas illiquid assets (houses, art) have large increases or decreases in their price when someone tries to buy or sell them in large quantities. Liquidity is considered an important property of a well functioning financial market, and declines in liquidity have been blamed for worsening or triggering stock market crashes.\nMany methods have been invented to measure liquidity, but for this problem we will focus on a method called “Kyle’s \\(\\lambda\\)”. Kyle’s \\(\\lambda\\) estimates liquidity by using a linear regression between the absolute value daily return of a stock and the logarithm of the dollar volume of that stock. The time periods used to estimate this regression can vary (each choice of time periods defines a unique measure of liquidity), and here we will use daily returns and a one month time period (defined as 20 trading days). You will learn a lot about linear models in DATA 606 and other classes, but to be complete, \\(\\lambda\\) is a coefficient in the following linear model: \\[\n|R_t-1| = c + \\lambda \\log((\\mathrm{Volume})_t (\\mathrm{close})_t) + \\epsilon_t\n\\] where the coefficients \\(c\\) and \\(\\lambda\\) will be calculated to minimize the error \\(\\epsilon_t\\) over the past 20 trading days.\n\\(\\lambda\\) stands for the amount that the stock price will move in units of basis points for a given \\(\\log\\) dollar volume of trade. A small \\(\\lambda\\) indicates high liquidity, and a high \\(\\lambda\\) indicates low liquidity.\n\\(\\lambda\\) can be be calculated using rolling averages on the time series data with the TTR package, specifically the function runMean which when used within a dplyr pipeline will calculate the mean over the past \\(n\\) data points. For example, the command:\n\nlibrary(TTR)\nstocks |&gt; group_by(symbol) |&gt;  mutate(return_20d = runMean(return,n=20))\n\nadds a new variable which is equal to the mean of the log_return over the past 20 days. The mathematical formula for \\(\\lambda\\) is: \\[\n\\lambda = \\frac{\\mathrm{mean}_{20}(R_a\\log( p_c V ))\n- \\mathrm{mean}_{20}\\left(R_a\\right) \\mathrm{mean}_{20}\\left(\\log\\left(p_c V\\right) \\right) }\n{\\mathrm{mean}_{20}\\left(\\log\\left( p_c V \\right)^2\\right)\n-\\mathrm{mean}_{20}\\left(\\log(p_c V)\\right)^2 }\n\\] where to make the formula easier to read we have defined \\(R_a = |\\mathrm{return} -1|\\) (note this is the regular return and not log_return), \\(p_c = \\mathrm{close}\\) and \\(V = \\mathrm{volume}\\), and the averages have been taken over the past 20 days of data.\n\nCalculate Kyle’s \\(\\lambda\\): Add a new variable called kyle to the data frame by implementing the above formula for \\(\\lambda\\). Make sure to read and implement the formula very carefully, and to use the runMean function to calculate the rolling average correctly. I recommend writing a function which computes kyle from the variables close, return, and volume, and then using that function in mutate. You can start with the template code below (I would prefer you code this by hand):\n\n\nkyle = function(return,close,volume){\n  \n  # put your code here\n}\n\n\nVibe Liquidity- LLM Prompting Exercise: LLMs can be powerful tools for code generation, but they can introduce unexpected bugs and generate code that is difficult to read, understand, and maintain. As such, it is important to verify their output and to be very specific about the style and features you want the output code to have. Without direction, LLMs will recapitulate the style of their training set. Recreate the kyle function above using an LLM of your choice by prompting it with instructions about the style you want and the mathematical formula. How does the output differ from the function you constructed in part (a)? Test the function by comparing it to kyle function you created earlier on the same output. Make sure to tell me the prompt that you used and the LLM as well.\nLiquidity in Extreme Events: Add a new variable to the dataframe called extreme which is true when the log_return for a given stock is either greater than 99% of other values of the log_return or less than 99% of all values of log_return. Use the percent_rank dplyr window function along with logical operators to create this variable. Then for each stock calculate the mean value of Kyle’s \\(\\lambda\\) for the days when the log_return had extreme values and for when it didn’t (as identified by the extreme variable). What do your calculations and figures indicate about liquidity during extreme events?"
  },
  {
    "objectID": "assignments/labs/Lab6.html",
    "href": "assignments/labs/Lab6.html",
    "title": "Lab 6: R and SQL",
    "section": "",
    "text": "This lab is divided into two parts. In the first part you will practice using joins for data wrangling and analysis on the nycflights dataset. Some of these problems originate from Chapter 19 of your book. For the second part, you will download a dataset on the budgets of college sports programs and process it for storage in a relational database (I strongly recommend using duckdb which can be installed using install.packages(\"duckdb\")- duckdb is highly performant, self-contained, and ideally suited both to learning SQL and performing data analysis). Then you will load this database and use dbplyr to perform an analysis. You will also practice using forcats to recode some of the variables as factors (which are supported by duckdb) and using separate_wider_delim to split columns of text data.\nYou will need to have installed and to use the following libraries:\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(duckdb)\nlibrary(nycflights13)"
  },
  {
    "objectID": "assignments/labs/Lab6.html#part-i-airline-flight-delays",
    "href": "assignments/labs/Lab6.html#part-i-airline-flight-delays",
    "title": "Lab 6: R and SQL",
    "section": "Part I: Airline Flight Delays",
    "text": "Part I: Airline Flight Delays\nFor the first part of this lab exercise, we will be using the nycflights library, which contains several different built in datasets including planes, which has information on each plane that appears in the data; flights, which has information on individual flights; airports, which has information on individual airports; and weather, which has information on the weather that the origin airports. In order to do this set of lab exercises, you will need to use different types of joins to combine variables in each data frame."
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-1",
    "href": "assignments/labs/Lab6.html#problem-1",
    "title": "Lab 6: R and SQL",
    "section": "Problem 1",
    "text": "Problem 1\n\nUse the flights and planes tibbles (both part nycflights) to compute the mean departure delay of each aircraft that has more than 30 recorded flights in the dataset. Hint: Make note of the fact that the variable year appears in both flights and planes but means different things in each before performing any joins.\nUse anti-join to identify flights where tailnum does not have a match in plane. Determine the carriers for which this problem is the most common.\nFind the airplane model whose planes made the most flights in the dataset, and filter the dataset to contain only flights flown by airplanes of that model, adding a variable which corresponds to the year each those airplanes were built. Then compute the average departure delay for each year of origin and plot the data. Is there any evidence that plane age relates to departure delays?"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-2",
    "href": "assignments/labs/Lab6.html#problem-2",
    "title": "Lab 6: R and SQL",
    "section": "Problem 2",
    "text": "Problem 2\n\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:\n\n\nairports |&gt;\n  semi_join(flights, join_by(faa == dest)) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()\n\nYou might want to use the size or color of the points to display the average delay for each airport."
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-3",
    "href": "assignments/labs/Lab6.html#problem-3",
    "title": "Lab 6: R and SQL",
    "section": "Problem 3:",
    "text": "Problem 3:\n\nsports_program_data.csv contains variables which either describe properties of a sports team or a college. Split sports_programs_data into two data frames, one called colleges and another called teams. How can you tell which variables describe colleges and which describe teams? Use the data dictionary and observations of how the values vary as you move from college to college to help make the decision easier. Make sure there are primary keys for both the colleges and teams data frames (verify with count)- what are the primary keys in each case and are they simple keys (one variable) or compound keys (require multiple variables)? After the split, one of the data-sets you created should contain a foreign key- which one has it and what variables comprise it?\nThe variable sector_name contains information about whether a college is public, private, non-profit, for-profit, a 2-year college, or a 4-year + college. Split this variable (using separate_wider_delim) into two variables, one of which describes whether the college is a Public, Private nonprofit, or private for-profit, and another which describes how many years the college programs run.\nSeveral variables are candidates to be recoded as factors, for example state_cd, zip_text, classification_name, sports, and the sector variables you just created for the previous part. Recode these variables as categorical variables. For the classification variable, use the classification_code to order the factors according to the numeric code."
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-4",
    "href": "assignments/labs/Lab6.html#problem-4",
    "title": "Lab 6: R and SQL",
    "section": "Problem 4",
    "text": "Problem 4\n\nUsing DBI, duckdb, and dbplyr, create a relational database with two tables, writing the sports data frame you created in problem 3 to one and the colleges data frame (also from problem 3) to the other. Write this database to disk. How does the size of the database file compare to the original csv? DuckDB uses a compression algorithm to store database files. Create a separate relational database with just a single table containing all the data in the original csv and compare the size of this database file to the normalized one you created (this is technically more of an apples to apples comparison of the effect of normalization).\nUse dbplyr to write a query to this database that calculates the top 10 colleges ranked by the average profit (defined as revenue - expenses) of their American football team over the years of data. Print the SQL query that results from your R pipeline using show_query() and then use collect() to show the results of this query."
  },
  {
    "objectID": "assignments/labs/Lab10.html",
    "href": "assignments/labs/Lab10.html",
    "title": "Lab 10: Tools for Working with Big Data",
    "section": "",
    "text": "This is a two part lab assignment. For both parts we will be using a dataset of New York City Taxi trips. In the first part you will complete a problem using data.tables and in the second part you will create a local spark instance on your computer and use the sparklyr library to simulate how you would process the dataset if it were stored on a distributed computing cluster.\nYou will need to download up to three data files for this lab, which are available via shared links in a google drive folder. The first is a dataset of taxi trips that took place in 2021. This file is 3GB in size so if your computer doesn’t have a large amount of RAM (at least 16GB), consider using the smaller alternative (but be clear which one you are using when you complete your assignment):\n\nClick here to download 2021 Taxi Rides\n\nThe second dataset just contains the rides from November for the 2021 dataset, and as a result is 10 times smaller:\n\nClick here to download the 2021 Taxi rides in November\n\nFinally, the final dataset contains details on the meaning of the taxi location code, which is important for determining the actual geographic location of the pickup and dropoff of each taxi ride:\n\nClick here to download the taxi location codes dataset\n\nProblem 1\n\nUse data.tables to load 2021 NYC taxi dataset (or, if your computer has low memory, the alternative dataset of just November 2021) and the dataset that describes the location of each taxi location code, and output the memory address of the data.table obtained after loading. Performing all operations in place, recode the the drop off and pick up time variables as date-times (whether or how you will do this depends on how your reader interprets the file). Then create new columns in the data table which are equal to the duration of each taxi ride (in whatever time units you prefer) and the average speed in miles per hour of each taxi ride. Next, set equal to NA all values of the ride speed where ride speed is either negative, greater than 90 mph, or where the ride time is longer than 3 hours. Next join with the location information so that the borough of origin and destination of each taxi ride is present in the data.table (this may require joining twice). Verify that this final data.table has the same memory address as the original data.frame. Hint: lubridate has a variety of functions for working with characters that represent time stamps.\nFor each combination of origin and destination boroughs, calculate the average speed (technically the average of the average speed) of taxi rides between those two boroughs and the total number of taxi rides between those boroughs, sort in descending order by average speed, and display the full answer.\n\nProblem 2\n\nCreate a local spark instance on your computer and load the November taxi dataset and taxi location dataset into your spark instance by using spark_read_csv. Join the datasets so that that the taxi ride data has data on the borough of origin of each taxi ride. Create a new column in the dataset equal to the tip percentage, and filter the tip percentage data so that it excludes data points where either the tip or fare was less 0. Then, for taxi rides originating in each borough, calculate the mean and maximum tip percentage.\nFor taxi rides on November 25th, filter the data so that tip percentage includes only rows where the tip and fare were non-negative, and the tip percentage is less than 100%. Make a plot of the distribution of cab fares for taxi rides originating in each borough (using ggplot2 or ggridges). Perform all of the computations and data wrangling in Spark, and only collect the final tibbles to display your results make your plot."
  },
  {
    "objectID": "assignments/labs/Lab10.html#overview",
    "href": "assignments/labs/Lab10.html#overview",
    "title": "Lab 10: Tools for Working with Big Data",
    "section": "",
    "text": "This is a two part lab assignment. For both parts we will be using a dataset of New York City Taxi trips. In the first part you will complete a problem using data.tables and in the second part you will create a local spark instance on your computer and use the sparklyr library to simulate how you would process the dataset if it were stored on a distributed computing cluster.\nYou will need to download up to three data files for this lab, which are available via shared links in a google drive folder. The first is a dataset of taxi trips that took place in 2021. This file is 3GB in size so if your computer doesn’t have a large amount of RAM (at least 16GB), consider using the smaller alternative (but be clear which one you are using when you complete your assignment):\n\nClick here to download 2021 Taxi Rides\n\nThe second dataset just contains the rides from November for the 2021 dataset, and as a result is 10 times smaller:\n\nClick here to download the 2021 Taxi rides in November\n\nFinally, the final dataset contains details on the meaning of the taxi location code, which is important for determining the actual geographic location of the pickup and dropoff of each taxi ride:\n\nClick here to download the taxi location codes dataset\n\nProblem 1\n\nUse data.tables to load 2021 NYC taxi dataset (or, if your computer has low memory, the alternative dataset of just November 2021) and the dataset that describes the location of each taxi location code, and output the memory address of the data.table obtained after loading. Performing all operations in place, recode the the drop off and pick up time variables as date-times (whether or how you will do this depends on how your reader interprets the file). Then create new columns in the data table which are equal to the duration of each taxi ride (in whatever time units you prefer) and the average speed in miles per hour of each taxi ride. Next, set equal to NA all values of the ride speed where ride speed is either negative, greater than 90 mph, or where the ride time is longer than 3 hours. Next join with the location information so that the borough of origin and destination of each taxi ride is present in the data.table (this may require joining twice). Verify that this final data.table has the same memory address as the original data.frame. Hint: lubridate has a variety of functions for working with characters that represent time stamps.\nFor each combination of origin and destination boroughs, calculate the average speed (technically the average of the average speed) of taxi rides between those two boroughs and the total number of taxi rides between those boroughs, sort in descending order by average speed, and display the full answer.\n\nProblem 2\n\nCreate a local spark instance on your computer and load the November taxi dataset and taxi location dataset into your spark instance by using spark_read_csv. Join the datasets so that that the taxi ride data has data on the borough of origin of each taxi ride. Create a new column in the dataset equal to the tip percentage, and filter the tip percentage data so that it excludes data points where either the tip or fare was less 0. Then, for taxi rides originating in each borough, calculate the mean and maximum tip percentage.\nFor taxi rides on November 25th, filter the data so that tip percentage includes only rows where the tip and fare were non-negative, and the tip percentage is less than 100%. Make a plot of the distribution of cab fares for taxi rides originating in each borough (using ggplot2 or ggridges). Perform all of the computations and data wrangling in Spark, and only collect the final tibbles to display your results make your plot."
  },
  {
    "objectID": "assignments/labs/Lab7.html",
    "href": "assignments/labs/Lab7.html",
    "title": "Lab 7: Rectangling and Webscraping",
    "section": "",
    "text": "Overview\nThis is a two part assignment. In the first part of the assignment you will practice rectangling on a dataset from the repurrrsive package. In the second part you will combine the rvest package along with functions and iteration to scrape data on foreign linked political action committees from the website open secrets.\n\n\nRectangling\nProblem 1: Load the repurrrsive package to get access to get access to the got_chars dataset. In section 23.4.2 of R4DS, there is code that extracts data from the got_chars list and converts it into a tibble with information on each character and a separate tibble which contains information on the titles held by each character. Perform similar operations to create separate tibbles containing the aliases, allegiances, books, and TV series of each Game of Thrones character.\n\n\nWebscraping Open Secrets\nIn the second part of this assignment we will scrape and work with data on foreign connected PACs that donate to US political campaigns. In the United States, only American citizens and green card holders can contribute to federal elections, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees.\nFirst, we will get data foreign connected PAC contributions in the 2024 election cycle. Then, you will use a similar approach to get data such contributions from previous years so that we can examine trends over time.\nYou may benefit from using the Selector Gadget extension or similar tools.\nIn addition to tidyverse, you will need to install and load the packages robotstxt and rvest\nProblem 2:\n\nCheck that open secrets allows you to webscrape by running the paths_allowed function on the url https://www.opensecrets.org.\nWrite a function called scrape_pac() that scrapes information from the Open Secrets webpage for foreign connected PAC contributions in a given year. The url for this data is https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2024. This function should take the url of the webpage as its only input and should output a data frame. The variables of this data-frame should be renamed so that they are in snake_case format (lower_case_and_underscores_for_spaces, see R4DS section 2.3). Use str_squish() to remove excess whitespace from the Country of Origin/Parent Company variables, and add a new column which records the year by extracting from the input url.\n\nHints (you may not need all of these):\n\nIf you have trouble finding the right elements to search for using the selector gadget try looking for a table element.\nUse read_html_live instead of read_html\nTo improve reliability, put a small pause after you use read_html_live (Sys.sleep(1) should work)\nBefore your function returns, call rm to remove the object created by read_html_live. This prevents the creation of large numbers of browser sessions\nSometimes your working function may fail inexplicably. Modify your function using insistently so that it retries multiple times.\n\n\nTest your function on the urls for 2024, 2022, and 2000, and show the first several rows of each of the outpus. Does the function seem to do what you expected it to do?\n\nProblem 3:\n\nConstruct a vector called urls that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year (combine seq and string functions). Using the map_dfr function from the purrr package, apply the scrape_pac() function over urls in a way that will result in a data frame called pac_all that contains the data for all of the years. I recommend the range 2000 to 2024.\nClean this combined dataset by separating the country of origin from the parent company (use separate_wider_delim or another tool of your choice, you will need to be cautious with some special cases in this column) and by converting the strings in the total, dems, and repubs columns into numbers. Print out the top 10 rows over your dataset after completing these steps.\nCalculate the total contributions from PACs linked to Canada and Mexico each year and plot how these contributions change over time.\nFind the 5 countries who over the entire time period of the dataset have the greatest total contribution from affiliated PACs. Then calculate the total contribution for each of those countries for each year of the data and make a plot of it to visualize how the contributions have changed over time."
  },
  {
    "objectID": "assignments/labs/labData/imdb_recent.html",
    "href": "assignments/labs/labData/imdb_recent.html",
    "title": "DATA 607 Fall 2025",
    "section": "",
    "text": "rating, review 2, “Where do I start? The opening credits promised so much but I went in with gritted teeth with it being a 12A I knew it will not be the movie I was expecting, and boy was I right. This is the exact thing I feared it’s the sanitised, disneyification and taming of a beloved franchise. The main predator is cgi slop,his constant strained expression and incessant need to roar every scene. The monsters on Genna are generic none threatening cgi tosh. The film makes the predator into a angsty teen trying to prove some point the happy go lucky android that constantly yaps is annoying the cutesy side creature makes it feel like some Star Wars rip off. The ending is terrible the lore of the yauja is abused.” 9, “It was a good movie. I liked it but I have no clue if it was like the book or not. The cast was really good though so I may be biased because I love Dave franco, McKenna grace and mason thames. It was good though you should watch it if you’re scrolling through the reviews on the only movies in theaters rn.” 10, “I saw the movie at the premiere and I will just say, the way that Madeliene McGraw delivers is just completely astonishing. The casting director needs to get a raise with casting such amazing young talent! Director Scott Derrickson effectively creates a tense and unsettling atmosphere, keeping viewers on edge throughout the film. The 1970s setting adds a nostalgic yet eerie backdrop to the story. The film takes time to develop its characters, particularly Finney and Gwen, making their plight more engaging and emotionally impactful. The use of the supernatural through the black phone adds a unique twist to the typical abduction narrative, providing an intriguing and suspenseful element to the plot.” 8, “Zachary Levi plays a charming hustler with heart and integrity. Naya Desir-Johnson is a precocious young girl who is as quick witted as Anne of Green Gables. I definitely think this family friendly story is worth 1hr 44 minutes of your time, whether in a theater or at home. You’ll get a history lesson on top of being entertained. Here is a summary I found…”The story behind Sarah’s Oil is the true story of Sarah Rector, an African American girl in Oklahoma Territory who became one of the nation’s youngest millionaires at age 11. Born in 1902, she inherited a 160-acre parcel of land that, while appearing worthless, turned out to be rich in oil. The discovery brought challenges as others tried to take control of her land, but Rector fought back and maintained ownership, becoming known as the “Richest Colored Girl in the World”.” I’m sure a quick Google search will flesh out the real events vs the artist license. The rest of the cast does a good job rounding out the story and keeping up suspense in this David and Goliath type story. I wouldn’t mind a film about the rest of this Sarah Rector’s life as she ended up rubbing elbows with the rich, powerful and famous. It’s well paced, doesn’t look low budget or come off hokie.” 1, “It’s disgraceful how Hollywood is trying to stylise one of the most horrific genocides in history just to appease a modern going audience addicted to TikTok. They dramatise, imbelish and add quick camera cuts to give the movie a frenetic feel in case anyone nods off. And the dialogue is so bad I was expecting them to break the fourth wall any minute and say,”these are the bad guys”. These directors have lost the art of conveying their meaning through camera and instead just dump everything into dialogue that doesn’t remotely sound how real people talk - and everything has to have background music to try to illicit emotion rather than scenes of reflection allowing US and the film to breathe. Overrated, corny, Hollywood drivel” 3, “The Chainsaw Man: Reze arc should have been a turning point for the series, but the adaptation falls flat in almost every meaningful way. It starts off unbearably slow, with the first half dragging along at a snail’s pace. Instead of building mystery around Reze or drawing the viewer deeper into her dynamic with Denji, the story meanders through empty scenes that feel padded out and pointless. By the time anything remotely interesting happens, the buildup has already killed any sense of tension. The visuals, while flashy, are a perfect example of over-animation gone wrong. MAPPA throws fluid movement at everything, even the most mundane moments, and it ends up looking more like a demo reel for animators than a carefully crafted piece of storytelling. Every gesture is exaggerated, every scene oversold, until the impact of the actual action sequences gets diluted. It’s exhausting to watch, not engaging. Reze herself is a tragic character in the manga, but the anime handles her with no subtlety at all. The adaptation leans heavily on sexual tension and surface-level fan service, reducing her to just another”seductive love interest” instead of a layered figure. There’s no emotional depth in how she’s presented, just empty pandering dressed up as romance. The soundtrack only makes matters worse. Instead of heightening emotion or giving the arc its own identity, the music feels out of place or bland to the point of being forgettable. In big fights, it’s generic noise that could’ve been ripped from a royalty-free library, and in quieter moments it completely fails to add atmosphere. It drags the experience down rather than lifting it up. Overall, this adaptation of the Reze arc is a huge disappointment. ” 7, “Like any Lanthimos film, it’s crazy, it’s weird but probably the one that’ll make the most sense to you among his works. It’s part sci-fi, part satire, part Hitchcock, part Psycho, and entirely the kind of weirdness only Lanthimos can do. Jesse Plemons and Emma Stone deliver two of their career-best performances-surely Oscar-nomination worthy. Visually stunning, tonally rich, and packed with striking close-ups, Bugonia is a delightfully deranged piece of cinema with an even crazier ending wrapped in layers of conspiracy.”"
  },
  {
    "objectID": "assignments/labs/Lab2.html",
    "href": "assignments/labs/Lab2.html",
    "title": "Lab 2: Data Tidying",
    "section": "",
    "text": "In this assignment you will work to tidy, clean, and analyze two different datasets, the first is a small dataset contained in a csv file called flightdelays.csv, and the second called MixedDrinkRecipes-Prep.csv.\nThe most important book chapters which cover the techniques you will practice here are R4DS Chapters 5 and 7. You may find helpful this tutorial page for a description for how to make multiway dot-plots"
  },
  {
    "objectID": "assignments/labs/Lab2.html#overview",
    "href": "assignments/labs/Lab2.html#overview",
    "title": "Lab 2: Data Tidying",
    "section": "",
    "text": "In this assignment you will work to tidy, clean, and analyze two different datasets, the first is a small dataset contained in a csv file called flightdelays.csv, and the second called MixedDrinkRecipes-Prep.csv.\nThe most important book chapters which cover the techniques you will practice here are R4DS Chapters 5 and 7. You may find helpful this tutorial page for a description for how to make multiway dot-plots"
  },
  {
    "objectID": "assignments/labs/Lab2.html#instructions",
    "href": "assignments/labs/Lab2.html#instructions",
    "title": "Lab 2: Data Tidying",
    "section": "Instructions",
    "text": "Instructions\nSubmit your completed assignment on the course brightspace page by uploading your .qmd file and a compiled pdf to brightspace."
  },
  {
    "objectID": "assignments/labs/Lab2.html#part-1-airplane-flight-delays",
    "href": "assignments/labs/Lab2.html#part-1-airplane-flight-delays",
    "title": "Lab 2: Data Tidying",
    "section": "Part 1: Airplane flight delays",
    "text": "Part 1: Airplane flight delays\nConsider the following dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos_Angeles\nPhoenix\nSan_Diego\nSan_Francisco\nSeattle\n\n\n\n\nALASKA\nOn_Time\n497\n221\n212\n503\n1841\n\n\n\nDelayed\n62\n12\n20\n102\n305\n\n\nAM WEST\nOn_Time\n694\n4840\n383\n320\n301\n\n\n\nDelayed\n117\n415\n65\n129\n61\n\n\n\nThe above table describes arrival delays for two different airlines across several destinations. The numbers correspond the the number of flights that were in either the delayed category or the on time category."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problems",
    "href": "assignments/labs/Lab2.html#problems",
    "title": "Lab 2: Data Tidying",
    "section": "Problems",
    "text": "Problems\nProblem 1: Read the information from flightdelays.csv into R, and use tidyr and dplyr to convert this data into a tidy/tall format with names and complete data for all columns. Your final data frame should have City, On_Time_Flights and Delayed_Flights as columns (the exact names are up to you). In addition to pivot_longer, pivot_wider and rename, you might find the tidyr function fill helpful for completing this task efficiently. Although this is a small dataset that you could easily reshape by hand, you should solve this problem using tidyverse functions that do the work for you.\nProblem 2: Take the data-frame that you tidied and cleaned in Problem 1 and create additional columns which contain the fraction of on-time and delayed flights at each airport. Then create a Cleveland Multiway Dot Plot (see this tutorial page for a description for how) to visualize the difference in flight delays between the two airlines at each city in the dataset. Compare the airlines and airports using the dot-plot- what are your conclusions?\nOptional: If you want to make a fancier visualization consider adding text labels containing the airline names above the dots using geom_text and position = position_nudge(...) with appropriate arguments."
  },
  {
    "objectID": "assignments/labs/Lab2.html#part-2-mixed-drink-recipes",
    "href": "assignments/labs/Lab2.html#part-2-mixed-drink-recipes",
    "title": "Lab 2: Data Tidying",
    "section": "Part 2: Mixed Drink Recipes",
    "text": "Part 2: Mixed Drink Recipes\nIn the second part of this assignment we will be working with a dataset containing ingredients for different types of mixed drinks. This dataset is untidy and messy- it is in a wide data format and contains some inconsistencies that should be fixed."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problems-1",
    "href": "assignments/labs/Lab2.html#problems-1",
    "title": "Lab 2: Data Tidying",
    "section": "Problems",
    "text": "Problems\nProblem 3: Load the mixed drink recipe dataset into R from the file MixedDrinkRecipes-prep.csv, which you can download from my github page by clicking here. The variables ingredient1 through ingredient6 list the ingredients of the cocktail listed in the name column. Notice that there are many NA values in the ingredient columns, indicating that most cocktails have under 6 ingredients.\nTidy this dataset using pivot_longer to create a new data frame where each there is a row corresponding to each ingredient of all the cocktails, and an additional variable specifying the “rank” of that cocktail in the original recipe, i.e. it should look like this:\n\n\n\n\n\n\n\n\n\nname\ncategory\nIngredient_Rank\nIngredient\n\n\n\n\nGauguin\nCocktail Classics\n1\nLight Rum\n\n\nGauguin\nCocktail Classics\n2\nPassion Fruit Syrup\n\n\nGauguin\nCocktail Classics\n3\nLemon Juice\n\n\nGauguin\nCocktail Classics\n4\nLime Juice\n\n\nFort Lauderdale\nCocktail Classics\n1\nLight Rum\n\n\n\nwhere the data-type of Ingredient_Rank is an integer. Hint: Use the parse_number() function in mutate after your initial pivot.\nProblem 4: Some of the ingredients in the ingredient list have different names, but are nearly the same thing. An example of such a pair is Lemon Juice and Juice of a lemon, which are considered different ingredients in this dataset, but which perhaps should be treated as the same depending on the analysis you are doing.\n\nMake a list of the ingredients appearing in the ingredient list ranked by how commonly they occur along with the number of occurrences, and print the first 10 elements of the list here. Then check more ingredients (I suggest looking at more ingredients and even sorting them alphabetically using arrange(asc(ingredient))) and see if you can spot pairs of ingredients that are similar but have different names. Use if_else( click here for if_else ) or case_when in combination with mutate to make it so that the pairs of ingredients you found have the same name. You don’t have to find all pairs, but find at least 5 pairs of ingredients to rename. Because the purpose of this renaming is to facilitate a hypothetical future analysis, you can choose your own criteria for similarity as long as it is somewhat justifiable.\nThis type of fuzzy text matching is a tedious problem to do by hand and is the sort of problem where an LLM can be very effective. Pick two different LLMs and prompt them to analyze the list of ingredients to identify groups of ingredients with multiple names. Investigate the lists that you found using the LLMs and compare them to each-other and the ingredients you identified by hand. How much overlap is there between the two LLM solutions? Do you agree with all the ingredient names that were paired together? For this problem report the prompts that you used in addition to the output- put some thought into the formatting of you show this output, you can also submit csv files that show which values go together instead.\n\nThe class has not yet covered how to automate the replacement you did “by hand” in the first part of this question, but in Module 7 you will learn about a tool called left_join that when combined with if_else lets you rename everything based on the LLM output in two lines of code.\nAlso Notice that there are some ingredients that appear to be two or more ingredients strung together with commas. These would be candidates for more cleaning though this exercise doesn’t ask you to fix them.\nProblem 5: Some operations are easier to do on wide data rather than tall data. Find the 10 most common pairs of ingredients occurring in the top 2 ingredients in a recipe (using the renamed data). It is much easier to do this with a wide dataset, so use pivot_wider to change the data so that each row contains all of the ingredients of a single cocktail, just like in the format of the original data-set. Then use count on the 1 and 2 columns to determine the most common pairs (see chapter 3 for a refresher on count).\nNote: You may be interested to read about the widyr package here: widyr page. It is designed to solve problems like this one and uses internal pivot steps to accomplish it so that the final result is tidy. I’m actually unaware of any easy ways of solving problem 5 without pivoting to a wide dataset."
  },
  {
    "objectID": "assignments/tidygit/tidyextend.html",
    "href": "assignments/tidygit/tidyextend.html",
    "title": "Tidyverse Extend",
    "section": "",
    "text": "Overview\nIn this assignment, you’ll practice collaborating around a code project with GitHub. You could consider our collective work as building out a book of examples on how to use use httr2 to interact with different APIs.\nGitHub repository: https://github.com/georgehagstrom/Fall2025TIDYVERSE\nYour task here is to Extend an Existing Example. Using one of your classmate’s examples (as created above), extend his or her example with additional annotated code.\nMake sure to pull in the latest version of the repository so you have all the code examples, and make sure to pull in any changes before you commit.\nYou should also update the README.md file with a line saying your name and whose code example you picked to extend.\nAfter you’ve extended your classmate’s vignette, please submit the combined qmd with the code that you added noted to brightspace."
  },
  {
    "objectID": "assignments/participation.html",
    "href": "assignments/participation.html",
    "title": "Participation",
    "section": "",
    "text": "One Minute Papers\nA “one minute paper” (Angelo & Cross, 1993) is a short written reflection to be completed after each class meetup. You are to answer two questions: 1) What was the most important thing you learned during this class? and 2) What important question remains unanswered for you? Our goal is to give you a moment to reflect on the most important concepts presented were and to provide me with information about what concepts are still unclear. At the completion of each meetup (whether attended live or after watching the recording), complete the Google Form linked from the last slide.\n\n\nSlack\nPlease be an active participant in the Slack channel. In addition to being a good resource for asking and answer questions, we hope you begin to make connections with other students. Contribute at least one resource related to data science that you find and think would be useful to your fellow students. Please post it in the #resource channel.",
    "crumbs": [
      "Assignments",
      "Participation"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Data Project",
    "section": "",
    "text": "The idea of this project is for you to find data and do something interesting with it that relates to the skills you have developed in this course.\nThe project is intentionally open ended- this is an opportunity for you to explore your interests, through using techniques or technologies that branch off those that were introduced in this class, datasets related to problems you face in your career or that you are curious about, or both.\nA great project will demonstrate a complete “Data Science Workflow”, such as the one introduced at the beginning of the textbook, starting from multiple distinct datasets from public sources (if you have a private dataset that you want to use you may discuss that with me), data tidying, exploratory data analysis, cleaning, hypothesis generation, and lastly modeling. This course has not focused on modeling and the mathematical sophistication of your model will not factor heavily in your evaluation, but by this point in the semester you will have been exposed to enough modeling techniques in other courses to incorporate models within your project.\nThe output of your project should software that allows me to reproduce your work and a research report which describes what you did with the data and your findings, which could be contained within a github repository. You will also present your project in a short presentation in the final week of the semester.\n\n\nThe first step of your project is to submit an initial proposal, which will help to ensure that your project is both worthwhile and feasible in the time frame of the last half of the semester. Your proposal should describe the area that you are interested in (and if you have a specific question already you can describe that too), the datasets you are going to use, and your data analysis plan. Your proposal should be a maximum of 2 pages (excluding figures and references), and should include three sections (each with target length 1-2 paragraphs):\n\nSection 1 - Introduction: The introduction should describe the motivation for your project, the subject matter area of your dataset, and any general research questions you may already have. It is important to include some references to provide background information.\nSection 2 - Data: At the time of submitting the proposal you should have already identified some datasets that you plan to use in your analysis. Give a quick description of those datasets here, describe where the data can be found, the number of variables and observations, and/or the output of the glimpse() or skim() functions on the data.\nSection 3 - Data analysis plan: Describe your initial approach to analyzing your data. How will your datasets need to be processed in for your to make use of all of them in the analysis? Which sort of comparisons do you plan to make in your exploratory data analysis (you can share early visualizations of this is you have them)? What statistical methods do you think are appropriate to address the questions that motivated your interest?\n\nThe proposal is preliminary, and you can include additional data or a different analysis as needs require.\n\n\n\nWhen writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow much will it cost? (Should be until the end of the semester)\nHow long will it take? (Should be $0 for this)\nWhat are the midterm and final “exams” to check for success? How will progress be measured?\n\n\n\n\nThe emphasis of class has been working with datasets that have different types of variables and which are messy or often heterogeneous. For this project you need to select data that comes from multiple sources (there should be at least two datasets). You should be able to access the data and it should be large enough and contain enough variables so that multiple interesting relationships can be explored. A good starting point is that across your dataset, there should be at least 50 observations and more than 10 variables (exceptions can be made but you must speak with me first). The dataset’s variables should include categorical variables, discrete numerical variables, and continuous numerical variables.\nNote on reusing datasets from class: Do not reuse datasets used in examples, homework assignments, or labs in the class. Also do not use datasets exclusively from Kaggle or other sources of curated data for data science competitions (these have been made clean and tidy already).\nBelow are a list of data repositories that might be of interest to browse. You're not limited to these resources, and in fact you're encouraged to venture beyond them. But you might find something interesting there:\n\nNew York City Open Data\nFiveThirtyEight https://github.com/fivethirtyeight/data\nRStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/\nAnalyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/\nThe World Bank Data Catalog http://datacatalog.worldbank.org/\nGoogle Public Data search engine http://www.google.com/publicdata/directory\nVanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\nProgramme of International Student Assessment (PISA) http://www.oecd.org/pisa/\nBehavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/\nWorld Values Survey http://www.worldvaluessurvey.org/wvs.jsp\nAmerican National Election Survey (ANES) http://www.electionstudies.org/\nGeneral Social Survey (GSS) http://www3.norc.org/GSS+Website/\nIntegrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/\nU.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html\n10 Standard Datasets for Practicing Applied Machine Learning\nAwesome Public Datasets\nUCI Machine Learning Repository - See also this R package: https://github.com/tyluRp/ucimlr\nOpenML\nBikeshare data portal\nUK Gov Data\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nAmerican University Government Data Archive There has been not only a government shutdown but also some removal of data. This website contains a list of different archives in case you can’t find what you need on a .gov site\nNicholas Decker’s Economics Dataset Collection A curated list of datasets useful for economics applications.\n\n\n\n\nThe project report should include an Abstract which gives a summary of your project in under 300 words, and Introduction, a section on the Data which describes the cleaning, tidying, and exploratory data analysis steps you took and what hypotheses you generated, a section on the Data Analysis, and a Conclusion section. All of the code you used to perform your cleaning and analysis should be included either in a github repository or in your report (if you submit in a quarto format for example). You will be evaluated using the following rubric:\n\n\n\nYou will record a short video presentation with a slide-deck presenting your project. It should cover the motivation for your project, describe the data and analysis, and show visualizations of your main conclusions. Each student is required to watch two of the uploaded presentations and provided anonymous feedback on the presentations.\n\n\n\n\n\n\n\n\n\n\n\n\nDomain\nAccomplished (100%)\nProficient (80%)\nNeeds Improvement (60%)\n\n\n\n\nProposal (20 pts)\nMotivation explained well, data science workflow clearly present, datasets and proposed analysis appropriate\nDescription of motivation, workflow, datasets, and analysis present but a few are not appropriate or unclear\nSeveral of motivation, workflow, datasets, and analysis are missing or inappropriate\n\n\nAbstract (5 pts)\nAbstract is less than 300 words, free of grammatical errors, summarizes the project analysis, conclusions, and implications\nNA\nNA\n\n\nIntroduction (20 pts)\nClear explanation of motivation for the project, choice of data, and analysis methods/workflow.\nRationale for the project, analysis/workflow, or choice of data is present but unclear.\nRationale is unstated.\n\n\nDatasets and Wrangling (30 pts)\nMultiple datasets used with different data types, data is properly tidied, reproducible code for tidying\nOne of the previous criteria is missing.\nSeveral criteria not met: insufficient datasets and types, incorrect tidying, non-reproducible code\n\n\nExploratory Data Analysis (30 pts)\nAppropriate summary statistics and visualization of distributions/covariance, hypotheses, and treatment of missing values/outliers, code reproducible\nEDA completed but some of summary statistics, visualizations, hypotheses, and missing data treatment not appropriate\nEDA substantially inappropriate or has several aspects missing\n\n\nData Display (20 pts)\nIncludes appropriate, well-labeled, accurate displays (graphs and tables) of the data.\nIncludes appropriate, accurate displays of the data.\nIncludes appropriate but no accurate displays of the data.\n\n\nStatistical Model (5 pts)\nstatistical test(s) was used for the data and interpretation was clear.\nThe appropriate statistical test(s) was used but interpretation was not fully clear or well articulated.\nThe incorrect statistical test was used an/or not justified for the data as presented.\n\n\nConclusion (20 pts)\nConclusion includes a clear description of results consistent with the EDA and statistical modeling and discusses limitations of analysis and how to go further.\nConclusion describes results, limitations, and potential future steps but some descriptions inappropriate or unclear\nDescription of results, limitations, and future steps missing and/or unclear\n\n\nOverall Presentation (20 pts)\nAttractive, well-organized, well-written presentation\nPresentation has two of the three qualities: attractive, well-organized, well-written.\nPresentation is not attractive, organized, or written. There are numerous errors throughout.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#heilmeiers-questions",
    "href": "assignments/project.html#heilmeiers-questions",
    "title": "Data Project",
    "section": "",
    "text": "When writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow much will it cost? (Should be until the end of the semester)\nHow long will it take? (Should be $0 for this)\nWhat are the midterm and final “exams” to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#data",
    "href": "assignments/project.html#data",
    "title": "Data Project",
    "section": "",
    "text": "The emphasis of class has been working with datasets that have different types of variables and which are messy or often heterogeneous. For this project you need to select data that comes from multiple sources (there should be at least two datasets). You should be able to access the data and it should be large enough and contain enough variables so that multiple interesting relationships can be explored. A good starting point is that across your dataset, there should be at least 50 observations and more than 10 variables (exceptions can be made but you must speak with me first). The dataset’s variables should include categorical variables, discrete numerical variables, and continuous numerical variables.\nNote on reusing datasets from class: Do not reuse datasets used in examples, homework assignments, or labs in the class. Also do not use datasets exclusively from Kaggle or other sources of curated data for data science competitions (these have been made clean and tidy already).\nBelow are a list of data repositories that might be of interest to browse. You're not limited to these resources, and in fact you're encouraged to venture beyond them. But you might find something interesting there:\n\nNew York City Open Data\nFiveThirtyEight https://github.com/fivethirtyeight/data\nRStudio data sources http://blog.rstudio.org/2014/07/23/new-data-packages/\nAnalyze Survey Data for Free (ASDFree) has many open data sources that can be used http://www.asdfree.com/\nThe World Bank Data Catalog http://datacatalog.worldbank.org/\nGoogle Public Data search engine http://www.google.com/publicdata/directory\nVanderbilt data sources http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\nProgramme of International Student Assessment (PISA) http://www.oecd.org/pisa/\nBehavioral Risk Factor Surveillance System (BRFSS) http://www.cdc.gov/brfss/\nWorld Values Survey http://www.worldvaluessurvey.org/wvs.jsp\nAmerican National Election Survey (ANES) http://www.electionstudies.org/\nGeneral Social Survey (GSS) http://www3.norc.org/GSS+Website/\nIntegrated Postsecondary Education Data System (IPEDS) https://nces.ed.gov/ipeds/\nU.S. Census and American Community Survey https://cran.r-project.org/web/packages/acs/index.html\n10 Standard Datasets for Practicing Applied Machine Learning\nAwesome Public Datasets\nUCI Machine Learning Repository - See also this R package: https://github.com/tyluRp/ucimlr\nOpenML\nBikeshare data portal\nUK Gov Data\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nAmerican University Government Data Archive There has been not only a government shutdown but also some removal of data. This website contains a list of different archives in case you can’t find what you need on a .gov site\nNicholas Decker’s Economics Dataset Collection A curated list of datasets useful for economics applications.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#project-report-and-code-repository",
    "href": "assignments/project.html#project-report-and-code-repository",
    "title": "Data Project",
    "section": "",
    "text": "The project report should include an Abstract which gives a summary of your project in under 300 words, and Introduction, a section on the Data which describes the cleaning, tidying, and exploratory data analysis steps you took and what hypotheses you generated, a section on the Data Analysis, and a Conclusion section. All of the code you used to perform your cleaning and analysis should be included either in a github repository or in your report (if you submit in a quarto format for example). You will be evaluated using the following rubric:",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#presentation",
    "href": "assignments/project.html#presentation",
    "title": "Data Project",
    "section": "",
    "text": "You will record a short video presentation with a slide-deck presenting your project. It should cover the motivation for your project, describe the data and analysis, and show visualizations of your main conclusions. Each student is required to watch two of the uploaded presentations and provided anonymous feedback on the presentations.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#overall-grading-rubric",
    "href": "assignments/project.html#overall-grading-rubric",
    "title": "Data Project",
    "section": "",
    "text": "Domain\nAccomplished (100%)\nProficient (80%)\nNeeds Improvement (60%)\n\n\n\n\nProposal (20 pts)\nMotivation explained well, data science workflow clearly present, datasets and proposed analysis appropriate\nDescription of motivation, workflow, datasets, and analysis present but a few are not appropriate or unclear\nSeveral of motivation, workflow, datasets, and analysis are missing or inappropriate\n\n\nAbstract (5 pts)\nAbstract is less than 300 words, free of grammatical errors, summarizes the project analysis, conclusions, and implications\nNA\nNA\n\n\nIntroduction (20 pts)\nClear explanation of motivation for the project, choice of data, and analysis methods/workflow.\nRationale for the project, analysis/workflow, or choice of data is present but unclear.\nRationale is unstated.\n\n\nDatasets and Wrangling (30 pts)\nMultiple datasets used with different data types, data is properly tidied, reproducible code for tidying\nOne of the previous criteria is missing.\nSeveral criteria not met: insufficient datasets and types, incorrect tidying, non-reproducible code\n\n\nExploratory Data Analysis (30 pts)\nAppropriate summary statistics and visualization of distributions/covariance, hypotheses, and treatment of missing values/outliers, code reproducible\nEDA completed but some of summary statistics, visualizations, hypotheses, and missing data treatment not appropriate\nEDA substantially inappropriate or has several aspects missing\n\n\nData Display (20 pts)\nIncludes appropriate, well-labeled, accurate displays (graphs and tables) of the data.\nIncludes appropriate, accurate displays of the data.\nIncludes appropriate but no accurate displays of the data.\n\n\nStatistical Model (5 pts)\nstatistical test(s) was used for the data and interpretation was clear.\nThe appropriate statistical test(s) was used but interpretation was not fully clear or well articulated.\nThe incorrect statistical test was used an/or not justified for the data as presented.\n\n\nConclusion (20 pts)\nConclusion includes a clear description of results consistent with the EDA and statistical modeling and discusses limitations of analysis and how to go further.\nConclusion describes results, limitations, and potential future steps but some descriptions inappropriate or unclear\nDescription of results, limitations, and future steps missing and/or unclear\n\n\nOverall Presentation (20 pts)\nAttractive, well-organized, well-written presentation\nPresentation has two of the three qualities: attractive, well-organized, well-written.\nPresentation is not attractive, organized, or written. There are numerous errors throughout.",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpam Email Classification using Ellmer and Malls\n\n\n\n\n\nClick here for a video coding vignette using the ellmer and mall packages to classify text messagess into spam or legitimate\n\n\n\n\n\nNov 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating LLMs for R Coding using Vitals\n\n\n\n\n\nClick here for a video coding vignette using the vitals package to evaluate different large language models on the are ggplot2 coding problem dataset\n\n\n\n\n\nNov 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 12 Extras: Structured Chats and Tool Calling\n\n\n\n\n\nClick here for a video which covers extra slides from Meetup 12, namely structured chats and tool calling\n\n\n\n\n\nNov 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12 Info: Large Language Models\n\n\n\n\n\nClick here for info on Week 12\n\n\n\n\n\nNov 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Movie Reviews Sentiment Analysis Using Bigrams\n\n\n\n\n\nClick here for a video coding vignette on sentiment analysis and bigrams\n\n\n\n\n\nNov 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11 Info: Tidy Text and Natural Language Processing\n\n\n\n\n\nClick here for info on Week 11\n\n\n\n\n\nNov 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGithub and RStudio Project Coding Vignette\n\n\n\n\n\nClick here for a link to a video (in two parts) covering the basics of git and how to interact with github through RStudio\n\n\n\n\n\nOct 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nhttr2 EPA Air Quality Coding Vignette\n\n\n\n\n\nClick here for a link to two videos showing how to use httr2 to extract data from the EPA AQS API\n\n\n\n\n\nOct 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10 Info: Collaboration, git, and APIs\n\n\n\n\n\nClick here for info on Week 10\n\n\n\n\n\nOct 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping Vignette\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video which goes through three examples of webscraping using rvest\n\n\n\n\n\nOct 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9 Info: Webscraping and Hierarchical Data\n\n\n\n\n\nClick here for info on Week 9\n\n\n\n\n\nOct 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFunctional Programming with Purrr\n\n\n\n\n\nClick here for a link a video on functional programming and the purrr package variation of baby names in the US over time\n\n\n\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8 Info: Functions and Iteration and Project Proposals\n\n\n\n\n\nClick here for info on Week 8\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignettes on Joins and Databases and a SQL Lecture\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo focused on debugging and reprex\n\n\n\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7 Info: Joins and Databases\n\n\n\n\n\nClick here for info on Week 7\n\n\n\n\n\nOct 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Changes in US Baby Names Using stringr, forcats, and regex\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video which illustrates the use of the stringr and forcats libraries as well as regular expressions to explore the variation of baby names in the US over time\n\n\n\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6 Info: Working With Text and Strings\n\n\n\n\n\nClick here for info on Week 6\n\n\n\n\n\nSep 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 5 Continuation: Correlation Trick\n\n\n\n\n\nClick here for a video on the correlation trick\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to find help and make a reproducible example (Debugging)\n\n\n\n\n\nClick here for a link to a code vignette and a youtube video of a coding demo focused on debugging and reprex\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5 Info: Data Transformations\n\n\n\n\n\nClick here for info on Week 5\n\n\n\n\n\nSep 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 4 Extra Video\n\n\n\n\n\nClick here to watch the material in Meetup 4 that we did not get to on Monday.\n\n\n\n\n\nSep 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 4 Slides\n\n\n\n\n\nClick here to download slides for Meetup 4.\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4 Info: Exploratory Data Analysis\n\n\n\n\n\nClick here for info on Week 4\n\n\n\n\n\nSep 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPivot Vignette\n\n\n\n\n\nClick here to watch a coding demonstration on different types of pivoting and tidying.\n\n\n\n\n\nSep 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 3 Slides\n\n\n\n\n\nClick here to download slides for Meetup 3.\n\n\n\n\n\nSep 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3 Info: Working with Tidy Data\n\n\n\n\n\nClick here for info on Week 3\n\n\n\n\n\nSep 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Meetup Video and Coding Vignette\n\n\n\n\n\nClick here to watch the Meetup 2 video and the additional content (completed transit cost vignette and graphics quality)\n\n\n\n\n\nSep 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 2 Slides\n\n\n\n\n\nClick here to download slides for Meetup 2.\n\n\n\n\n\nSep 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2 Info: Data Visualizations and Transformations\n\n\n\n\n\nClick here for info on Week 2\n\n\n\n\n\nAug 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 1 Slides\n\n\n\n\n\nClick here to download slides for Meetup 1.\n\n\n\n\n\nAug 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DATA 607\n\n\n\n\n\nImportant information on how to get started with this course. Click this post now for instructions on getting started.\n\n\n\n\n\nAug 17, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "The primary homework assignments in this course are lab assignments where you will use R and occassionally other software to acquire, explore, wrangle, and manage different data sets. Please submit a PDF (preferred) or HTML file along with your qmd file. Labs should be submitted on Blackboard.\n\n\nIntroduction to Data Visualization using ggplot (Template)\n\n\nTidy Data (Template)\n\n\nExploratory Data Analysis (Template)\n\n\nData Transformations (Template)\n\n\nProcesing Text and Strings (Template)\n\n\nR and SQL (Template)\n\n\nRectangling and Web Scraping (Template)\n\n\nText Mining and NLP (Template)\n\n\nWorking with LLMs in R (Template)\n\n\nLarge Datasets and Cloud Computing (Template)",
    "crumbs": [
      "Assignments",
      "Labs"
    ]
  },
  {
    "objectID": "assignments/dsincontext.html",
    "href": "assignments/dsincontext.html",
    "title": "Data Science in Context Presentations",
    "section": "",
    "text": "Overview\nEach student needs to complete one data science in context presentation. These are short presentations with a target length of 5-7 minutes. At the beginning of the semester you should sign up for a presentation slot using this sign up sheet. The presentations will be made live in class, with an opportunity for a few minutes of question or discussion. You can pick any topic that interests you and is related to data science. If you have a strong preference not to present live, you may record and upload your presentation separately.",
    "crumbs": [
      "Assignments",
      "Data Science in Context Presentation"
    ]
  },
  {
    "objectID": "assignments/tidygit/tidycreate.html",
    "href": "assignments/tidygit/tidycreate.html",
    "title": "Tidyverse Create",
    "section": "",
    "text": "Overview\nIn this assignment, you’ll practice collaborating around a code project with GitHub. Each student will select a website API from the list of public APIs curated on this github repository. I recommend choosing a well documented repository which allows you to access data that you can load into R and visualize. If you know of a website with an API that isn’t listed here, you may use it but be aware that this list is curated so there is a better chance that it will be easy to learn how to use the APIs on the websites listed here. I recommend choosing an API that doesn’t require authentication. If authentication is required, make sure not to share you credentials.\nUsing your selected API, you will create a vignette (in the form of a qmd file) using the httr2 library: https://httr2.r-lib.org/. This vignette should include three sections:\n\nA description of the API that you selected and the data it contains, along with a question that you are interested in exploring using the API\nCode that uses httr2 and other tidyverse functions which downloads the data you are interested in from the website and performs a simple visualization or analysis to address your question. Make sure to describe how your code works with the API.\nA conclusion section where you discuss the meaning of your visualization and/or analysis.\n\nYou should incorporate your vignette into a shared github repository which will contain all of the examples. I have created the following repository for you to use for this assignment:\nhttps://github.com/georgehagstrom/Fall2025TIDYVERSE\nYou should clone the provided repository. In order to push code to the repository, you will need to be added as a collaborator, which requires you to send me your github username (preferably over slack). You should also update the README.md file with your example. You may want to review Section 25 of Happy git and github for the useR. The github repo I have setup is to be organized following the first “ours” model.\nAfter you’ve created your vignette and uploaded it to github, render your pdf file and submit it along with your github handle to brightspace.\nIn several weeks, there will be a followup assignment where we will re-use the work submitted for this assignment and where you will select a vignette uploaded by one of your classmates and extend it by adding additional analyses.\nIf you have not yet done so, you will need to create a github account. There are many ways to work with git and R together, but one way I recommend is to follow the tutorial in the book Happy git and gitHub for the useR. To interface with our github repository use section 15.2 and create a project from an existing github repository."
  },
  {
    "objectID": "assignments/labs/Lab3.html",
    "href": "assignments/labs/Lab3.html",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "",
    "text": "This is a two part lab where each part will focus on a different dataset: the first part will use a dataset containing a series of diagnostic measurements taken on members of the Akimel O’odham people (an indigenous group living in the Southwestern United States who are also called the Pima) to understand diabetes risk (click here to download diabetes.csv), and the second dataset contains information on traffic accidents in New York City in the months of July and August of this year, and was compiled by NYC Open Data (click here to download crashes.csv).\nFor this problem set you will need to install the skimr and GGally packages, and in particular the functions skim and ggpairs.\nWe will also explore the concept of an inlier, which is an erroneous value that occurs in the interior of the distribution of a variable, rather than in the tails of the variable. The US Census published an article on the problem of inliers here"
  },
  {
    "objectID": "assignments/labs/Lab3.html#overview",
    "href": "assignments/labs/Lab3.html#overview",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "",
    "text": "This is a two part lab where each part will focus on a different dataset: the first part will use a dataset containing a series of diagnostic measurements taken on members of the Akimel O’odham people (an indigenous group living in the Southwestern United States who are also called the Pima) to understand diabetes risk (click here to download diabetes.csv), and the second dataset contains information on traffic accidents in New York City in the months of July and August of this year, and was compiled by NYC Open Data (click here to download crashes.csv).\nFor this problem set you will need to install the skimr and GGally packages, and in particular the functions skim and ggpairs.\nWe will also explore the concept of an inlier, which is an erroneous value that occurs in the interior of the distribution of a variable, rather than in the tails of the variable. The US Census published an article on the problem of inliers here"
  },
  {
    "objectID": "assignments/labs/Lab3.html#part-1-health-diagnostics-and-diabetes-incidence",
    "href": "assignments/labs/Lab3.html#part-1-health-diagnostics-and-diabetes-incidence",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "Part 1: Health Diagnostics and Diabetes Incidence",
    "text": "Part 1: Health Diagnostics and Diabetes Incidence\nProblem 1: Data Description and Outliers.\nLoad diabetes.csv into R and take a look at the data using the skimr package (make sure to install it if you don’t have it). Skimr provides a tidy summary function called skim. Use skim on the data frame that you loaded from diabetes.csv.\nSkim will list several variables. Pregnancies is the past number of pregnancies (this dataset includes women 21 years or older), glucose describes the concentration of glucose in the blood after an oral glucose tolerance test (drinking a sugary drink and measuring two hours later), skin thickness is the result of a skinfold thickness test taken at the triceps (upper arm), Insulin is the insulin concentration in the blood taken at the same time as the glucose measurement (Insulin is a hormone that transports glucose into cells), BMI is “Body Mass Index”, Diabetes Pedigree Function is a measure of diabetes risk based on the family history of diabetes for each patient (this is an engineered feature) and outcome is equal to 1 if the patient was diagnosed with diabetes with 5 years and 0 otherwise.\n\nSkim should show no missing data, but should indicate potential data issues. Do any of the percentile ranges (p0, p25, p50, p75, or p100) for the reported variables suggest a potential problem?\nFurther investigate the dataset to find potentially problematic variables using a qq-plot (geom_qq) or group_by combined with count and arrange. For which variables do you find repeated values and what are those values? Do you believe these values represent real measurements or could they correspond to missing data? Do the repeated variables occur in the same rows or different rows?\n\nWrite an overview of which values are missing and replace all missing values with NA for the next stage of analysis.\n\nPerform Tukey Box plots on each variable to identify potential outliers. Which variables have the most outliers? Are there any outliers that you think come from measurement error? If so remove them.\n\nProblem 2: Pair Plots\n\nUse the GGally package and its function ggpair on both the original dataset and the cleaned dataset. Which correlations change the most? What are the strongest correlations between variables overall and with the Outcome?\nLLM Prompting Exercise: Pair plots can often format poorly. Look at your pair plot and identify any deficiencies, which could be issues with legibility of variable names, axis labels, or the visibility of the visual elements itself. Try to see if you can improve some of these issues using an LLM of your choice. Report which LLM you used and what prompt you gave in addition to showing the changes to the figure.\nRemark: This dataset has been used as a model dataset for the construction of binary classifiers using machine learning and there are a large number of published studies showing these analyses. However, many of these analyses did not exclude the missing values erroneously coded as zero, as is discussed in this interesting paper by Breault, leading to highly degraded accuracy."
  },
  {
    "objectID": "assignments/labs/Lab3.html#part-2-car-crashes-in-nyc",
    "href": "assignments/labs/Lab3.html#part-2-car-crashes-in-nyc",
    "title": "Lab3: Exploratory Data Analysis",
    "section": "Part 2: Car Crashes in NYC",
    "text": "Part 2: Car Crashes in NYC\nProblem 3: Finding Inliers and Missing Data\nLoad the NYC car crash dataset using read_csv. You can download the data from the course website by clicking here.\n\nWhich variables have missing data (use skim or another tool of your choosing)? Some missing values have a different interpretation than others- what does it mean when VEHICLE TYPE CODE 2 is missing compared to LATITUDE?\nLatitude and Longitude have the same number of missing values. Verify that they always occur in the same row. Check the counts of latitude and longitude values- do you find any hidden missing values? If so recode them as NA.\nMany of the geographic values are missing, but geographic information is redundant in multiple variables in the dataset. For example, with effort you could determine the borough of an accident from the zip code, the latitude and longitude, or the streets (not part of the assignment for this week). Consider the borough variable- what percentage of the missing values of borough have values present of at least one of zip code or latitude. What about if we include all the street name variables? What fraction of rows don’t have any detailed location information (latitude, zip code, or street names)?\nThe CRASH TIME variable has no missing values. Compute the count of how many times each individual time occurs in the crash data set. This will suggest that there are some inliers in the data. Compute summary statistics on the count data, and determine how many inliers there are (define an inlier as a data value where the count is an outlier, i.e. the count of that value is greater than 1.5*IQR + P75, i.e. 1.5 times the interquartile range past the 75th percentile for the distribution of counts for values of that variable.) For which inliers do you believe the time is most likely to be accurate? For which is it least likely to be accurate and why do you think so?\n\nProblem 4: Finding Patterns in the Data\nFormulate a question about crash data in NYC and make visualizations to explore your question. It could be related to the geographical distribution of accidents, the timing of accidents, which types of vehicles lead to more or less dangerous accidents, or anything else you want. Write comments/notes describing your observations in each visualizations you create and mention how these observations impact your initial hypotheses.\nUseful questions to consider when you observe a pattern (not all of them might apply to your case):\n\nCould this pattern be due to coincidence (i.e. random chance)?\nHow can you describe the relationship implied by the pattern?\nHow strong is the relationship implied by the pattern?\nWhat other variables might affect the relationship?\nDoes the relationship change if you look at individual subgroups of the data?"
  },
  {
    "objectID": "assignments/labs/labData/sports_program_data_dictionary.html",
    "href": "assignments/labs/labData/sports_program_data_dictionary.html",
    "title": "Sports Program Data Dictionary",
    "section": "",
    "text": "Year Year data was collected\nunitid Unique code which identifies institutions\ninstitution_name Name of institution\ncity_txt City in which institution is located\nstate_cd 2 digit state code identifying the university\nzip_text 5 digit zip code where the university is located\nclassification_code Numerical code which identifies the sports division in which the school belongs. Note that these are structured by the overall division (Division I, II, III etc) and also some other features such as whether there is an American Football team and which grouping that football team competes in.\nclassification_name Name of the classification the university sports teams belong to\nclassification_other Description of classification if university belongs to a different sports league than the one enumerated previously\nef_male_count Count of males in the student body\nef_female_count Count of females in the student body\nef_total_count Total number of students at the institution\nsector_cd Numeric code identifying the type of university\nsector_name Description of university type\nsportscode Numeric code identifying the sport that the team plays\npartic_men Number of men participating on men’s team\npartic_women Number of women participating on women’s team\npartic_coed_men Number of men participating on coed team\npartic_coed_women Number of women participating on coed team\nsum_partic_men Sum of men participating in this sport\nsum_partic_women Sum of women participating in this sport\nrev_men Revenue generated by Men’s Team\nrev_women Revenue generated by Women’s Team\ntotal_rev_men_women Revenue across all teams\nexp_men Expenses for men’s team\nexp_women Expenses for women’s team\ntotal_exp_menwomen Expenses across both teams\nsports Name of sport"
  },
  {
    "objectID": "assignments/labs/labData/sports_program_data_dictionary.html#list-of-variables",
    "href": "assignments/labs/labData/sports_program_data_dictionary.html#list-of-variables",
    "title": "Sports Program Data Dictionary",
    "section": "",
    "text": "Year Year data was collected\nunitid Unique code which identifies institutions\ninstitution_name Name of institution\ncity_txt City in which institution is located\nstate_cd 2 digit state code identifying the university\nzip_text 5 digit zip code where the university is located\nclassification_code Numerical code which identifies the sports division in which the school belongs. Note that these are structured by the overall division (Division I, II, III etc) and also some other features such as whether there is an American Football team and which grouping that football team competes in.\nclassification_name Name of the classification the university sports teams belong to\nclassification_other Description of classification if university belongs to a different sports league than the one enumerated previously\nef_male_count Count of males in the student body\nef_female_count Count of females in the student body\nef_total_count Total number of students at the institution\nsector_cd Numeric code identifying the type of university\nsector_name Description of university type\nsportscode Numeric code identifying the sport that the team plays\npartic_men Number of men participating on men’s team\npartic_women Number of women participating on women’s team\npartic_coed_men Number of men participating on coed team\npartic_coed_women Number of women participating on coed team\nsum_partic_men Sum of men participating in this sport\nsum_partic_women Sum of women participating in this sport\nrev_men Revenue generated by Men’s Team\nrev_women Revenue generated by Women’s Team\ntotal_rev_men_women Revenue across all teams\nexp_men Expenses for men’s team\nexp_women Expenses for women’s team\ntotal_exp_menwomen Expenses across both teams\nsports Name of sport"
  },
  {
    "objectID": "assignments/labs/labData/imdb_train.html",
    "href": "assignments/labs/labData/imdb_train.html",
    "title": "DATA 607 Fall 2025",
    "section": "",
    "text": "rating, review\n1, “This is one of the worse cases of film drivel I have seen in a long while. It is so awful, that I am not sure where to begin, or even if it is worth it. The plot is the real problem, and I feel sorry for ‘Sly’ as he puts in a decent performance for his part. But that plot … Oh dear oh dear. I particularly love the way near the end he manages to pop from the foot of a mountain to the top, whilst the helicopter is on the way. A climb of a day or two takes him all of five minutes! I could go on: but it isn’t worth it. Apart from the grim opening (which even a five year old would be able to predict the outcome of) the rest is drivel. Sorry folks, but this is about as bad as film making gets.”\n2, “Ironically, although he can do the”“splits”“, Thomas is a complete stiff as an actor. This film is seared into my memory as one of the most side-splittingly cheesy and incompetent movies I have ever seen. As such, I’m actually rather fond of it. Still,the only reason this gets more than one star is that Thomas is great shape, and it’s fun to see his tiny, muscular body performing various feats of gymnastic skill on the oddly shaped rocks and poles scattered about the East European country side (including the infamous”“pommel horse”” shaped well cap in the middle of a village square that Thomas uses to plant his feet in the faces of various insane villagers). But let the poor guy open his mouth and try to emote, and any illusion that he might have a film career is immediately dashed to bits. Thomas at least had the excuse that he wasn’t really an actor. Everyone else in the film - actors, director, editor, camera guys, etc. is at least as bad, or even worse, and most of them are professionals. So Kurt doesn’t come off quite as badly as you might think. I hope poor Kurt took the money and ran. If anyone ever asks him to perform in a martial arts film again, I’ll bet that Thomas kicks the guy in the face.”\n2, “““Smithereens”” is the kind of worthless flick which just hangs out among the cable channels taking up space like a cheesy dime novel in the public library. A worthless bit of tripe and first effort for mediocre director Seidelman, the film is fraught with bad acting, bad sound, bad camera work, and poor quality in all aspects of the film. Many better films never make it to market and why junk flicks like this one do and never seem to go away is one of life’s great mysteries. (D-)”\n3, “I bet you Gene Simmons and Vincent Pastore negotiated in advance how many episodes they would be willing to appear in. Isn’t just too contrived for Gene to switch to the ladies team and then throw himself on his sword? And Big Pussy? What the hell was that”“look at me, I’m a rat!”” double episode crap? All that cliché mafia banter- COME ON! The big names voted off just happened to already have received money for their charity and got a custom tailored exit. Hmm… This is not reality but staged drama! Mark Burnett’s other show, ““Survivor”” also raised questions for me when Johnny Fairplay stages his departure when he clearly had just a short time before his child is to be born.Yuk!”\n10, “I saw The Glacier Fox in the theatre when I was nine years old - I bugged my parents to take me back three times. I began looking for it on video about five years ago, finally uncovering a copy on an online auction site, but I would love to see it either picked up by a new distributor and rereleased (I understand the original video run was small), or have the rights purchased by The Family Channel, Disney, etc. and shown regularly. It is a fascinating film that draws you into the story of the life struggle of a family of foxes in northern Japan, narrated by a wise old tree. The excellent soundtrack compliments the film well. It would be a good seller today, better than many of the weak offerings to children’s movies today.”\n4, “I must say that I am fairly disappointed by this”“horror”” movie. I did not get scared even once while watching it. It also is not very suspenseful either…. I was able to guess the ending half way through the movie… So.. what’s left?““The Ring”” is a trully scary movie… I wish other movies would stop copying from it (e.g. the trade-mark: long hair). Please give me some originality.Will not recommend this movie.”\n1, “Good Deaths. Good Mask. Cool Axe. Good Looking Girls….But Watch Out!!! No Plot and Little Scares Completely lower it’s Standards. They Tried to make an”“I Know what you Did Last Summer”“, but ended up making A”“Scream”“. But Hey, What do people Expect From a Horror Movie? Answers Totally Vary. Rent It If You Want, but I Regret Ever Seeing It.”\n9, “La Coda Dello Scorpione (a.k.a. Case of the Scorpion’s Tail) was director Sergio Martino’s follow-up to the wonderful giallo Strano vizio della Signora Wardh. This is the quintessential giallo, featuring all the aspects fans of the genre have come to know and love. Twisty plot, beautiful girls, black gloves, sharp blades, and a bit of gore all come together to make one heck of a piece of Italian exploitation.A group of gialli favorites, both in front of and behind the camera, work to make this one of the best non-Argento gialli around. There’s the aforementioned Martino adding his touches as director, giallo great Ernesto Gastaldi as the writer, Bruno Nicolai creating the music, and a host of giallo stars and starlets, such as George Hilton, character actor Luigi Pistilli, and the fetching Anita Strindberg.With all this talent behind it, does Scorpione deliver? You bet. The film works on many different levels. It’s a thrilling murder mystery, a tense and violent horror film, and a suspenseful thriller. All in all one of the best gialli around.Martino definitely knows what fans want when it comes to gialli. At some points in the film, he almost seems to be channeling Argento in his approach. For example, there is a direct rip-off of the scene in Bird With the Crystal Plumage where the killer tries to break through the door, that actually outdoes Argento’s flick.Are there any problems with the flick? Hmmm… only minor ones. First, any scenes that aren’t following the murders or the budding romance between the two leads begin to bore. But just before you fall asleep, the killer will pop out of nowhere and you’ll be right back in the swing of things.Also, towards the end, the twists get a little too bizarre. I mean, what purpose did the scorpion pins really serve? If you don’t play close attention to the dialogue, you could easily become lost with the twisting, weaving storyline.But these minor quibbles aside, La Coda Dello Scorpione is a tense, suspenseful, classy and all around entertaining film for giallo fans. Seek it out!”\n1, “As big as a Texas prairie and equally as boring. Even Liz Taylor, James Dean, Chill Wills, and Dennis Hopper can’t float this overbloated boat. Taylor actually LOOKS bad–wrong wardrobe, wrong hair, and wrong makeup–a unique accomplishment in her remarkable career. Hopper gives the only believable performance, and Dean in the climactic scene displays remarkable talent as something we usually don’t remember him for–a comic actor. Rock Hudson is his usual prototype of Barbie Doll Ken and makes one wonder what a, say, Redford could have done with the male lead. There is no discernible plot that provides any tension until the final twenty minutes, just a pastiche of milestones that have little relationship to each other. Except for Hopper, there is no character development, only a collection of cardboard cutouts that pop up periodically for no discernible reason like random targets in a shooting gallery. To its credit, the film does tackle racism and sexism at a time when they were taboo subjects, and it does have SIZE, making it an excellent choice for ridding yourself of unwelcome house guests. Those with the DVD version can spare themselves some of the tedium by starting with the second disk. You won’t be missing anything of interest.” 3, “Cheap and mind-blisteringly dull story and acting. Not a single good line, not even a line bad enough to be good, and no memorable delivery. Even the blooper reel included with the DVD showed how inept the actors were and how little fun any of them were having. The esoteric and occult basis was apathetically inauthentic, and the antagonists failed to be creepy or believable. The ‘homoerotic’ overtones were pointlessly tame and dissatisfying, and were limited to young boys caressing their chests while flaccid in their boxers. I’m not gay enough to appreciate it, but a little action might have at least kept me and my girlfriend awake.” 7, “I work in a library and expected to like this movie when it came out 5 years ago. Well I liked Parker Posey a lot (she’s a wonderful actress) and Omar Townsend was really cute as her boyfriend (he couldn’t act but when you look like him who cares?) but the movie was bad. It wasn’t funny or cute or much of anything. Posey kept the movie afloat with her energy. But she learned the Dewey Decimal system OVERNIGHT and then shelves tons of books to the beat of music??!!!!??? Come on! Also I did have a problem with the way she looked when she became a full-fledged librarian at the end–hair in a bun, glasses, no sense of humor–can we let that stereotype go please? Worth seeing for Posey and Townsend but that’s about it. The TV series was much better.” 2, “REALLY? REALLY???? I know if you make a political war movie you will get noticed but this movie was just garbage. Horrible in every sense. Terribly inaccurate in so many ways. I have an easier time believing the president of the United States suiting up, flying a jet fighter, and shooting down aliens. It is easier to note the few things that were right. My jaw dropped when I saw some one say that this movie was the best in the last 25 years. It was overacted, seemingly pointless plot diversions, and had questionable cinematography at times.X-box, YouTube, ACUPAT utilities did anyone check that these things did not exist in 2004? It’s not like you had to do extensive research, it was only five years before this movie came out. I am an Iraq war Veteran and if you spent ONE day with an infantry platoon or an EOD squad you would realize how B.S. this movie is. To compare this to Platoon or Saving Private Ryan is ludicrous. Why don’t you just throw Commando and Red Dawn in there too; I think those might be more accurate. If for some reason you can see past the unbelievable plot, the historical and factual discrepancies, then this movie might just be OK. Nothing more. If you keep on hearing”“Oscar buzz”“, and have to add your own pompous review, go right ahead. As for me, I am writing the director to see if I can get my 131 minutes back.” 4, “Andie McDowell is beautiful as the 40-ish woman whose late start at a serious relationship leads her to a considerably younger man and a subsequenet falling-out with 2 long-time best girldfriends.Seeing a gigolo/gold-digger in the sincere young man, the”“girl-friends”“, dead-set on terminating this”“silly relationship”“, go over and beyond the call of duty in”“helping out”” their friend (who obviously is blinded by this gigolo’s tricky game”“.A short succession of situations is absolutely ridiculous. Far fetched no longer covers it. Without these unbelievable scenes, there may have been hope for a sweet love story. Instead, all the viewer is left with is an involuntary shaking of head – these things just don’t happen! Without giving away cliff-hanger details, I warn the viewer of having high expectations for this film; most (like me) will be very disappointed. On a scale of 1 to 10, this one ranks a weak 4 with me. There is much better material out there. This one isn’t worth your time.” 10, “Season after season, the players or characters in this show appear to be people who you’d absolutely love to hate. Is this show rigged to be that or were they chosen for the same? Each episode vilifies one single person specifically and he ends up getting killed off. You enjoy seeing them get screwed although its totally wrong and sick. You enjoy seeing them screwing others, getting screwed themselves, playing dirty, getting it back, escaping and finally getting kicked out by Trump. The amount of tears also seems to be increasing by the season.The rewards which attempt to compensate for past humiliation and suffering are also heavily reduced. In the newer seasons, its like”“You get to meet xyx who’ll lecture you about uvw”“..like who freaking cares? The characters are so hateable, collectively and individually, that you wonder if they’re paid actors? The only sane one gets to win.Watch with caution and maintain a conscience. Those are your fellow human beings in the firing line.”"
  },
  {
    "objectID": "assignments/labs/Lab9.html",
    "href": "assignments/labs/Lab9.html",
    "title": "Lab 9: Using and Evaluating LLMs",
    "section": "",
    "text": "In this assignment you will be working with several different datasets.\n\nThe first dataset is a subset of the Large Movie Reviews dataset from imdb, which is divided into two files:\n‘imdb_test.csv’\nimdb_train.md\n\nEach contains a text string which corresponds to a review and the numerical rating given to that movie.\n\nThe second dataset contains randomly selected negative Amazon reviews that have been labeled according to whether they identify certain flaws in a given product. The values of the human response are None, Quality (the product is of low overall quality, is not fit for purpose), Quality Control (the product sometimes arrives broken or differs from batch to batch), Durability (the product breaks too easily), False Advertising (the product does not match expectations based on the picture or product description). There is a “test” set and a “training” set.\n‘amazon_training_examples.csv’\n‘amazon_testing_examples.csv’\nThe third dataset (for the extra credit question) consists of a subset of negative (below 4 stars) product reviews from a selection of different products, which are identified by their SKU. Each row contains the SKU for a single product and a string containing the text of 10 negative Amazon reviews for that product, concatenated together.\n’amazon_reviews_by_product.csv”\n\n\n\n\nTo complete this lab assignment, you will need to be able to interact with LLMs using the R packages ellmer and ’vitals](https://vitals.tidyverse.org/). You may also want to use the [mall`. There are two ways to connect LLMs with R:\n\nAcquiring an API key from one of the major providers, such as anthropic, openAI, or google amongst others\nInstalling open weights models on your computer, for instance using ollama\n\nThe downside of (1) is that it can cost a small amount of money or have otherwise limited usage. Google provides one year of free AI Pro to all college students. If you subscribe to this you will be able to obtain an API key with likely sufficient usage to complete this assignment (though do be cautious about what models you use). I will provide API keys for anthropic for those who request them, but credits are limited. The downside of (2) is that the models are inferior and run more slowly. However in all cases there are cheap/fast models that can be used for this assignment (flash,flash-lite,haiku, llama3, nanogpt etc). Installation instructions and API key acquisition depends on what you choose, and there are links to compatible LLMs on the ellmer page. Ollama can be installed by clicking here. You can read about google gemini API keys here."
  },
  {
    "objectID": "assignments/labs/Lab9.html#overview",
    "href": "assignments/labs/Lab9.html#overview",
    "title": "Lab 9: Using and Evaluating LLMs",
    "section": "",
    "text": "In this assignment you will be working with several different datasets.\n\nThe first dataset is a subset of the Large Movie Reviews dataset from imdb, which is divided into two files:\n‘imdb_test.csv’\nimdb_train.md\n\nEach contains a text string which corresponds to a review and the numerical rating given to that movie.\n\nThe second dataset contains randomly selected negative Amazon reviews that have been labeled according to whether they identify certain flaws in a given product. The values of the human response are None, Quality (the product is of low overall quality, is not fit for purpose), Quality Control (the product sometimes arrives broken or differs from batch to batch), Durability (the product breaks too easily), False Advertising (the product does not match expectations based on the picture or product description). There is a “test” set and a “training” set.\n‘amazon_training_examples.csv’\n‘amazon_testing_examples.csv’\nThe third dataset (for the extra credit question) consists of a subset of negative (below 4 stars) product reviews from a selection of different products, which are identified by their SKU. Each row contains the SKU for a single product and a string containing the text of 10 negative Amazon reviews for that product, concatenated together.\n’amazon_reviews_by_product.csv”\n\n\n\n\nTo complete this lab assignment, you will need to be able to interact with LLMs using the R packages ellmer and ’vitals](https://vitals.tidyverse.org/). You may also want to use the [mall`. There are two ways to connect LLMs with R:\n\nAcquiring an API key from one of the major providers, such as anthropic, openAI, or google amongst others\nInstalling open weights models on your computer, for instance using ollama\n\nThe downside of (1) is that it can cost a small amount of money or have otherwise limited usage. Google provides one year of free AI Pro to all college students. If you subscribe to this you will be able to obtain an API key with likely sufficient usage to complete this assignment (though do be cautious about what models you use). I will provide API keys for anthropic for those who request them, but credits are limited. The downside of (2) is that the models are inferior and run more slowly. However in all cases there are cheap/fast models that can be used for this assignment (flash,flash-lite,haiku, llama3, nanogpt etc). Installation instructions and API key acquisition depends on what you choose, and there are links to compatible LLMs on the ellmer page. Ollama can be installed by clicking here. You can read about google gemini API keys here."
  },
  {
    "objectID": "assignments/labs/Lab9.html#problem-1-llms-for-sentiment-analysis",
    "href": "assignments/labs/Lab9.html#problem-1-llms-for-sentiment-analysis",
    "title": "Lab 9: Using and Evaluating LLMs",
    "section": "Problem 1: LLMs for Sentiment Analysis",
    "text": "Problem 1: LLMs for Sentiment Analysis\nIn this problem you are going to use either ellmer (with parrallel_chat_structured) or mall (with llm_sentiment) to perform sentiment analysis on the large movie reviews dataset at IMDB.\n\nSelect three different LLMs (don’t use Claude Opus or Gemini Pro, and I recommend against expensive deep thinking models but you can try things versions of sonnet, haiku, nanogpt, llama-3, or even deepseek if you have the processing power for it). Use either the mall package’s llm_sentiment function or the ellmer package’s parallel_chat_structured function. Report the system prompt that you use for each LLM. Ensure that the output of the LLMs is restricted to a integer values between 1 and 10, using options if you are using mall or type_enum if you are using ellmer. The reviews are contained in the file ‘imdb_test.csv’. You will need to put the reviews into a list where each element contains a single review in a character format to use the functions in these packages. Compute the correlation coefficient of each of the models output and create a table which shows the correlation and the cost for each model. Do the more expensive models perform better? When I performed a sentiment analysis using standard methods on this dataset, the correlation coefficient was 0.53. Compare that to the LLM performance.\nUpdate the system prompts that you used in (b) by adding examples of reviews from the imdb_train.md file (you can add text to the beginning of this file to prompt the LLM and then use the function interpolate_file to add a file to the system prompt), or by modifying the chat object to include instances of receiving movie reviews as text and outputing the correct rating. If you do include additional text in the file as a system prompt, be sure to report what text you used and to upload the file you used for your prompt. Repeat the sentiment analysis of the part (b) using these modified prompts, and again compute the correlation and the cost, outputing them in a table. How did enhancing the prompts change the results? Was it model dependent?\n\nNote: I was shocked at the high correlation achievable with some models and wondered if there was a data leakage issue (data leakage is a term from machine learning that describes a situation where data for testing the algorithm is unexpectedly part of the data set that was used to train the algorithm), because LLMs are trained using a very large corpus of data which may include the reviews and the ratings. I did a small experiment on reviews gathered from the past few weeks and found roughly the same answer so I think the result of this homework is not explained by data laekage."
  },
  {
    "objectID": "assignments/labs/Lab9.html#problem-2-llms-for-identifying-flaws-in-products-from-user-reviews.",
    "href": "assignments/labs/Lab9.html#problem-2-llms-for-identifying-flaws-in-products-from-user-reviews.",
    "title": "Lab 9: Using and Evaluating LLMs",
    "section": "Problem 2: LLMs for identifying flaws in products from user reviews.",
    "text": "Problem 2: LLMs for identifying flaws in products from user reviews.\nFor this problem, I have selected a subset of negative amazon reviews taken from a public dataset of amazon reviews. The goal is to extract information about potential flaws in products from these reviews automatically using LLMs. These reviews are divided into two files, a training set which may be incorporated into prompts, and a test set. I have labeled the Amazon reviews into several categories:\n\nNone indicates no specific flaw was identified with the product\nQuality indicates that the product is poorly designed for the intended purpose\nQuality Control indicates that different instances of the product are inconsistent with each other or that the product sometimes arrives broken\nDurability indicates that the product is damaged easily from use\nFalse Advertising indicates that the product doesn’t correspond to the images or online descriptions on Amazon\n\nYour goal is to determine the accuracy of LLMs at classifying text into one of these categories, compared against a human labeler.\n\nAgain select three LLMs (they do not have to be the same as in problem 1, but it would make sense to use the same ones to observe if the same accuracy relationships hold on a new task). The human labeled reviews for training are contained in the file ‘amazon_training_examples.csv’, and the labeled testing reviews are contained in ‘amazon_testing_examples.csv’. Use parallel_chat_structured with a type_enum corresponding to the categories described above. Again you should create a list of prompts containing each review, and modify the system prompt or the chat object history with the training reviews. Compute the accuracy and cost of each LLM in comparison to the labels, and comment on the results. If you selected the same models as in problem 1, do the same relationships hold in this experiment?\nDetermine which questions have the highest error and examine them. Do you agree with the human label? Compare the error patterns across models- note if there are some models which tend to make mistakes on a distinct set of reviews compared to other questions."
  },
  {
    "objectID": "assignments/labs/Lab9.html#problem-3-extra-credit--up-to-10-points-using-a-llm-judge-and-the-vitals-package-to-screen-products-for-problems.",
    "href": "assignments/labs/Lab9.html#problem-3-extra-credit--up-to-10-points-using-a-llm-judge-and-the-vitals-package-to-screen-products-for-problems.",
    "title": "Lab 9: Using and Evaluating LLMs",
    "section": "Problem 3 (Extra Credit- up to 10 points): Using a LLM judge and the vitals package to screen products for problems.",
    "text": "Problem 3 (Extra Credit- up to 10 points): Using a LLM judge and the vitals package to screen products for problems.\nThe vitals package has a number of features which enable comparison and evaluation of LLMs. These include the ability to use an LLM to score the outputs of models, which is called an “LLM as a judge” evaluation. For this problem, I have assembled another dataset of amazon reviews, tabulated by product. The reviews are contained in the file ’amazon_reviews_by_product.csv”. For each product I have assembled 10 negative reviews, with the text concatenated together for each product so that it forms a single string per product. Create a vitals dataset where each input contains all of the negative reviews, and each target contains criteria for the llm-judge to evaluate the outputs by. Create a solver for your chosen LLMs and use model_graded_qa as the scorer. Use a strong model as the judge (not a “thinking model” but something like sonnet or flash , and use cheaper models to compare against each-other (make sure the judge model isn’t also one of the models being used for a comparison). Perform the comparison of the different models at the task of identifying the primary problems with each product. I recommend using the “Incorrect, Correct, Partial Credit” scheme to score the models. Output the results of the model comparison using vitals. Which models perform best? Examine some of the product reviews for which model performance varies, or which are very challenging for all models. Can you understand why those problems were challenging or differentiators between models (I realize this is a subjective question- I just want your thoughts on what happened in your experiment)."
  },
  {
    "objectID": "assignments/labs/Lab9_old_graph_theory.html",
    "href": "assignments/labs/Lab9_old_graph_theory.html",
    "title": "Lab 9: Graphs and Social Networks in Pride & Prejudice",
    "section": "",
    "text": "In the this lab assignment, you are going to construct a social network from the characters in the book “Pride & Prejudice”, a novel written by Jane Austen and available in the janeaustenr package. The social network will be a weighted graph connecting the characters, where the weight is equal to the number of times the names of each character appeared in each 10 line section of the book. Once you create the graph, you will load it into tidygraph, make a visualization of the graph, and rank the most connected characters by a measure called degree centrality\nProblem 1\nLoad the text of Pride & Prejudice into R using the janeaustenr library. Then download and read pride_prejudice_characters.csv, the csv file from my github page containing a list of characters in Pride & Prejudice and their aliases. Here aliases refers to the different names that the characters go by in the books, for example “Darcy” also goes by the names “Mr. Darcy”, and “Mr. Fitzwilliam Darcy” (not to be confused with his cousin “Colonel Fitzwilliam”).\nProcess the text of Pride & Prejudice to replace instances where an alias occurs with the full name of the character- I recommend using the iteration techniques you learned ealier, I arranged the order of names in the csv file to minimize misidentifications if you replace names in the order that they appear in the file. Making this perfect would require a bit of effort but we are ok if there are some misidentifications. Here the final name of each character will be a single word.\nProblem 2\nFollowing the example in chapter 4 of the text mining with R book, create a new column in the data frame corresponding to the Pride & Prejudice text that divides the text into sections of 10 lines each. Then use the pairwise_count function from widyr to determine the number of times each name occurs with each other name in the same 10 line section.\nProblem 3\nCreate a dataframe of nodes which contains the id and unique names of each character, and create a dataframe of edges which contains three columns: a column named from, a column named to, and a column named weight, where the from and to are the id numbers of each character and weight is the number of co-occurences you found in Problem 2. Each pair should only appear once in the edge list (i.e. Elizabeth and MrDarcy but not MrDarcy and then Elizabeth). Create a tidygraph object using tbl_graph that contains the social network data that we just constructed.\nProblem 4\nUsing ggraph, graph the connections between the characters. Make sure that each node is labeled by the character name, and make sure that the weight is represented by the thickness of the edge plotted between the two nodes. Then use the centrality_degree function to calculate the weighted degree centrality of each character, and make a plot which shows the degree centrality of each character where the characters are arranged in order of degree centrality."
  },
  {
    "objectID": "assignments/labs/Lab9_old_graph_theory.html#overview",
    "href": "assignments/labs/Lab9_old_graph_theory.html#overview",
    "title": "Lab 9: Graphs and Social Networks in Pride & Prejudice",
    "section": "",
    "text": "In the this lab assignment, you are going to construct a social network from the characters in the book “Pride & Prejudice”, a novel written by Jane Austen and available in the janeaustenr package. The social network will be a weighted graph connecting the characters, where the weight is equal to the number of times the names of each character appeared in each 10 line section of the book. Once you create the graph, you will load it into tidygraph, make a visualization of the graph, and rank the most connected characters by a measure called degree centrality\nProblem 1\nLoad the text of Pride & Prejudice into R using the janeaustenr library. Then download and read pride_prejudice_characters.csv, the csv file from my github page containing a list of characters in Pride & Prejudice and their aliases. Here aliases refers to the different names that the characters go by in the books, for example “Darcy” also goes by the names “Mr. Darcy”, and “Mr. Fitzwilliam Darcy” (not to be confused with his cousin “Colonel Fitzwilliam”).\nProcess the text of Pride & Prejudice to replace instances where an alias occurs with the full name of the character- I recommend using the iteration techniques you learned ealier, I arranged the order of names in the csv file to minimize misidentifications if you replace names in the order that they appear in the file. Making this perfect would require a bit of effort but we are ok if there are some misidentifications. Here the final name of each character will be a single word.\nProblem 2\nFollowing the example in chapter 4 of the text mining with R book, create a new column in the data frame corresponding to the Pride & Prejudice text that divides the text into sections of 10 lines each. Then use the pairwise_count function from widyr to determine the number of times each name occurs with each other name in the same 10 line section.\nProblem 3\nCreate a dataframe of nodes which contains the id and unique names of each character, and create a dataframe of edges which contains three columns: a column named from, a column named to, and a column named weight, where the from and to are the id numbers of each character and weight is the number of co-occurences you found in Problem 2. Each pair should only appear once in the edge list (i.e. Elizabeth and MrDarcy but not MrDarcy and then Elizabeth). Create a tidygraph object using tbl_graph that contains the social network data that we just constructed.\nProblem 4\nUsing ggraph, graph the connections between the characters. Make sure that each node is labeled by the character name, and make sure that the weight is represented by the thickness of the edge plotted between the two nodes. Then use the centrality_degree function to calculate the weighted degree centrality of each character, and make a plot which shows the degree centrality of each character where the characters are arranged in order of degree centrality."
  },
  {
    "objectID": "assignments/labs/Lab8Backup.html",
    "href": "assignments/labs/Lab8Backup.html",
    "title": "Lab 8: Text Mining and NLP",
    "section": "",
    "text": "Overview\nIn this lab, we will use tidy text techniques to analyze a dataset of amazon reviews. Each problem utilizes the tidy text mining techniques described in either chapter 2 (Problem 1), chapter 3 (Problem 2), or chapter 4 (Problem 3) of the tidy text mining with r textbook. Note: the dataset for this assignment is a bit bigger than what we have typically worked with in the class. On my computer everything worked fast enough, but if your computer is older and you find the computations intolerably slow you may reduce the size of the dataset by 90% by taking only the first 10% of reviews. If you do this make sure it is clearly stated.\nProblem 1: Sentiment and Review Score\n\nDownload “simple_reviews.json” from the following google drive link: https://drive.google.com/drive/folders/1bk_2ihR5gQ8k6Tkn0NNpB58K1efruKnX?usp=sharing, read it into R, and rectangle it so that it is a dataframe where each row contains a single amazon review.\nTo make sentiment analysis possible, add an index variable to the review data frame so that each review is uniquely identified by an integer. Then tokenize the review data frame using words as the tokens, and remove all stop words from the data set.\nDoes sentiment correlate with reviews? Use the afinn lexicon to calculate a sentiment score for each review, normalizing by the number of lexicon words in each review. Visualize the distribution of sentiment scores for each rating and calculate the mean sentiment score for each review category. What do you observe?\nReviewer Personalities: For each reviewer, compute the number of ratings, the mean sentiment, and the mean review score. Filter for reviewers who have written more than 10 reviews, and plot the relationship between mean rating and mean sentiment. What do you observe?\n\nProblem 2: Words with high relative frequency \n\nAs your starting point, take the tokenized data frame that has been filtered to remove stop words, but hasn’t been joined with the sentiment lexicon data. For each item (asin), use the bind_tf_idf function to find the word that occurs in the reviews of that item with the highest frequency relative to the frequency of words in the entire review test dataset.\nSelect five items from (either at random or by hand) and look up the asin code on Amazon.com. In each of these cases, does the highest relative frequency word correspond to the identify or type of the item that you chose? You may not be able to find every single item, but I was able to find a solid majority of the ones I searched for by searching amazon.com for the asin.\n\nProblem 3: Bigrams and Sentiment\n\nConsider the two negative words not and don't. Starting from the original dataset, tokenize the data into bigrams. Then calculate the frequency of bigrams that start with either not or don't. What are the 10 most common words occurring after not and after don't? What are their sentiment values according to the afinn lexicon?\nPick the most commonly occurring bigram where the first word is not or don't and the afinn sentiment of the second word is 2 or greater. Compute the mean rating of the reviews containing this bigram. How do they compare the average review score over the entire dataset?"
  },
  {
    "objectID": "assignments/labs/Lab8.html",
    "href": "assignments/labs/Lab8.html",
    "title": "Lab 8: Text Mining and NLP",
    "section": "",
    "text": "Overview\nIn this lab, we will use tidy text techniques to analyze a dataset of amazon reviews. Each problem utilizes the tidy text mining techniques described in either chapter 2 (Problem 1), chapter 3 (Problem 2), or chapter 4 (Problem 3) of the tidy text mining with r textbook. Note: the dataset for this assignment is a bit bigger than what we have typically worked with in the class. On my computer everything worked fast enough, but if your computer is older and you find the computations intolerably slow you may reduce the size of the dataset by 90% by taking only the first 10% of reviews. If you do this make sure it is clearly stated. I have also listed a second shorter version of the file.\nProblem 1: Sentiment and Review Score\n\nDownload “simple_reviews.json” from the following google drive link: https://drive.google.com/drive/folders/1bk_2ihR5gQ8k6Tkn0NNpB58K1efruKnX?usp=sharing, read it into R, and rectangle it so that it is a dataframe where each row contains a single amazon review. If this file is too large, click here to download a shorter version where 90% of the reviews have been dropped].\nTo make sentiment analysis possible, add an index variable to the review data frame so that each review is uniquely identified by an integer. Then tokenize the review data frame using words as the tokens, and remove all stop words from the data set.\nDoes sentiment correlate with reviews? Use the afinn lexicon to calculate a sentiment score for each review, normalizing by the number of lexicon words in each review. Visualize the distribution of sentiment scores for each rating and calculate the mean sentiment score for each review category. What do you observe?\nReviewer Personalities: For each reviewer, compute the number of ratings, the mean sentiment, and the mean review score. Filter for reviewers who have written more than 10 reviews, and plot the relationship between mean rating and mean sentiment. What do you observe?\n\nProblem 2: Words with high relative frequency \n\nAs your starting point, take the tokenized data frame that has been filtered to remove stop words, but hasn’t been joined with the sentiment lexicon data. For each item (asin), use the bind_tf_idf function to find the word that occurs in the reviews of that item with the highest frequency relative to the frequency of words in the entire review test dataset.\nSelect five items from the dataset (either at random or by hand) and look up the asin code for those items on Amazon.com. In each of these cases, does the highest relative frequency word correspond to the identify or type of the item that you chose? You may not be able to find every single item, but I was able to find a solid majority of the ones I searched for by searching amazon.com for the asin.\n\nProblem 3: Bigrams and Sentiment\n\nConsider the two negative words not and don't. Starting from the original dataset, tokenize the data into bigrams. Then calculate the frequency of bigrams that start with either not or don't. What are the 10 most common words occurring after not and after don't? What are their sentiment values according to the afinn lexicon?\nPick the most commonly occurring bigram where the first word is not or don't and the afinn sentiment of the second word is 2 or greater. Compute the mean rating of the reviews containing this bigram. How do they compare the average review score over the entire dataset?"
  },
  {
    "objectID": "assignments/labs/Lab1.html",
    "href": "assignments/labs/Lab1.html",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "",
    "text": "Airbnb has had a disruptive effect on the hotel, rental home, and vacation industry throughout the world. The success of Airbnb has not come without controversies, with critics arguing that Airbnb has adverse impacts on housing and rental prices and also on the daily lives of people living in neighborhoods where Airbnb is popular. This controversy has been particularly intense in NYC, where the debate been Airbnb proponents and detractors eventually led to the city imposing strong restrictions on the use of Airbnb. If you find this issue interesting and want to go deeper, there is the potential for an interesting project that brings in hotels (which have interesting regulations in NYC), hotel price data, and rental data and looks at these things together.\nBecause Airbnb listings are available online through their website and app, it is possible for us to acquire and visualize the impacts of Airbnb on different cities, including New York City. This is possible through the work of an organization called inside airbnb"
  },
  {
    "objectID": "assignments/labs/Lab1.html#airbnb-in-nyc-or-your-city",
    "href": "assignments/labs/Lab1.html#airbnb-in-nyc-or-your-city",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "",
    "text": "Airbnb has had a disruptive effect on the hotel, rental home, and vacation industry throughout the world. The success of Airbnb has not come without controversies, with critics arguing that Airbnb has adverse impacts on housing and rental prices and also on the daily lives of people living in neighborhoods where Airbnb is popular. This controversy has been particularly intense in NYC, where the debate been Airbnb proponents and detractors eventually led to the city imposing strong restrictions on the use of Airbnb. If you find this issue interesting and want to go deeper, there is the potential for an interesting project that brings in hotels (which have interesting regulations in NYC), hotel price data, and rental data and looks at these things together.\nBecause Airbnb listings are available online through their website and app, it is possible for us to acquire and visualize the impacts of Airbnb on different cities, including New York City. This is possible through the work of an organization called inside airbnb"
  },
  {
    "objectID": "assignments/labs/Lab1.html#instructions",
    "href": "assignments/labs/Lab1.html#instructions",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Instructions",
    "text": "Instructions\n\nComplete the assigned questions in a quarto notebook (I recommend using this file as a template) and render the final document to a pdf file.\nSubmit both the pdf file and the qmd source file that you used to create the pdf to brightspace."
  },
  {
    "objectID": "assignments/labs/Lab1.html#packages",
    "href": "assignments/labs/Lab1.html#packages",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, and the ggridges package to make a ridge plot in the last exercise. You may need to install ggridges if you haven’t already, you can do that using:\n\ninstall.packages(\"ggridges\")\n\nThen make sure to load both packages:"
  },
  {
    "objectID": "assignments/labs/Lab1.html#data",
    "href": "assignments/labs/Lab1.html#data",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Data",
    "text": "Data\nThe data for this assignment can be found on my github page by clicking here and downloading nycbnb.csv\nYou can read the data into R using the command:\n\nnycbnb = read_csv(\"https://github.com/georgehagstrom/DATA607Fall2025/blob/master/data/nyc_airbnb_listings_short.csv\")\n\nThis will download it from my site and read it directly, but you may want to download it into a local directory and load it from there as well. In that case you can use read_csv(where your file is located) to laod it.\nYou can view the dataset as a spreadsheet using the View() function. Note that you should not put this function in your R Markdown document, but instead type it directly in the Console, as it pops open a new window (and the concept of popping open a window in a static document doesn’t really make sense…). When you run this in the console, you’ll see the following data viewer window pop up."
  },
  {
    "objectID": "assignments/labs/Lab1Old.html",
    "href": "assignments/labs/Lab1Old.html",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "",
    "text": "Airbnb has had a disruptive effect on the hotel, rental home, and vacation industry throughout the world. The success of Airbnb has not come without controversies, with critics arguing that Airbnb has adverse impacts on housing and rental prices and also on the daily lives of people living in neighborhoods where Airbnb is popular. This controversy has been particularly intense in NYC, where the debate been Airbnb proponents and detractors eventually led to the city imposing strong restrictions on the use of Airbnb. If you find this issue interesting and want to go deeper, there is the potential for an interesting project that brings in hotels (which have interesting regulations in NYC), hotel price data, and rental data and looks at these things together.\nBecause Airbnb listings are available online through their website and app, it is possible for us to acquire and visualize the impacts of Airbnb on different cities, including New York City. This is possible through the work of an organization called inside airbnb"
  },
  {
    "objectID": "assignments/labs/Lab1Old.html#airbnb-in-nyc-or-your-city",
    "href": "assignments/labs/Lab1Old.html#airbnb-in-nyc-or-your-city",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "",
    "text": "Airbnb has had a disruptive effect on the hotel, rental home, and vacation industry throughout the world. The success of Airbnb has not come without controversies, with critics arguing that Airbnb has adverse impacts on housing and rental prices and also on the daily lives of people living in neighborhoods where Airbnb is popular. This controversy has been particularly intense in NYC, where the debate been Airbnb proponents and detractors eventually led to the city imposing strong restrictions on the use of Airbnb. If you find this issue interesting and want to go deeper, there is the potential for an interesting project that brings in hotels (which have interesting regulations in NYC), hotel price data, and rental data and looks at these things together.\nBecause Airbnb listings are available online through their website and app, it is possible for us to acquire and visualize the impacts of Airbnb on different cities, including New York City. This is possible through the work of an organization called inside airbnb"
  },
  {
    "objectID": "assignments/labs/Lab1Old.html#instructions",
    "href": "assignments/labs/Lab1Old.html#instructions",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Instructions",
    "text": "Instructions\n\nComplete the assigned questions in a quarto notebook (I recommend using this file as a template) and render the final document to a pdf file.\nSubmit both the pdf file and the qmd source file that you used to create the pdf to brightspace."
  },
  {
    "objectID": "assignments/labs/Lab1Old.html#packages",
    "href": "assignments/labs/Lab1Old.html#packages",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, and the ggridges package to make a ridge plot in the last exercise. You may need to install ggridges if you haven’t already, you can do that using:\n\ninstall.packages(\"ggridges\")\n\nThen make sure to load both packages:"
  },
  {
    "objectID": "assignments/labs/Lab1Old.html#data",
    "href": "assignments/labs/Lab1Old.html#data",
    "title": "Lab 1: Airbnbs in NYC",
    "section": "Data",
    "text": "Data\nThe data for this assignment can be found on my github page by clicking here and downloading nycbnb.csv\nYou can read the data into R using the command:\n\nnycbnb = read_csv(\"/home/georgehagstrom/work/Teaching/DATA608/DataStory4/nyc_airbnb_listings.csv\")\n\nwhere you should replace /home/georgehagstrom/work/Teaching/DATA607/website/data/nycbnb.csv\"nycbnb.csv with the local path to your file.\nYou can view the dataset as a spreadsheet using the View() function. Note that you should not put this function in your R Markdown document, but instead type it directly in the Console, as it pops open a new window (and the concept of popping open a window in a static document doesn’t really make sense…). When you run this in the console, you’ll see the following data viewer window pop up."
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#week-summary",
    "href": "meetups/Meetup5/Meetup5.html#week-summary",
    "title": "Meetup 5: Data Transformation",
    "section": "Week Summary",
    "text": "Week Summary\n\nReading: Chapters 12, 13, 17 of R4DS\nLab 4 on Data Transformations Due Sunday at Midnight"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-graph",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-graph",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Graph?",
    "text": "Common Questions: What Graph?\n\nOne Continuous variable\nDensity plot, histogram, frequency polygon (geom_density, geom_histogram, geom_freqpoly)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-graph-1",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-graph-1",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Graph?",
    "text": "Common Questions: What Graph?\n\nOne Categorical variable\nBar Plot, dot plot (geom_bar, geom_point with stat=\"count\")"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-plot",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-plot",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Plot?",
    "text": "Common Questions: What Plot?\n\nTwo Numerical variables\ngeom_point, geom_hex, geom_contour"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-1",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-1",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Plot?",
    "text": "Common Questions: What Plot?\n\nNumerical and Categorical\ngeom_density_ridges, geom_boplot, geom_violin\nUse other aesthetics (fill), use facets"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-2",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-2",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Plot?",
    "text": "Common Questions: What Plot?\n\nTwo numerical and one categorical\nFaceted Scatterplot or other aesthetics"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-3",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-3",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Plot?",
    "text": "Common Questions: What Plot?\n\nOne Numerical and multiple categorical\nMultiway dot plot"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-4",
    "href": "meetups/Meetup5/Meetup5.html#common-questions-what-plot-4",
    "title": "Meetup 5: Data Transformation",
    "section": "Common Questions: What Plot?",
    "text": "Common Questions: What Plot?\n\nYou can keep going in this direction, but gets harder to understand\nMultiple categorical and numerical, can use facets, aesthetics, do a scatterplot etc"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#data-transformation",
    "href": "meetups/Meetup5/Meetup5.html#data-transformation",
    "title": "Meetup 5: Data Transformation",
    "section": "Data Transformation",
    "text": "Data Transformation\n\nProcess of creating new variables from current data\nNeeded to prepare data for your analyses and questions\nData transformations can lead to dramatically improved insight at little cost\nTypes: Numbers, logical variables, dates, later strings and factors."
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#case-study-brain-and-body-size",
    "href": "meetups/Meetup5/Meetup5.html#case-study-brain-and-body-size",
    "title": "Meetup 5: Data Transformation",
    "section": "Case Study: Brain and Body Size",
    "text": "Case Study: Brain and Body Size"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#no-correlation",
    "href": "meetups/Meetup5/Meetup5.html#no-correlation",
    "title": "Meetup 5: Data Transformation",
    "section": "No Correlation?",
    "text": "No Correlation?\n\nCompute the Pearson correlation between brain mass and body mass:\n\n\ncor(Animals$body,Animals$brain)\n\n[1] -0.005341163\n\n\n\nWe expect a strong relationship between brain mass and body mass\nVery difficult to believe that there is no relationship between brain mass and body mass."
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#log-transformation",
    "href": "meetups/Meetup5/Meetup5.html#log-transformation",
    "title": "Meetup 5: Data Transformation",
    "section": "Log Transformation",
    "text": "Log Transformation\n\n\n\nBrain and Body size:\n\nPositive variables\nRange over orders of magnitude\nResult from multiplicative growth processes\n\nSuggests we try logarithms\n\n\n\n\n\nxkcd"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#log-transformation-different-story",
    "href": "meetups/Meetup5/Meetup5.html#log-transformation-different-story",
    "title": "Meetup 5: Data Transformation",
    "section": "Log Transformation: Different Story",
    "text": "Log Transformation: Different Story\n\n\nAnimals |&gt; \n  summarize(Correlation = cor(log(body),log(brain)) ) |&gt; print()\n\n  Correlation\n1   0.7794935"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#booleanlogical-operations",
    "href": "meetups/Meetup5/Meetup5.html#booleanlogical-operations",
    "title": "Meetup 5: Data Transformation",
    "section": "Boolean/Logical Operations",
    "text": "Boolean/Logical Operations\n\nLogical Types: TRUE and FALSE\nNOT: !TRUE = FALSE, !FALSE=TRUE\nAND: only TRUE & TRUE = TRUE rest FALSE\nOR: only FALSE | FALSE = FALSE rest TRUE\n\nComparison statements don’t work like in English:\n\n4 &gt; 5 | 6 # Wrong\n\n[1] TRUE\n\n(4 &gt; 5) | (4 &gt; 6) # Right\n\n[1] FALSE"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#boolean-variables-for-comparison",
    "href": "meetups/Meetup5/Meetup5.html#boolean-variables-for-comparison",
    "title": "Meetup 5: Data Transformation",
    "section": "Boolean Variables for Comparison",
    "text": "Boolean Variables for Comparison\n\nlibrary(kableExtra)\nnycflights13::flights |&gt; select(year,month,day,arr_delay,carrier,origin,dest) |&gt;  head(10) |&gt; kable()\n\n\n\n\nyear\nmonth\nday\narr_delay\ncarrier\norigin\ndest\n\n\n\n\n2013\n1\n1\n11\nUA\nEWR\nIAH\n\n\n2013\n1\n1\n20\nUA\nLGA\nIAH\n\n\n2013\n1\n1\n33\nAA\nJFK\nMIA\n\n\n2013\n1\n1\n-18\nB6\nJFK\nBQN\n\n\n2013\n1\n1\n-25\nDL\nLGA\nATL\n\n\n2013\n1\n1\n12\nUA\nEWR\nORD\n\n\n2013\n1\n1\n19\nB6\nEWR\nFLL\n\n\n2013\n1\n1\n-14\nEV\nLGA\nIAD\n\n\n2013\n1\n1\n-8\nB6\nJFK\nMCO\n\n\n2013\n1\n1\n8\nAA\nLGA\nORD"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#compare-dl-ua-aa-to-others",
    "href": "meetups/Meetup5/Meetup5.html#compare-dl-ua-aa-to-others",
    "title": "Meetup 5: Data Transformation",
    "section": "Compare DL, UA, AA to others",
    "text": "Compare DL, UA, AA to others\n\nCreate variable that is TRUE when airline is in the group\nFALSE otherwise\n\n\nflights = nycflights13::flights |&gt; \n  mutate(IS_CARRIER_DL_UA_AA = carrier %in% c(\"DL\",\"UA\",\"AA\"))\nflights |&gt; \n  group_by(IS_CARRIER_DL_UA_AA) |&gt; \n  summarise(fraction_delayed = sum(arr_delay&gt;30,na.rm=TRUE)/\n              sum(is.finite(arr_delay),na.rm=TRUE)) |&gt; \n  print()\n\n# A tibble: 2 × 2\n  IS_CARRIER_DL_UA_AA fraction_delayed\n  &lt;lgl&gt;                          &lt;dbl&gt;\n1 FALSE                          0.180\n2 TRUE                           0.125"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logical-subsetting",
    "href": "meetups/Meetup5/Meetup5.html#logical-subsetting",
    "title": "Meetup 5: Data Transformation",
    "section": "Logical Subsetting",
    "text": "Logical Subsetting\n\nPowerful way of manipulating Data Frames\nif_else and case_when: vector output\n\n\ndiamonds |&gt; \n  mutate( cut = if_else(\n      cut == \"Very Good\", \"Very_Good\", cut)\n    )"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base",
    "href": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base",
    "title": "Meetup 5: Data Transformation",
    "section": "Logical Subsetting using Base",
    "text": "Logical Subsetting using Base\n\nCan use [] for other data types\n\n\ndiabetes = read_csv(\"../../assignments/labs/labData/diabetes.csv\")\ndiabetes\n\n# A tibble: 768 × 9\n   Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1           6     148            72            35       0  33.6\n 2           1      85            66            29       0  26.6\n 3           8     183            64             0       0  23.3\n 4           1      89            66            23      94  28.1\n 5           0     137            40            35     168  43.1\n 6           5     116            74             0       0  25.6\n 7           3      78            50            32      88  31  \n 8          10     115             0             0       0  35.3\n 9           2     197            70            45     543  30.5\n10           8     125            96             0       0   0  \n# ℹ 758 more rows\n# ℹ 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;dbl&gt;, Outcome &lt;dbl&gt;"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base-1",
    "href": "meetups/Meetup5/Meetup5.html#logical-subsetting-using-base-1",
    "title": "Meetup 5: Data Transformation",
    "section": "Logical Subsetting using Base",
    "text": "Logical Subsetting using Base\n\nCan use [] for other data types\n\n\ndiabetes[2:6][diabetes[2:6]==0] = NA\ndiabetes\n\n# A tibble: 768 × 9\n   Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n         &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1           6     148            72            35      NA  33.6\n 2           1      85            66            29      NA  26.6\n 3           8     183            64            NA      NA  23.3\n 4           1      89            66            23      94  28.1\n 5           0     137            40            35     168  43.1\n 6           5     116            74            NA      NA  25.6\n 7           3      78            50            32      88  31  \n 8          10     115            NA            NA      NA  35.3\n 9           2     197            70            45     543  30.5\n10           8     125            96            NA      NA  NA  \n# ℹ 758 more rows\n# ℹ 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;dbl&gt;, Outcome &lt;dbl&gt;"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#numerical-transformations",
    "href": "meetups/Meetup5/Meetup5.html#numerical-transformations",
    "title": "Meetup 5: Data Transformation",
    "section": "Numerical Transformations",
    "text": "Numerical Transformations\n\nCan implement any mathematical formula you can imagine\nConstruct quantity of interest:\n\nDomain expertise\n\nAs part of EDA\n\nMake a complicated relationship look simpler or even linear\n\nTo boost other tools\n\nSome algorithms run better with standardized data"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#defining-functions",
    "href": "meetups/Meetup5/Meetup5.html#defining-functions",
    "title": "Meetup 5: Data Transformation",
    "section": "Defining Functions",
    "text": "Defining Functions\n\nFor complicated data transformations, define a function\n\n\nmy_transformation = function(var1,var2,kfact = 1){\n  \n  mean_var1 = mean(var1)\n  sd_var1 = sd(var1)\n  mean_var2 = mean(var2)\n  sd_var2 = sd(var2)\n  return(exp(-kfact*abs((var1-mean_var1)*(var2-mean_var2))/(sd_var1*sd_var2)))\n  \n}\n\n\nFunctions make code easier to read\nWrite function when code reused 3 times"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#recycling-rules-for-aggregates",
    "href": "meetups/Meetup5/Meetup5.html#recycling-rules-for-aggregates",
    "title": "Meetup 5: Data Transformation",
    "section": "Recycling Rules for Aggregates",
    "text": "Recycling Rules for Aggregates\n\nR “boosts” numbers/vectors so that calculations work\nBehavior is very different from other languages:\n\n\nc(5,2,3,10)/c(1,9,1)\n\n[1]  5.0000000  0.2222222  3.0000000 10.0000000\n\n\n\nShorter vector/number is repeated to match the length of longer one\nCan be very unintuitive for when the vectors have lengths different from 1"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#recycling-fails",
    "href": "meetups/Meetup5/Meetup5.html#recycling-fails",
    "title": "Meetup 5: Data Transformation",
    "section": "Recycling Fails:",
    "text": "Recycling Fails:\n\nSource of silent bugs\n\n\nflights |&gt; \n  filter(month == c(1, 2)) |&gt; select(year, month, flight) |&gt; head(5)\n\n# A tibble: 5 × 3\n   year month flight\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  2013     1   1545\n2  2013     1   1141\n3  2013     1    461\n4  2013     1    507\n5  2013     1     79\n\nflights |&gt; \n  filter(month %in% c(1, 2)) |&gt; select(year, month, flight) |&gt; head(5)\n\n# A tibble: 5 × 3\n   year month flight\n  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1  2013     1   1545\n2  2013     1   1714\n3  2013     1   1141\n4  2013     1    725\n5  2013     1    461"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#logarithms",
    "href": "meetups/Meetup5/Meetup5.html#logarithms",
    "title": "Meetup 5: Data Transformation",
    "section": "logarithms",
    "text": "logarithms\n\n\n\\(\\log(e^x) = x\\)\n\\(\\log(x_1 \\cdot x_2 \\cdot \\dots \\cdot x_n) = \\log(x_1) + \\log(x_2) + \\dots + \\log(x_n)\\)\n\\(\\log_{10}\\) counts the digits of a number"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#log-in-data-science-and-stats",
    "href": "meetups/Meetup5/Meetup5.html#log-in-data-science-and-stats",
    "title": "Meetup 5: Data Transformation",
    "section": "\\(\\log\\) in data science and stats",
    "text": "\\(\\log\\) in data science and stats\n\nStarting point when:\n\nPositive variables\nMultiplicative growth\nWide Range\n\nAvoiding overflow/underflow in probability models\n\nIn maximum likelihood focus on log likelihood\n\nSpecial case of Tukey’s Ladder:"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#two-approaches-to-transformations",
    "href": "meetups/Meetup5/Meetup5.html#two-approaches-to-transformations",
    "title": "Meetup 5: Data Transformation",
    "section": "Two Approaches to Transformations:",
    "text": "Two Approaches to Transformations:\n\nDomain Focused:\n\n\nConstruct quantities of interest\nTransformation always inspired by knowledge of domain\nPower transformation to scale physical values\n\n\nStats Focused:\n\n\nTransform with goal to make data more Normal, relations more linear\nExemplified by Box-Cox Transformation"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#window-functions-values",
    "href": "meetups/Meetup5/Meetup5.html#window-functions-values",
    "title": "Meetup 5: Data Transformation",
    "section": "Window Functions: Values",
    "text": "Window Functions: Values\n\nPerform transformations on “nearby” rows:\nlead Next value(s)\nlag Previous value(s)\n\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lead() \n\n [1]  2  3  4  5  6  7  8  9 10 NA\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lead(n=3) \n\n [1]  4  5  6  7  8  9 10 NA NA NA\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lag() \n\n [1] NA  1  2  3  4  5  6  7  8  9\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; lag(n=3) \n\n [1] NA NA NA  1  2  3  4  5  6  7\n\n\n\nfirst() and last()"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#window-functions-ranks",
    "href": "meetups/Meetup5/Meetup5.html#window-functions-ranks",
    "title": "Meetup 5: Data Transformation",
    "section": "Window Functions: Ranks",
    "text": "Window Functions: Ranks\n\nPerform transformations on “nearby” rows:\npercent_rank()\nrank()\nntile()\n\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; percent_rank() \n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n [8] 0.7777778 0.8888889 1.0000000\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; rank() \n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nc(1,2,3,4,5,6,7,8,9,10) |&gt; ntile(4) \n\n [1] 1 1 1 2 2 2 3 3 4 4"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\ncumsum, cummean\nRolling window (several libraries including RcppRoll, TTR, slider)\n\n\nlibrary(TTR)\nlibrary(cranlogs)\npkgs = c(\n    \"tidyr\", \"lubridate\", \"dplyr\", \n    \"ggplot2\", \"purrr\", \n    \"stringr\", \"knitr\"\n    )\n\ntidyverse_downloads = cran_downloads(\n    packages = pkgs, \n    from     = \"2024-01-01\", \n    to       = \"2024-08-31\") %&gt;%\n    tibble::as_tibble() %&gt;%\n    group_by(package)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-1",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-1",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\ncumsum, cummean\nRolling window (several libraries including RcppRoll, TTR, slider)\n\n\ntidyverse_downloads |&gt; filter(package == \"lubridate\") |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point()+\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-2",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-2",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\nRolling window TTR:\n\n\ntidyverse_downloads |&gt; filter(package == \"lubridate\") |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point()+\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-3",
    "href": "meetups/Meetup5/Meetup5.html#cumulative-and-rolling-aggregates-3",
    "title": "Meetup 5: Data Transformation",
    "section": "Cumulative and Rolling Aggregates",
    "text": "Cumulative and Rolling Aggregates\n\ntidyverse_downloads |&gt; filter(package == \"lubridate\") |&gt; \n  mutate(rolling_count = runMean(count,n=7)) |&gt; \n  ggplot(aes(x=date,y=count)) + geom_point() + geom_line(aes(y=rolling_count)) +\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#caution-about-group-boundaries",
    "href": "meetups/Meetup5/Meetup5.html#caution-about-group-boundaries",
    "title": "Meetup 5: Data Transformation",
    "section": "Caution About Group Boundaries",
    "text": "Caution About Group Boundaries\n\ntidyverse_downloads |&gt;  ungroup() |&gt; \n  mutate(rolling_count = runMean(count,n=7)) |&gt; \n  filter(package == \"lubridate\") |&gt; \n  ggplot(aes(x=date,y=count)) + \n  geom_point() + \n  geom_line(aes(y=rolling_count)) +\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#data-science-in-context-presentation",
    "href": "meetups/Meetup5/Meetup5.html#data-science-in-context-presentation",
    "title": "Meetup 5: Data Transformation",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#rank",
    "href": "meetups/Meetup5/Meetup5.html#rank",
    "title": "Meetup 5: Data Transformation",
    "section": "Rank",
    "text": "Rank\n\nrank transforms data from values to numerical rank:\n\n\nrandVec = rnorm(10)\nprint(randVec)\n\n [1]  2.3018724  0.4560994  0.8971370  1.2285259 -0.5153130 -0.4754930\n [7] -1.0822176  1.7008024 -0.1618288  0.2311937\n\nprint(randVec |&gt; rank())\n\n [1] 10  6  7  8  2  3  1  9  4  5\n\n\n\nRank preserved by all monotone transformations"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#rank-and-correlation-trick",
    "href": "meetups/Meetup5/Meetup5.html#rank-and-correlation-trick",
    "title": "Meetup 5: Data Transformation",
    "section": "Rank and Correlation Trick",
    "text": "Rank and Correlation Trick\n\nSpearman Correlation is correlation between ranks of two variables\n\n\nAnimals  |&gt; \n           summarise(\n  rank_corr = cor(body,brain,method='spearman'),\n  corr = cor(body,brain)\n)\n\n  rank_corr         corr\n1 0.7162994 -0.005341163\n\n\n\nCorrelation Trick: Look for Nonlinear Transformations when Rank-Correlation is much higher than Pearson Correlation"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#rank-correlation-graphical-version",
    "href": "meetups/Meetup5/Meetup5.html#rank-correlation-graphical-version",
    "title": "Meetup 5: Data Transformation",
    "section": "Rank Correlation Graphical Version",
    "text": "Rank Correlation Graphical Version"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#what-didnt-we-cover",
    "href": "meetups/Meetup5/Meetup5.html#what-didnt-we-cover",
    "title": "Meetup 5: Data Transformation",
    "section": "What didn’t we cover?",
    "text": "What didn’t we cover?\n\nlogit, probit, ReLU: Data transformations that go between a finite interval and the real line\nEnormous number of different ranking functions available in R\nLinear Algebra Based Methods, PCA, dimensionality reduction\nTools from the sciences: Fourier, Wavelets, etc"
  },
  {
    "objectID": "meetups/Meetup5/Meetup5.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup5/Meetup5.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 5: Data Transformation",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup12/spam_prompt.html",
    "href": "meetups/Meetup12/spam_prompt.html",
    "title": "DATA 607 Fall 2025",
    "section": "",
    "text": "You are an expert in spam email detection. You will be given emails to classify and you will return TRUE if the email is legitimate, and FALSE if it is a spam email. Below is a list of 10 examples, listing in a CSV format. THe first column is the class (TRUE for legitimate, FALSE for spam), and the second column is a string containing the email text. Use these examples to classify the prompts correctly. Return a single word answer, either TRUE or FALSE\nclass,text TRUE , No i am not having not any movies in my laptop TRUE , “Yep, by the pretty sculpture” TRUE , 4 oclock at mine. Just to bash out a flat plan. TRUE , I can take you at like noon TRUE , Ok lor. I ned 2 go toa payoh 4 a while 2 return smth u wan 2 send me there or wat? FALSE , Filthy stories and GIRLS waiting for your FALSE , wamma get laid?want real doggin locations sent direct to your mobile? join the UKs largest dogging network. txt dogs to 69696 now!nyt. ec2a. 3lp £1.50/msg. FALSE , Text PASS to 69669 to collect your polyphonic ringtones. Normal gprs charges apply only. Enjoy your tones FALSE , Dear Subscriber ur draw 4 £100 gift voucher will b entered on receipt of a correct ans. When was Elvis Presleys Birthday? TXT answer to 80062 FALSE , FREEMSG: Our records indicate you may be entitled to 3750 pounds for the Accident you had. To claim for free reply with YES to this msg. To opt out text STOP"
  },
  {
    "objectID": "meetups/Meetup12/spam_prompt_mall.html",
    "href": "meetups/Meetup12/spam_prompt_mall.html",
    "title": "DATA 607 Fall 2025",
    "section": "",
    "text": "You are an expert in spam email detection. You will be given emails to classify and you will return NOT_SPAM if the email is legitimate, and SPAM if it is a spam email. Below is a list of 10 examples, listing in a CSV format. THe first column is the class (NOT_SPAM for legitimate, SPAM for spam), and the second column is a string containing the email text. Use these examples to classify the prompts correctly. Return only single word answer, either NOT_SPAM or SPAM, with no other text.\nclass,text NOT_SPAM , No i am not having not any movies in my laptop NOT_SPAM , “Yep, by the pretty sculpture” NOT_SPAM , 4 oclock at mine. Just to bash out a flat plan. NOT_SPAM , I can take you at like noon NOT_SPAM , Ok lor. I ned 2 go toa payoh 4 a while 2 return smth u wan 2 send me there or wat? SPAM , Filthy stories and GIRLS waiting for your SPAM , wamma get laid?want real doggin locations sent direct to your mobile? join the UKs largest dogging network. txt dogs to 69696 now!nyt. ec2a. 3lp £1.50/msg. SPAM , Text PASS to 69669 to collect your polyphonic ringtones. Normal gprs charges apply only. Enjoy your tones SPAM , Dear Subscriber ur draw 4 £100 gift voucher will b entered on receipt of a correct ans. When was Elvis Presleys Birthday? TXT answer to 80062 SPAM , FREEMSG: Our records indicate you may be entitled to 3750 pounds for the Accident you had. To claim for free reply with YES to this msg. To opt out text STOP"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#questions-from-last-week",
    "href": "meetups/Meetup3/Meetup3.html#questions-from-last-week",
    "title": "Meetup 3: Tidy Data",
    "section": "Questions from last week",
    "text": "Questions from last week\n\nWhich graph for which data?\n\nWilke Directory of Visualizations\n\nHow to troubleshoot?\n\nSearch on google/stackoverflow\nLLMs are amazing for this\n?function_call, Healy: How to read a help page"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#event",
    "href": "meetups/Meetup3/Meetup3.html#event",
    "title": "Meetup 3: Tidy Data",
    "section": "Event!",
    "text": "Event!\n\nWe are having a department wide, online event tomorrow at 7PM!\nMeet and chat with the program director, your professors, and other students\nhttps://us02web.zoom.us/j/2994528670?omn=85792887090"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#what-to-do-this-week",
    "href": "meetups/Meetup3/Meetup3.html#what-to-do-this-week",
    "title": "Meetup 3: Tidy Data",
    "section": "What to do this Week",
    "text": "What to do this Week\nRead:\n\nRDS (R for Data Science): Chapters 5-7\ntidyr pivot vignette\nTidy Data. Hadley Wickham. Journal of statistical software 59 (2014): 1-23.\ntidyr vignette"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#what-to-do-this-week-1",
    "href": "meetups/Meetup3/Meetup3.html#what-to-do-this-week-1",
    "title": "Meetup 3: Tidy Data",
    "section": "What to do this Week",
    "text": "What to do this Week\nHomework:\n\nLab 2: Tidy Data\n\nHelpful tutorial:\n\nTutorial on Cleveland Dot Plots"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#what-to-do-this-week-2",
    "href": "meetups/Meetup3/Meetup3.html#what-to-do-this-week-2",
    "title": "Meetup 3: Tidy Data",
    "section": "What to do this Week",
    "text": "What to do this Week\nWatch (alternative to my lectures):\n\nData Science Box Intro to Tidy Data\nData Science Box Data Wranglig\nData Science Box Data Transformations\nData Science Box Exploration of Tidy Data"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#unstructured-data",
    "href": "meetups/Meetup3/Meetup3.html#unstructured-data",
    "title": "Meetup 3: Tidy Data",
    "section": "Unstructured Data",
    "text": "Unstructured Data\nNeed to reinvent wheel every time to analyze:"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#tidy-data-rebranded-data-matrix",
    "href": "meetups/Meetup3/Meetup3.html#tidy-data-rebranded-data-matrix",
    "title": "Meetup 3: Tidy Data",
    "section": "Tidy Data (Rebranded Data Matrix?)",
    "text": "Tidy Data (Rebranded Data Matrix?)\n\nColumns are variables\nRows are observations\nOne value per cell\n\n\nFig 5.1 from R4DS"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#example",
    "href": "meetups/Meetup3/Meetup3.html#example",
    "title": "Meetup 3: Tidy Data",
    "section": "Example",
    "text": "Example\nUntidy:\n\n\n# A tibble: 5 × 58\n  country   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544 new_sp_m4554\n  &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 Afghani…  2001         129          379          349          274          204\n2 Afghani…  2002          90          476          481          368          246\n3 Afghani…  2003         127          511          436          284          256\n4 Afghani…  2004         139          537          568          360          358\n5 Afghani…  2005         151          606          560          472          453\n# ℹ 51 more variables: new_sp_m5564 &lt;dbl&gt;, new_sp_m65 &lt;dbl&gt;, new_sp_f014 &lt;dbl&gt;,\n#   new_sp_f1524 &lt;dbl&gt;, new_sp_f2534 &lt;dbl&gt;, new_sp_f3544 &lt;dbl&gt;,\n#   new_sp_f4554 &lt;dbl&gt;, new_sp_f5564 &lt;dbl&gt;, new_sp_f65 &lt;dbl&gt;,\n#   new_sn_m014 &lt;dbl&gt;, new_sn_m1524 &lt;dbl&gt;, new_sn_m2534 &lt;dbl&gt;,\n#   new_sn_m3544 &lt;dbl&gt;, new_sn_m4554 &lt;dbl&gt;, new_sn_m5564 &lt;dbl&gt;,\n#   new_sn_m65 &lt;dbl&gt;, new_sn_f014 &lt;dbl&gt;, new_sn_f1524 &lt;dbl&gt;,\n#   new_sn_f2534 &lt;dbl&gt;, new_sn_f3544 &lt;dbl&gt;, new_sn_f4554 &lt;dbl&gt;, …\n\n\nTidy:\n\n\n# A tibble: 7 × 6\n  country      year diagnosis gender age   count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 Afghanistan  2001 sp        m      014     129\n2 Afghanistan  2001 sp        m      1524    379\n3 Afghanistan  2001 sp        m      2534    349\n4 Afghanistan  2001 sp        m      3544    274\n5 Afghanistan  2001 sp        m      4554    204\n6 Afghanistan  2001 sp        m      5564    139\n7 Afghanistan  2001 sp        m      65      103"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#why-does-the-tidyverse-work",
    "href": "meetups/Meetup3/Meetup3.html#why-does-the-tidyverse-work",
    "title": "Meetup 3: Tidy Data",
    "section": "Why Does the TidyVerse Work?",
    "text": "Why Does the TidyVerse Work?\n\n\nTidyverse functions:\n\nTake tidy data as input\nReturn tidy data as output\nCan freely chain together functions\n\n\n\ntidyDataset |&gt; \n  filter(...) |&gt; \n  mutate(...) |&gt; \n  select(...) |&gt; \n  group_by(...) |&gt; \n  summarise(...) |&gt; \n  ggplot(aes(...)) +\n  ...\n\n\nTidyverse is successful because it insists on the tidy format and because it is designed around it. Other ecosystems (base R, pandas, etc) don’t always return tidy output from tidy input."
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#originated-from-relational-databases",
    "href": "meetups/Meetup3/Meetup3.html#originated-from-relational-databases",
    "title": "Meetup 3: Tidy Data",
    "section": "Originated from Relational Databases",
    "text": "Originated from Relational Databases\n\n\nOriginal Tidy Data Def:\n\nColumns are variables\nRows are observations\nOne observational unit per table\n\n\n\n\n\nOriginal Tidy Data Paper\n\n\n\nIf you have studied relational databases you might recognize Codd’s 3rd normal form."
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy",
    "href": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy",
    "title": "Meetup 3: Tidy Data",
    "section": "New Versus Old Tidy",
    "text": "New Versus Old Tidy\n New Tidy- repeats a lot of data on songs but easier to analyze because of flat structure"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy-1",
    "href": "meetups/Meetup3/Meetup3.html#new-versus-old-tidy-1",
    "title": "Meetup 3: Tidy Data",
    "section": "New Versus Old Tidy",
    "text": "New Versus Old Tidy\n Old tidy: Space efficient like a database"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#wide-data",
    "href": "meetups/Meetup3/Meetup3.html#wide-data",
    "title": "Meetup 3: Tidy Data",
    "section": "Wide Data",
    "text": "Wide Data\n\nAll data corresponding to a single observational unit in a single row\nValues of First Column unique\nEasy to process visually\nSome software works better with wide data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack\nwk1\nwk2\nwk3\nwk4\nwk5\nwk6\nwk7\nwk8\nwk9\nwk10\n\n\n\n\nBaby Don’t Cry (Keep…\n87\n82\n72\n77\n87\n94\n99\nNA\nNA\nNA\n\n\nThe Hardest Part Of …\n91\n87\n92\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nKryptonite\n81\n70\n68\n67\n66\n57\n54\n53\n51\n51\n\n\nLoser\n76\n76\n72\n69\n67\n65\n55\n59\n62\n61\n\n\nWobble Wobble\n57\n34\n25\n17\n17\n31\n36\n49\n53\n57"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#tall-data",
    "href": "meetups/Meetup3/Meetup3.html#tall-data",
    "title": "Meetup 3: Tidy Data",
    "section": "Tall Data",
    "text": "Tall Data\n\n\n\nFirst column values repeat\nWorks well with software like R\nHarder to process visually\nTidy data typically Tall\n\n\n\n\n\n\n\ntrack\nWeek\nRank\n\n\n\n\nBaby Don’t Cry (Keep…\n1\n87\n\n\nBaby Don’t Cry (Keep…\n2\n82\n\n\nBaby Don’t Cry (Keep…\n3\n72\n\n\nBaby Don’t Cry (Keep…\n4\n77\n\n\nBaby Don’t Cry (Keep…\n5\n87\n\n\nBaby Don’t Cry (Keep…\n6\n94\n\n\nBaby Don’t Cry (Keep…\n7\n99\n\n\nThe Hardest Part Of …\n1\n91\n\n\n\n\n\n\nTall and wide can contain the same info, distinction between them is not always completely precise."
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\n\n\n\n\n\n\nid\nbp1\nbp2\n\n\n\n\nA\n100\n120\n\n\nB\n140\n115\n\n\nC\n120\n125\n\n\n\n\n\n\n\n\n\n\n\nid\nmeasurement\nvalue\n\n\n\n\nA\nbp1\n100\n\n\nA\nbp2\n120\n\n\nB\nbp1\n140\n\n\nB\nbp2\n115\n\n\nC\nbp1\n120\n\n\nC\nbp2\n125"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-1",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-1",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\n\n# A tibble: 6 × 3\n  id    measurement value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     bp1           100\n2 A     bp2           120\n3 B     bp1           140\n4 B     bp2           115\n5 C     bp1           120\n6 C     bp2           125"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-2",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-2",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\nR4DS 5.3"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-3",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-3",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\nR4DS 5.4"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-longer-4",
    "href": "meetups/Meetup3/Meetup3.html#pivot-longer-4",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\ndf_long = df |&gt; pivot_longer(\n                   cols = bp1:bp2,\n                   names_to = \"measurement\",\n                   values_to = \"value\"\n                   )\n\n\nR4DS 5.5"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider",
    "text": "Pivot Wider\n\nSelect column for names and column for values\nDistinct values in column become new column headers\nValues get mapped from other column\nPotential for NA values…..\n\n\n\n\n\n\nid\nmeasurement\nvalue\n\n\n\n\nA\nbp1\n100\n\n\nA\nbp1\n102\n\n\nB\nbp2\n120\n\n\nB\nbp2\n140\n\n\nA\nbp3\n115"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider-1",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider-1",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider",
    "text": "Pivot Wider\n\n\n\n\n\n\n\nid\nmeas\nval\n\n\n\n\nA\nbp1\n100\n\n\nB\nbp1\n102\n\n\nB\nbp2\n120\n\n\nA\nbp2\n140\n\n\nA\nbp3\n115\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbp1\nbp2\nbp3\n\n\n\n\nA\n100\n140\n115\n\n\nB\n102\n120\nNA\n\n\n\n\n\n\n\ndf |&gt; pivot_wider(\n  names_from = meas,\n  values_from = val\n)"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider-non-unique-values",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider-non-unique-values",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider: Non-unique values",
    "text": "Pivot Wider: Non-unique values\n\n\n\n\n\n\n\nid\nmeas\nval\n\n\n\n\nA\nbp1\n100\n\n\nB\nbp1\n102\n\n\nB\nbp2\n120\n\n\nA\nbp2\n140\n\n\nA\nbp2\n115\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbp1\nbp2\n\n\n\n\nA\n100\n140, 115\n\n\nB\n102\n120\n\n\n\n\n\n\n\ndf |&gt; pivot_wider(\n  names_from = meas,\n  values_from = val\n)"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#pivot-wider-multiple-rows",
    "href": "meetups/Meetup3/Meetup3.html#pivot-wider-multiple-rows",
    "title": "Meetup 3: Tidy Data",
    "section": "Pivot Wider: Multiple Rows",
    "text": "Pivot Wider: Multiple Rows\n\n\n\n\n\n\n\nid\nmeas\nval\ntime\n\n\n\n\nA\nbp1\n100\nday\n\n\nB\nbp1\n102\nnight\n\n\nB\nbp2\n120\nnight\n\n\nA\nbp2\n140\nday\n\n\nA\nbp3\n115\nmorning\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntime\nbp1\nbp2\nbp3\n\n\n\n\nA\nday\n100\n140\nNA\n\n\nB\nnight\n102\n120\nNA\n\n\nA\nmorning\nNA\nNA\n115"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot",
    "href": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot",
    "title": "Meetup 3: Tidy Data",
    "section": "When to use each pivot",
    "text": "When to use each pivot\n\npivot_longer tidies data where variables and/or data are in the column names\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nsp_m_014\nsp_m_1524\nsp_m_2534\nsp_m_3544\nsp_m_4554\n\n\n\n\nAfghanistan\n1980\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1981\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1982\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1983\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n1984\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot-1",
    "href": "meetups/Meetup3/Meetup3.html#when-to-use-each-pivot-1",
    "title": "Meetup 3: Tidy Data",
    "section": "When to use each pivot",
    "text": "When to use each pivot\n\npivot_wider tidies data where measurements are spread across multiple rows\n\n\n\n\n\n\nstation\ndate\nmeasure\nvalue\n\n\n\n\n1\n2024/09/01\ntemp\n25.0 C\n\n\n1\n2024/09/01\npressure\n700mmHg\n\n\n1\n2024/09/01\nhumidity\n60%\n\n\n1\n2024/09/01\nrain\n.2in\n\n\n1\n2024/09/02\ntemp\n22.0 C\n\n\n1\n2024/09/02\npressure\n800mmHg\n\n\n1\n2024/09/02\nhumidity\n40%"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#tidy-data-defined-based-on-your-goals",
    "href": "meetups/Meetup3/Meetup3.html#tidy-data-defined-based-on-your-goals",
    "title": "Meetup 3: Tidy Data",
    "section": "Tidy Data Defined Based on Your Goals",
    "text": "Tidy Data Defined Based on Your Goals\n\n“variable” and “measurement” is partially up to you!\nPick the definitions that are best suited for you analysis"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#weather-station-vignette",
    "href": "meetups/Meetup3/Meetup3.html#weather-station-vignette",
    "title": "Meetup 3: Tidy Data",
    "section": "Weather Station Vignette",
    "text": "Weather Station Vignette\nFollow along:\n\nDownload the data\nDownload the code"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#data-science-in-context-presentation",
    "href": "meetups/Meetup3/Meetup3.html#data-science-in-context-presentation",
    "title": "Meetup 3: Tidy Data",
    "section": "Data Science in Context Presentation",
    "text": "Data Science in Context Presentation\n\nSign up here!"
  },
  {
    "objectID": "meetups/Meetup3/Meetup3.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup3/Meetup3.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 3: Tidy Data",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here\n\n\n\n\nDATA 607"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#lab-1-comments",
    "href": "meetups/Meetup4/Meetup4.html#lab-1-comments",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Lab 1 Comments",
    "text": "Lab 1 Comments\n\nMostly good\nLook at your figures carefully\n\nBe specific when prompting\n\nDon’t print html to pdf, render direct to pdf\nMake sure I can see the results\nDon’t overuse LLMs- I will take off points for massively overbuilt code"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#meetup-wednesday",
    "href": "meetups/Meetup4/Meetup4.html#meetup-wednesday",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Meetup Wednesday!",
    "text": "Meetup Wednesday!\n\n\nJoin us this Wednesday Night at NYU\nnyhackr.org for tickets!"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#week-overview",
    "href": "meetups/Meetup4/Meetup4.html#week-overview",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Week Overview",
    "text": "Week Overview\n\nEDA Lab due Sunday at midnight\nPosted coding demo on reprex/debugging\nHadley Wickham EDA Demo\nSign up for Data Science in Context!\nToday’s focus is EDA (Chapter 10)\n\nVast subject, entire books on it\nNot from book: Inliers, QQ-Plots, Dot-Plots, Pair Plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#case-study-barley-yields-in-mn",
    "href": "meetups/Meetup4/Meetup4.html#case-study-barley-yields-in-mn",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Case Study: Barley Yields in MN",
    "text": "Case Study: Barley Yields in MN\n\n10 Varieties of Barley\nGrown at 6 Different Sites\nYield Measured in 1931 and 1932\nData Used as an Exemplar in Stats Literature"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#barley-data",
    "href": "meetups/Meetup4/Meetup4.html#barley-data",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Barley Data",
    "text": "Barley Data\n\n\n\n\n\nsite\nyear\nvariety\nyield\n\n\n\n\nUniversity Farm\n1931\nManchuria\n27.00000\n\n\nWaseca\n1931\nManchuria\n48.86667\n\n\nMorris\n1931\nManchuria\n27.43334\n\n\nCrookston\n1931\nManchuria\n39.93333\n\n\nGrand Rapids\n1931\nManchuria\n32.96667\n\n\nDuluth\n1931\nManchuria\n28.96667\n\n\nUniversity Farm\n1931\nGlabron\n43.06666\n\n\nWaseca\n1931\nGlabron\n55.20000\n\n\nMorris\n1931\nGlabron\n28.76667\n\n\nCrookston\n1931\nGlabron\n38.13333"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#visualize-the-data",
    "href": "meetups/Meetup4/Meetup4.html#visualize-the-data",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Visualize the Data",
    "text": "Visualize the Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#swapped-years-at-morris",
    "href": "meetups/Meetup4/Meetup4.html#swapped-years-at-morris",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Swapped Years at Morris?",
    "text": "Swapped Years at Morris?"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#years-later-an-investigation",
    "href": "meetups/Meetup4/Meetup4.html#years-later-an-investigation",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "80 Years Later… an Investigation",
    "text": "80 Years Later… an Investigation"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#evidence-emerges",
    "href": "meetups/Meetup4/Meetup4.html#evidence-emerges",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Evidence Emerges",
    "text": "Evidence Emerges\n\nAmbiguity in years of study noticed (1930 and 1931 versus 1931 and 1932)\nSwapping of a different sample discovered\nStatistical fits much more parsimonious on swapped data\nI would bet on data error, but we may never know without more experiments"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#what-is-exploratory-data-analysis",
    "href": "meetups/Meetup4/Meetup4.html#what-is-exploratory-data-analysis",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "What is Exploratory Data Analysis",
    "text": "What is Exploratory Data Analysis\nExploratory Data Analysis is the art of looking at data in a systematic way in order to understand the underlying structure of the data.\nTwo main goals:\n\nEnsure data quality\nUncover patterns to guide future analysis\n\nEDA is detective work- you ask and answer questions, which inspires more questions"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#eda-steps",
    "href": "meetups/Meetup4/Meetup4.html#eda-steps",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "EDA Steps",
    "text": "EDA Steps\n\nGeneral Characteristics of Data and Descriptive Stats (summary, skim, counts)\nVisualize Variation (Histograms, QQ Plots, Box plots, …)\nDeal with outliers, inliers, missing data\nVisualize relationships (bivariate and multivariate plots)\nIterate"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#quantile-quantile-qq-plots",
    "href": "meetups/Meetup4/Meetup4.html#quantile-quantile-qq-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Quantile-Quantile (QQ) Plots",
    "text": "Quantile-Quantile (QQ) Plots\n\nSorts data in increasing order, plots against sorted data from a probability distribution (usually Gaussian) or other dataset\nMost powerful/underused univariate visualization\n\nCreate percentile vec seq(0,1,1/(N+1))\nDrop ends and calculate data using qnorm(seq)\nScatterplot of Gaussian quantiles on x axis, your data on y\ngeom_qq, stat_qq do this for you"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data",
    "href": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "QQ Plot for US Cereals Data",
    "text": "QQ Plot for US Cereals Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-1",
    "href": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "QQ Plot for US Cereals Data",
    "text": "QQ Plot for US Cereals Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-2",
    "href": "meetups/Meetup4/Meetup4.html#qq-plot-for-us-cereals-data-2",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "QQ Plot for US Cereals Data",
    "text": "QQ Plot for US Cereals Data"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#comparing-visualizations",
    "href": "meetups/Meetup4/Meetup4.html#comparing-visualizations",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Comparing visualizations",
    "text": "Comparing visualizations"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#outliers",
    "href": "meetups/Meetup4/Meetup4.html#outliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Outliers",
    "text": "Outliers\n\nOutlier is a data point you suspect was generated by a different mechanism than the rest of your dataset"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#what-could-outliers-be",
    "href": "meetups/Meetup4/Meetup4.html#what-could-outliers-be",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "What could outliers be?",
    "text": "What could outliers be?\n\nCould be a spurrious value\n\nSomeone added an extra 0 to the spreadsheet value\nThe measurement device was broken or miscalibrated"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#what-could-outliers-be-1",
    "href": "meetups/Meetup4/Meetup4.html#what-could-outliers-be-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "What could outliers be?",
    "text": "What could outliers be?\n\nCould be a major discovery\n\nSign of a phenomenon you don’t understand yet\n\nCould be you just have a non-normal distribution\n\nNever discard outliers without thinking and investigating them first!"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#criteria-for-outliers",
    "href": "meetups/Meetup4/Meetup4.html#criteria-for-outliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Criteria for Outliers",
    "text": "Criteria for Outliers\n\nHeuristic for suspecting outliers:\n\n\\[ y &gt; p_{75} + k \\mathrm{IQR}\\] Common to pick \\(k=1.5\\)\n\nYour domain specific expertise tells you what to do with outliers!"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#inliers",
    "href": "meetups/Meetup4/Meetup4.html#inliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Inliers",
    "text": "Inliers\n\nInliers: Value in the interior of the distribution of your data that is in error"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#inliers-1",
    "href": "meetups/Meetup4/Meetup4.html#inliers-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Inliers",
    "text": "Inliers\n\nMuch more subtle and pernicious than outliers\nOften Disguised Missing Data\n\nNA values systematically coded as 0 or some other default\nBusiness or government transactions that require a some information for a form to be filled out that isn’t always available\n\nCommon Manifestation in Repated Values"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#check-for-inliers",
    "href": "meetups/Meetup4/Meetup4.html#check-for-inliers",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Check for Inliers",
    "text": "Check for Inliers\n\nFlat regions of your QQ-Plot\nLook for outliers of your count data:\n\n\nUScereal |&gt; \n  group_by(fibre) |&gt;\n  summarise(count = n()) |&gt; \n  arrange(desc(count)) |&gt; \n  ggplot(aes(x=fibre,y=count)) +\n  geom_point()\n\n\nOnce you find them investigate\nCan also use boxplots, look at your data, or compute summary statistics"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#check-for-inliers-1",
    "href": "meetups/Meetup4/Meetup4.html#check-for-inliers-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Check for Inliers",
    "text": "Check for Inliers\n\nFlat regions of your QQ-Plot\nLook for outliers of your count data:"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#visualizing-covariation",
    "href": "meetups/Meetup4/Meetup4.html#visualizing-covariation",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Visualizing Covariation",
    "text": "Visualizing Covariation\n\nCovariation is how two variables vary together\nNumerical and Categorical\n\nNumerical variable has difference distribution for different values of the categorical variable\n\nTools: Boxplots, violin plots\n\nTwo Numerical Variables\n\nScatterplots, 2D density plots such as hex-plots\nPairs Plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#tukey-box-plot",
    "href": "meetups/Meetup4/Meetup4.html#tukey-box-plot",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Tukey Box Plot",
    "text": "Tukey Box Plot\n\nThe Tukey Box Plot is a robust visualization\nMedian, IQR, potential outliers"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#tukey-box-plot-1",
    "href": "meetups/Meetup4/Meetup4.html#tukey-box-plot-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Tukey Box Plot",
    "text": "Tukey Box Plot\n\nThe Tukey Box Plot is a robust visualization\nMedian, IQR, potential outliers"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#tukey-box-plot-2",
    "href": "meetups/Meetup4/Meetup4.html#tukey-box-plot-2",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Tukey Box Plot",
    "text": "Tukey Box Plot\n\nThe Tukey Box Plot is a robust visualization\nMedian, IQR, potential outliers\nCode:\n\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot() +\n  theme_bw(base_size=24)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#eda-detective-work",
    "href": "meetups/Meetup4/Meetup4.html#eda-detective-work",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "EDA Detective Work",
    "text": "EDA Detective Work\n\nWhen you make a plot- ask questions\nShouldn’t high quality cost more?"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#look-at-other-variables",
    "href": "meetups/Meetup4/Meetup4.html#look-at-other-variables",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Look at other variables",
    "text": "Look at other variables\n\nClearer diamonds aren’t more expensive"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#look-at-other-variables-1",
    "href": "meetups/Meetup4/Meetup4.html#look-at-other-variables-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Look at other variables",
    "text": "Look at other variables\n\nBigger diamonds are very pricey"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity",
    "href": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "How does size covary with cut/clarity?",
    "text": "How does size covary with cut/clarity?\n\nBig diamonds have worse clarity"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity-1",
    "href": "meetups/Meetup4/Meetup4.html#how-does-size-covary-with-cutclarity-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "How does size covary with cut/clarity?",
    "text": "How does size covary with cut/clarity?\n\nBig diamonds have slightly worse clarity"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#simpsons-paradox",
    "href": "meetups/Meetup4/Meetup4.html#simpsons-paradox",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\n\nFor fixed size, quality leads to higher price\nSize is a common cause of price and quality"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#density-plots",
    "href": "meetups/Meetup4/Meetup4.html#density-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Density Plots",
    "text": "Density Plots\n\nWhen there are lots of points, density plots are better than scatter plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#density-plots-1",
    "href": "meetups/Meetup4/Meetup4.html#density-plots-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Density Plots",
    "text": "Density Plots\n\nWhen there are lots of points, density plots are better than scatter plots\n\n\nggplot(diamonds, aes(x = carat, y = price)) +\n  geom_hex() +\n  theme_bw(base_size=24)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos",
    "href": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Pair Plots to Visualize All Combos",
    "text": "Pair Plots to Visualize All Combos\n\nGGally package ggpairs function"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos-1",
    "href": "meetups/Meetup4/Meetup4.html#pair-plots-to-visualize-all-combos-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Pair Plots to Visualize All Combos",
    "text": "Pair Plots to Visualize All Combos\n\npenguins |&gt; ggpairs(columns=3:6)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#fancy-pairs",
    "href": "meetups/Meetup4/Meetup4.html#fancy-pairs",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Fancy Pairs",
    "text": "Fancy Pairs\n\npenguins |&gt; ggpairs(columns=3:6,ggplot2::aes(colour = species))"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#multiway-visualizations-dot-plots",
    "href": "meetups/Meetup4/Meetup4.html#multiway-visualizations-dot-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Multiway Visualizations: Dot Plots",
    "text": "Multiway Visualizations: Dot Plots\n\nMultiple categorical variables and a response\nCleveland and McGill 1984\nBest to worst rank of perceptual cues for learning:\n\n\n\nDot-Chart created to emphasize position comparisons"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#dot-plots",
    "href": "meetups/Meetup4/Meetup4.html#dot-plots",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Dot Plots",
    "text": "Dot Plots"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#dot-plots-1",
    "href": "meetups/Meetup4/Meetup4.html#dot-plots-1",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Dot Plots",
    "text": "Dot Plots\n\nbarley |&gt; \n  ggplot(aes(x=yield,y=variety,color=year)) +\n  geom_point(size=2) +\n  facet_wrap(~site,nrow = 3) +\n  theme_bw(base_size = 10)"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#what-didnt-we-talk-about",
    "href": "meetups/Meetup4/Meetup4.html#what-didnt-we-talk-about",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "What Didn’t We Talk About",
    "text": "What Didn’t We Talk About\n\nSimple Models are used a lot in EDA\n\nSee example in Chapter 10\nResiduals remove the effect of a major variable and see what remains\n\nTransformations\n\nWe will discuss transformations more next week\nReexpress data in new way\nMost important by far is log transform"
  },
  {
    "objectID": "meetups/Meetup4/Meetup4.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup4/Meetup4.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 4: Exploratory Data Analysis",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#last-few-weeks-of-semester",
    "href": "meetups/Meetup14/Meetup14.html#last-few-weeks-of-semester",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Last few weeks of semester",
    "text": "Last few weeks of semester\n\nData Science in Context\nVideo Presentation/Final Projects\nIf you are missing work get it in ASAP\nFinal lab due this Sunday"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#why-distributed-computing",
    "href": "meetups/Meetup14/Meetup14.html#why-distributed-computing",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Why distributed computing?",
    "text": "Why distributed computing?\nAnswer: For the same reason you wouldn’t have one person build the NYC subway\n\nMany problems cannot be solved by a single processor/computer\n\nLimited by processing power\nLimited by memory\n\nEnormous applications:\n\nInvented for physical sciences\nBig Data, AI, Machine Learning, etc etc"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#what-is-distributed-computing",
    "href": "meetups/Meetup14/Meetup14.html#what-is-distributed-computing",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "What is Distributed Computing",
    "text": "What is Distributed Computing\nSerial computing: single task at a time:"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#what-is-distributed-computing-1",
    "href": "meetups/Meetup14/Meetup14.html#what-is-distributed-computing-1",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "What is Distributed Computing",
    "text": "What is Distributed Computing\n\nParallel or distributed Computing: divide tasks among compute units:\n\n\nllnl-hpc"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#distributed-computing-paradigms-shared-memory",
    "href": "meetups/Meetup14/Meetup14.html#distributed-computing-paradigms-shared-memory",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Distributed Computing Paradigms: Shared Memory",
    "text": "Distributed Computing Paradigms: Shared Memory\n\nllnl-hpc"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#distributed-computing-paradigms-cluster",
    "href": "meetups/Meetup14/Meetup14.html#distributed-computing-paradigms-cluster",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Distributed Computing Paradigms: Cluster",
    "text": "Distributed Computing Paradigms: Cluster\n\nllnl-hpc"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#distributed-computing-paradigms-distributed",
    "href": "meetups/Meetup14/Meetup14.html#distributed-computing-paradigms-distributed",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Distributed Computing Paradigms: Distributed",
    "text": "Distributed Computing Paradigms: Distributed\n\nllnl-hpc"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#apache-spark-architecture",
    "href": "meetups/Meetup14/Meetup14.html#apache-spark-architecture",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Apache Spark Architecture",
    "text": "Apache Spark Architecture\n\nSpark is a tool for high performance computing on a distributed cluster\n“Secret Sauce” is something called an RDD or “Resilient Distributed Dataset”\nDeveloped in 2009 and spun out of UC Berkeley\nRequires two external resources:\n\nDistributed file system (disk)\nCluster manager (cloud)"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#pros-of-spark",
    "href": "meetups/Meetup14/Meetup14.html#pros-of-spark",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Pros of Spark",
    "text": "Pros of Spark\n\nExtremely fast\nComplexity under the hood\nVery scalable\nRobust/fault tolerant\nGood library support for machine learning, graphs, and databases\nGreat for streaming analysis"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#cons-of-spark",
    "href": "meetups/Meetup14/Meetup14.html#cons-of-spark",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Cons of Spark",
    "text": "Cons of Spark\n\nMemory requirements $$$\nCan require programming chops as you get more into it"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#sparklyr",
    "href": "meetups/Meetup14/Meetup14.html#sparklyr",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "sparklyr",
    "text": "sparklyr\n\nYou can do data analysis in spark using almost the same syntax as dplyr\nComputational process is different- workflow creates a series of instructions, gets executed all at once\nOnly certain key results returned to R by collect\n\n\nMastering Spark with R"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#creating-a-spark-instance",
    "href": "meetups/Meetup14/Meetup14.html#creating-a-spark-instance",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Creating a Spark instance",
    "text": "Creating a Spark instance\n\nNormally you run spark on a cluster or a data center\nCan generate a “local” spark instance to practice\n\n\nlibrary(sparklyr)\nlibrary(tidyverse)\n\nspark_instance &lt;- spark_connect(master = \"local\", version = \"3.3\")\n\nspark_instance\n\n$master\n[1] \"local[20]\"\n\n$method\n[1] \"shell\"\n\n$app_name\n[1] \"sparklyr\"\n\n$config\n$spark.env.SPARK_LOCAL_IP.local\n[1] \"127.0.0.1\"\n\n$sparklyr.connect.csv.embedded\n[1] \"^1.*\"\n\n$spark.sql.legacy.utcTimestampFunc.enabled\n[1] TRUE\n\n$sparklyr.connect.cores.local\n[1] 20\n\n$spark.sql.shuffle.partitions.local\n[1] 20\n\n$sparklyr.shell.name\n[1] \"sparklyr\"\n\n$`sparklyr.shell.driver-memory`\n[1] \"2g\"\n\n\n$state\n&lt;environment: 0x57df2f50bb68&gt;\n\n$extensions\n$extensions$jars\ncharacter(0)\n\n$extensions$packages\ncharacter(0)\n\n$extensions$initializers\nlist()\n\n$extensions$catalog_jars\ncharacter(0)\n\n$extensions$repositories\ncharacter(0)\n\n$extensions$dbplyr_sql_variant\n$extensions$dbplyr_sql_variant$scalar\nlist()\n\n$extensions$dbplyr_sql_variant$aggregate\nlist()\n\n$extensions$dbplyr_sql_variant$window\nlist()\n\n\n\n$spark_home\n[1] \"/home/georgehagstrom/spark/spark-3.3.4-bin-hadoop3\"\n\n$backend\nA connection with                              \ndescription \"-&gt;localhost:8881\"\nclass       \"sockconn\"        \nmode        \"wb\"              \ntext        \"binary\"          \nopened      \"opened\"          \ncan read    \"yes\"             \ncan write   \"yes\"             \n\n$monitoring\nA connection with                              \ndescription \"-&gt;localhost:8881\"\nclass       \"sockconn\"        \nmode        \"wb\"              \ntext        \"binary\"          \nopened      \"opened\"          \ncan read    \"yes\"             \ncan write   \"yes\"             \n\n$gateway\nA connection with                              \ndescription \"-&gt;localhost:8880\"\nclass       \"sockconn\"        \nmode        \"rb\"              \ntext        \"binary\"          \nopened      \"opened\"          \ncan read    \"yes\"             \ncan write   \"yes\"             \n\n$output_file\n[1] \"/tmp/Rtmps6czbH/file148cb0686536ab_spark.log\"\n\n$sessionId\n[1] 40261\n\n$home_version\n[1] \"3.3\"\n\nattr(,\"class\")\n[1] \"spark_connection\"       \"spark_shell_connection\" \"DBIConnection\""
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#getting-data-into-spark",
    "href": "meetups/Meetup14/Meetup14.html#getting-data-into-spark",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Getting data into spark",
    "text": "Getting data into spark\n\nSmall datasets for practice: copy_to\n\n\nspark_cars = spark_instance |&gt; copy_to(mtcars)\n\nspark_cars\n\n# Source:   table&lt;`mtcars`&gt; [?? x 11]\n# Database: spark_connection\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ more rows"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#getting-data-into-spark-1",
    "href": "meetups/Meetup14/Meetup14.html#getting-data-into-spark-1",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Getting data into spark",
    "text": "Getting data into spark\n\nAlso possible to read files from a regular disk\nDistributes data across the cluster\n\n\ntaxi_spark = spark_read_csv(spark_instance,\"/home/georgehagstrom/Downloads/taxi_november_2021.csv\", memory = TRUE)\n\ntaxi_spark\n\n# Source:   table&lt;`taxi_november_2021_cefd428b_f2fa_413d_9eef_d5fbf361884f`&gt; [?? x 18]\n# Database: spark_connection\n   VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count\n      &lt;int&gt; &lt;dttm&gt;               &lt;dttm&gt;                &lt;chr&gt;          \n 1        2 2021-11-02 20:04:49  2021-11-02 20:12:26   2              \n 2        2 2021-11-02 20:22:13  2021-11-03 02:42:36   1              \n 3        2 2021-11-24 01:43:52  2021-11-24 01:53:43   1              \n 4        2 2021-11-24 01:56:09  2021-11-24 02:02:41   1              \n 5        2 2021-11-24 02:04:46  2021-11-24 02:17:20   1              \n 6        2 2021-11-16 15:45:39  2021-11-16 15:56:09   3              \n 7        2 2021-11-16 16:14:02  2021-11-16 23:30:18   2              \n 8        2 2021-11-01 05:58:54  2021-11-01 06:27:29   1              \n 9        2 2021-11-01 07:06:44  2021-11-01 07:27:44   1              \n10        2 2021-11-01 08:43:07  2021-11-01 09:12:14   1              \n# ℹ more rows\n# ℹ 14 more variables: trip_distance &lt;dbl&gt;, RatecodeID &lt;chr&gt;,\n#   store_and_fwd_flag &lt;chr&gt;, PULocationID &lt;int&gt;, DOLocationID &lt;int&gt;,\n#   payment_type &lt;int&gt;, fare_amount &lt;dbl&gt;, extra &lt;dbl&gt;, mta_tax &lt;dbl&gt;,\n#   tip_amount &lt;dbl&gt;, tolls_amount &lt;dbl&gt;, improvement_surcharge &lt;dbl&gt;,\n#   total_amount &lt;dbl&gt;, congestion_surcharge &lt;chr&gt;"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#data-lake-versus-etl",
    "href": "meetups/Meetup14/Meetup14.html#data-lake-versus-etl",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Data Lake versus ETL",
    "text": "Data Lake versus ETL\n\nSpark works well with “Data Lakes”\n\nStore data in its “natural” format, load into spark\nAlternative is to transform all new data into a data warehouse\n\n\n\nMastering Spark with R"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#transformations-and-actions",
    "href": "meetups/Meetup14/Meetup14.html#transformations-and-actions",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Transformations and Actions",
    "text": "Transformations and Actions\n\nTwo types main types of functions in sparklyr/spark\nTransformations create a logical string of operations\n\nfilter, select, join, map, mutate, …\n\nActions execute the transformations and can return results to R\n\ncollect, compute"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#example",
    "href": "meetups/Meetup14/Meetup14.html#example",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Example",
    "text": "Example\n\nTransformations use “lazy evaluation”\n\n\ntaxi_v2 = taxi_spark |&gt; \n  filter(VendorID == 2) |&gt;  group_by(passenger_count) |&gt; \n  summarise( num_trips = n(),\n    mean_distance =  mean(trip_distance,na.rm = TRUE),\n    mean_fare = mean(fare_amount,na.rm = TRUE),\n    mean_tip = mean(tip_amount,na.rm = TRUE))\n\ntaxi_v2\n\n# Source:   SQL [?? x 5]\n# Database: spark_connection\n   passenger_count num_trips mean_distance mean_fare mean_tip\n   &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 1                 1712508         3.23       13.6     2.59\n 2 4                   39014         3.36       15.0     2.62\n 3 2                  341259         3.64       15.0     2.75\n 4 5                   72569         3.21       13.6     2.61\n 5 6                   45744         3.30       13.8     2.64\n 6 0                     232         0.446      20.9     3.35\n 7 3                   94551         3.34       14.4     2.65\n 8 7                       7         5.03       56.6     3.83\n 9 9                       4         8.1        93.3     4.58\n10 8                       1        21.7        85       0   \n11 NA                  98449       151.         25.3     3.34"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#actions-complete-the-computation",
    "href": "meetups/Meetup14/Meetup14.html#actions-complete-the-computation",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Actions complete the computation",
    "text": "Actions complete the computation\n\ntaxi_v2 |&gt; collect()\n\n# A tibble: 11 × 5\n   passenger_count num_trips mean_distance mean_fare mean_tip\n   &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 1                 1712508         3.23       13.6     2.59\n 2 4                   39014         3.36       15.0     2.62\n 3 2                  341259         3.64       15.0     2.75\n 4 5                   72569         3.21       13.6     2.61\n 5 6                   45744         3.30       13.8     2.64\n 6 0                     232         0.446      20.9     3.35\n 7 3                   94551         3.34       14.4     2.65\n 8 7                       7         5.03       56.6     3.83\n 9 9                       4         8.1        93.3     4.58\n10 8                       1        21.7        85       0   \n11 NA                  98449       151.         25.3     3.34\n\n\n\nUse collect() to execute your analysis and bring the results to R for visualization etc"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#caching",
    "href": "meetups/Meetup14/Meetup14.html#caching",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Caching",
    "text": "Caching\n\nCommon workflow involves splitting into two steps:\n\nData wrangling/transformations\nStatistical modeling\n\nIt can be most efficient to cache outcome of transformations\nUse compute to create an intermediate dataset in spark\n\n\ncached_taxi = taxi_v2 |&gt; compute()"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup14/Meetup14.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup14/Meetup14.html#data-science-in-context-presentations",
    "href": "meetups/Meetup14/Meetup14.html#data-science-in-context-presentations",
    "title": "Meetup 14: Distributed Computing and Apache Spark",
    "section": "Data Science in Context Presentations",
    "text": "Data Science in Context Presentations"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#proposal-feedback",
    "href": "meetups/Meetup10/Meetup10.html#proposal-feedback",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Proposal Feedback",
    "text": "Proposal Feedback\n\nI saw lots of great proposal ideas\nThe more research and reading you do on your topic, the better\n\n“If you want to be a good data scientist, you should spend ~49% of your time developing your statistical intuition (i.e. how to ask good questions of the data), and ~49% of your time on domain knowledge (improving overall understanding of your field). Only ~2% on methods per se.”\n- Nate Silver"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#proposal-feedback-1",
    "href": "meetups/Meetup10/Meetup10.html#proposal-feedback-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Proposal Feedback",
    "text": "Proposal Feedback\n\nFor future proposals:\n\nAlways have some references\nI don’t need so many details on how you will do an EDA or what figures you will, devote more space to discussing your problem and potential challenges"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#week-10-nuts-and-bolts",
    "href": "meetups/Meetup10/Meetup10.html#week-10-nuts-and-bolts",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Week 10 Nuts and Bolts",
    "text": "Week 10 Nuts and Bolts\n\nTwo topics and two vignettes\nWorking with APIs\n\nUsing httr2 to get EPA air quality data\n\nCollaboration and git\n\nShort Vignette on merge conflicts based on your homework"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#two-part-tidyverse-assignment",
    "href": "meetups/Meetup10/Meetup10.html#two-part-tidyverse-assignment",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Two part Tidyverse Assignment",
    "text": "Two part Tidyverse Assignment\n\nAPI part:\n\nFind a web API\nLearn how to use it\nGet some data from it\nDo an analysis\n\ngithub part:\n\nclone github.com/georgehagstrom/FALL2025TIDYVERSE\nAdd your analysis as a separate file\nupdate the readme.md"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#assignment-considerations",
    "href": "meetups/Meetup10/Meetup10.html#assignment-considerations",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Assignment Considerations",
    "text": "Assignment Considerations\n\nThe biggest “challenge” with github will be resolving merge conflicts in the README.md file\nBest practice to minimize merge conflicts:\n\npull the repository\nadd your changes to README.md\npush\n\nIt should be straightforward to upload your qmd analysis\nTurn in a pdf to Brightspace"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#restful-apis",
    "href": "meetups/Meetup10/Meetup10.html#restful-apis",
    "title": "Meetup 10: Web APIs and Git",
    "section": "RESTful APIs",
    "text": "RESTful APIs\n\nAPI is an interface that allows applications to communicate\nEach one is unique- read the docs to figure out the rules\nREST APIs are a common standard\nUses HTTP, encodes commands in url\nFour verbs: get, post, put, delete\nHierarchical Output (usually json)"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#httr2-package",
    "href": "meetups/Meetup10/Meetup10.html#httr2-package",
    "title": "Meetup 10: Web APIs and Git",
    "section": "httr2 package",
    "text": "httr2 package\n\nhttr2 let’s you construct API calls systematically\n\n\nlibrary(tidyverse)\nlibrary(httr2)\n\nreq_weather = request(\"https://api.weather.gov/\")\nreq_weather |&gt; req_dry_run()\n\nGET / HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip, br, zstd\nhost: api.weather.gov\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/7.81.0"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#httr2-package-1",
    "href": "meetups/Meetup10/Meetup10.html#httr2-package-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "httr2 package",
    "text": "httr2 package\n\nreq_weather |&gt; req_perform() |&gt; resp_body_html()\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n        &lt;div class=\"wrapper\"&gt;\\n            &lt;section class=\"usa-ba ...\n\n\n\nfunctions start with req_* or resp_*\n\nreq_* builds requests\nres_* reads responses"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#query-structure",
    "href": "meetups/Meetup10/Meetup10.html#query-structure",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Query Structure",
    "text": "Query Structure\n\nTypically send requests to different endpoints\nUse req_url_path_append to add endpoint url\n\n\nalert_url = \"alerts/active\"\nrequest_weather_alerts = req_weather |&gt; req_url_path_append(alert_url)\n\nrequest_weather_alerts |&gt; req_dry_run()\n\nGET /alerts/active HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip, br, zstd\nhost: api.weather.gov\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/7.81.0"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#query-structure-1",
    "href": "meetups/Meetup10/Meetup10.html#query-structure-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Query Structure",
    "text": "Query Structure\n\nUse req_url_query to add “instructions” to url\n\n\nfl_alerts = request_weather_alerts |&gt; req_url_query(area = \"FL\")\n\nfl_alerts |&gt; req_dry_run()\n\nGET /alerts/active HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip, br, zstd\nhost: api.weather.gov\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/7.81.0"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#processing-output",
    "href": "meetups/Meetup10/Meetup10.html#processing-output",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Processing Output",
    "text": "Processing Output\n\nUsually get json\ntibblify helps you unnest\n\n\nlibrary(tibblify)\nfl_alert_data = fl_alerts |&gt; \n  req_perform() |&gt; \n  resp_body_json() |&gt; \n  tibblify()"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#processing-output-1",
    "href": "meetups/Meetup10/Meetup10.html#processing-output-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Processing Output",
    "text": "Processing Output\n\nlibrary(tibblify)\nfl_alert_data = fl_alerts |&gt; \n  req_perform() |&gt; \n  resp_body_json() |&gt; \n  tibblify()\n\nfl_alert_data\n\n$`@context`\n$`@context`[[1]]\n[1] \"https://geojson.org/geojson-ld/geojson-context.jsonld\"\n\n$`@context`[[2]]\n$`@context`[[2]]$`@version`\n[1] \"1.1\"\n\n$`@context`[[2]]$wx\n[1] \"https://api.weather.gov/ontology#\"\n\n$`@context`[[2]]$`@vocab`\n[1] \"https://api.weather.gov/ontology#\"\n\n\n\n$type\n[1] \"FeatureCollection\"\n\n$features\n# A tibble: 14 × 4\n   id                                       type  geometry$type properties$`@id`\n   &lt;chr&gt;                                    &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;           \n 1 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n 2 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n 3 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n 4 https://api.weather.gov/alerts/urn:oid:… Feat… Polygon       https://api.wea…\n 5 https://api.weather.gov/alerts/urn:oid:… Feat… Polygon       https://api.wea…\n 6 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n 7 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n 8 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n 9 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n10 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n11 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n12 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n13 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n14 https://api.weather.gov/alerts/urn:oid:… Feat… &lt;NA&gt;          https://api.wea…\n# ℹ 31 more variables: geometry$coordinates &lt;list&gt;, properties$`@type` &lt;chr&gt;,\n#   $id &lt;chr&gt;, $areaDesc &lt;chr&gt;, $geocode &lt;tibble[,2]&gt;, $affectedZones &lt;list&gt;,\n#   $references &lt;list&lt;tibble[,4]&gt;&gt;, $sent &lt;chr&gt;, $effective &lt;chr&gt;,\n#   $onset &lt;chr&gt;, $expires &lt;chr&gt;, $ends &lt;chr&gt;, $status &lt;chr&gt;,\n#   $messageType &lt;chr&gt;, $category &lt;chr&gt;, $severity &lt;chr&gt;, $certainty &lt;chr&gt;,\n#   $urgency &lt;chr&gt;, $event &lt;chr&gt;, $sender &lt;chr&gt;, $senderName &lt;chr&gt;,\n#   $headline &lt;chr&gt;, $description &lt;chr&gt;, $instruction &lt;chr&gt;, $response &lt;chr&gt;, …\n\n$title\n[1] \"Current watches, warnings, and advisories for Florida\"\n\n$updated\n[1] \"2025-10-29T17:04:43+00:00\""
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git",
    "href": "meetups/Meetup10/Meetup10.html#git",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git",
    "text": "git\n\nGit is an open source version control tool\nTracks different branches of projects, stores files according to their differences\nDesigned for collaborative teams of programmers\nDifferent from github or gitlab, which are repositories with git functionality"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#why-is-it-needed",
    "href": "meetups/Meetup10/Meetup10.html#why-is-it-needed",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Why is it Needed?",
    "text": "Why is it Needed?\n\nJorge Cham PhD Comis"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-structure",
    "href": "meetups/Meetup10/Meetup10.html#git-structure",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Structure",
    "text": "git Structure"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-structure-1",
    "href": "meetups/Meetup10/Meetup10.html#git-structure-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Structure",
    "text": "git Structure"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-structure-2",
    "href": "meetups/Meetup10/Meetup10.html#git-structure-2",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Structure",
    "text": "git Structure"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-clone",
    "href": "meetups/Meetup10/Meetup10.html#git-clone",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Clone",
    "text": "git Clone"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-add",
    "href": "meetups/Meetup10/Meetup10.html#git-add",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Add",
    "text": "git Add"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-add-staging-area",
    "href": "meetups/Meetup10/Meetup10.html#git-add-staging-area",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Add (Staging Area)",
    "text": "git Add (Staging Area)"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-commit",
    "href": "meetups/Meetup10/Meetup10.html#git-commit",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Commit",
    "text": "git Commit"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-tracks-commits",
    "href": "meetups/Meetup10/Meetup10.html#git-tracks-commits",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Tracks Commits",
    "text": "git Tracks Commits"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-checkout-changing-position",
    "href": "meetups/Meetup10/Meetup10.html#git-checkout-changing-position",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Checkout: Changing Position",
    "text": "git Checkout: Changing Position"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-branching",
    "href": "meetups/Meetup10/Meetup10.html#git-branching",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Branching",
    "text": "git Branching"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-push",
    "href": "meetups/Meetup10/Meetup10.html#git-push",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Push",
    "text": "git Push"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-pull",
    "href": "meetups/Meetup10/Meetup10.html#git-pull",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Pull",
    "text": "git Pull"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#git-pull-1",
    "href": "meetups/Meetup10/Meetup10.html#git-pull-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "git Pull",
    "text": "git Pull"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#merge-conflicts-push",
    "href": "meetups/Meetup10/Meetup10.html#merge-conflicts-push",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Merge Conflicts Push",
    "text": "Merge Conflicts Push"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#merge-conflicts-pull-fix",
    "href": "meetups/Meetup10/Meetup10.html#merge-conflicts-pull-fix",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Merge Conflicts Pull Fix",
    "text": "Merge Conflicts Pull Fix"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#merge-conflicts-push-1",
    "href": "meetups/Meetup10/Meetup10.html#merge-conflicts-push-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Merge Conflicts Push",
    "text": "Merge Conflicts Push"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#rebase",
    "href": "meetups/Meetup10/Meetup10.html#rebase",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Rebase",
    "text": "Rebase"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#rebase-1",
    "href": "meetups/Meetup10/Meetup10.html#rebase-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Rebase",
    "text": "Rebase"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#rebase-2",
    "href": "meetups/Meetup10/Meetup10.html#rebase-2",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Rebase",
    "text": "Rebase"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#merge-conflicts-pull",
    "href": "meetups/Meetup10/Meetup10.html#merge-conflicts-pull",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Merge Conflicts Pull",
    "text": "Merge Conflicts Pull"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#merge-conflicts-pull-1",
    "href": "meetups/Meetup10/Meetup10.html#merge-conflicts-pull-1",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Merge Conflicts Pull",
    "text": "Merge Conflicts Pull"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#push-frequently",
    "href": "meetups/Meetup10/Meetup10.html#push-frequently",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Push Frequently",
    "text": "Push Frequently\n\nHe/She who pushes first doesn’t have to deal with merge conflicts when pulling\nTry not to let your branch build up too many un-pushed changes"
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#what-else-about-git",
    "href": "meetups/Meetup10/Meetup10.html#what-else-about-git",
    "title": "Meetup 10: Web APIs and Git",
    "section": "What else about git?",
    "text": "What else about git?\n\nYou may not be allowed to push\n\nIn this case repo owner will have to approve your pull request\nYou may want to fork the repo\n\nMany other commands- learn them depending on your context\n\nstash, status, rebase, squash, etc…."
  },
  {
    "objectID": "meetups/Meetup10/Meetup10.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup10/Meetup10.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 10: Web APIs and Git",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup10/APISlides.html#restful-apis",
    "href": "meetups/Meetup10/APISlides.html#restful-apis",
    "title": "API Slides",
    "section": "RESTful APIs",
    "text": "RESTful APIs\n\nAPI is an interface that allows applications to communicate\nEach one is unique- read the docs to figure out the rules\nREST APIs are a common standard\nUses HTTP, encodes commands in url\nFour verbs: get, post, put, delete\nHierarchical Output (usually json)"
  },
  {
    "objectID": "meetups/Meetup10/APISlides.html#httr2-package",
    "href": "meetups/Meetup10/APISlides.html#httr2-package",
    "title": "API Slides",
    "section": "httr2 package",
    "text": "httr2 package\n\nhttr2 let’s you construct API calls systematically\n\n\nlibrary(tidyverse)\nlibrary(httr2)\n\nreq_weather = request(\"https://api.weather.gov/\")\nreq_weather |&gt; req_dry_run()\n\nGET / HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip, br, zstd\nhost: api.weather.gov\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/7.81.0\n\nreq_weather |&gt; req_perform() |&gt; resp_body_html()\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n        &lt;div class=\"wrapper\"&gt;\\n            &lt;section class=\"usa-ba ..."
  },
  {
    "objectID": "meetups/Meetup10/APISlides.html#httr2-package-1",
    "href": "meetups/Meetup10/APISlides.html#httr2-package-1",
    "title": "API Slides",
    "section": "httr2 package",
    "text": "httr2 package\n\nfunctions start with req_* or resp_*\n\nreq_* builds requests\nres_* reads responses"
  },
  {
    "objectID": "meetups/Meetup10/APISlides.html#query-structure",
    "href": "meetups/Meetup10/APISlides.html#query-structure",
    "title": "API Slides",
    "section": "Query Structure",
    "text": "Query Structure\n\nTypically send requests to different endpoints\nUse req_url_path_append to add endpoint url\n\n\nalert_url = \"alerts/active\"\nrequest_weather_alerts = req_weather |&gt; req_url_path_append(alert_url)\n\nrequest_weather_alerts |&gt; req_dry_run()\n\nGET /alerts/active HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip, br, zstd\nhost: api.weather.gov\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/7.81.0"
  },
  {
    "objectID": "meetups/Meetup10/APISlides.html#query-structure-1",
    "href": "meetups/Meetup10/APISlides.html#query-structure-1",
    "title": "API Slides",
    "section": "Query Structure",
    "text": "Query Structure\n\nUse req_url_query to add “instructions” to url\n\n\nny_alerts = request_weather_alerts |&gt; req_url_query(area = \"NY\")\n\nny_alerts |&gt; req_dry_run()\n\nGET /alerts/active HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip, br, zstd\nhost: api.weather.gov\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/7.81.0"
  },
  {
    "objectID": "meetups/Meetup10/APISlides.html#processing-output",
    "href": "meetups/Meetup10/APISlides.html#processing-output",
    "title": "API Slides",
    "section": "Processing Output",
    "text": "Processing Output\n\ntibblify to unnest json\n\n\nlibrary(tibblify)\nny_alerts |&gt; \n  req_perform() |&gt; \n  resp_body_json() |&gt; \n  tibblify()\n\n$`@context`\n$`@context`[[1]]\n[1] \"https://geojson.org/geojson-ld/geojson-context.jsonld\"\n\n$`@context`[[2]]\n$`@context`[[2]]$`@version`\n[1] \"1.1\"\n\n$`@context`[[2]]$wx\n[1] \"https://api.weather.gov/ontology#\"\n\n$`@context`[[2]]$`@vocab`\n[1] \"https://api.weather.gov/ontology#\"\n\n\n\n$type\n[1] \"FeatureCollection\"\n\n$features\n# A tibble: 1 × 4\n  id                                             type  geometry properties$`@id`\n  &lt;chr&gt;                                          &lt;chr&gt; &lt;list&gt;   &lt;chr&gt;           \n1 https://api.weather.gov/alerts/urn:oid:2.49.0… Feat… &lt;NULL&gt;   https://api.wea…\n# ℹ 30 more variables: properties$`@type` &lt;chr&gt;, $id &lt;chr&gt;, $areaDesc &lt;chr&gt;,\n#   $geocode &lt;tibble[,2]&gt;, $affectedZones &lt;list&gt;, $references &lt;list&gt;,\n#   $sent &lt;chr&gt;, $effective &lt;chr&gt;, $onset &lt;chr&gt;, $expires &lt;chr&gt;, $ends &lt;chr&gt;,\n#   $status &lt;chr&gt;, $messageType &lt;chr&gt;, $category &lt;chr&gt;, $severity &lt;chr&gt;,\n#   $certainty &lt;chr&gt;, $urgency &lt;chr&gt;, $event &lt;chr&gt;, $sender &lt;chr&gt;,\n#   $senderName &lt;chr&gt;, $headline &lt;chr&gt;, $description &lt;chr&gt;, $instruction &lt;chr&gt;,\n#   $response &lt;chr&gt;, $parameters &lt;tibble[,6]&gt;, $scope &lt;chr&gt;, $code &lt;chr&gt;, …\n\n$title\n[1] \"Current watches, warnings, and advisories for New York\"\n\n$updated\n[1] \"2025-10-27T13:00:00+00:00\""
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes",
    "href": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes",
    "title": "Meetup 7: Joins and SQL",
    "section": "Lab 3-4 Review: Axes",
    "text": "Lab 3-4 Review: Axes\n\nNever plot different units on the same axis\nairquality dataset\ncontains ozone, solar.R, Wind, Temp\n\n\nairquality |&gt; \n  pivot_longer(Ozone:Temp,\n               names_to = \"variable\",\n               values_to = \"value\") |&gt; \n               ggplot(aes(x=value,y=variable)) +\n  geom_boxplot() +\n  labs(title=\"NOT LIKE THIS\") +\n  theme_bw(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes-1",
    "href": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Lab 3-4 Review: Axes",
    "text": "Lab 3-4 Review: Axes\n\nNever plot different units on the same axis\nairquality dataset\ncontains ozone, solar.R, Wind, Temp"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes-2",
    "href": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes-2",
    "title": "Meetup 7: Joins and SQL",
    "section": "Lab 3-4 Review: Axes",
    "text": "Lab 3-4 Review: Axes\n\nInstead use facets to make each axis distinct\n\n\nairquality |&gt; \n  pivot_longer(Ozone:Temp,\n               names_to = \"variable\",\n               values_to = \"value\") |&gt; \n               ggplot(aes(x=value)) +\n  geom_boxplot() +\n  facet_wrap(~variable,ncol = 2,scales=\"free_x\") +\n  labs(title=\"LIKE THIS\") +\n  theme_bw(base_size = 16) +\n  theme( axis.text.y = element_blank(),\n           axis.ticks = element_blank())"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes-3",
    "href": "meetups/Meetup7/Meetup7.html#lab-3-4-review-axes-3",
    "title": "Meetup 7: Joins and SQL",
    "section": "Lab 3-4 Review: Axes",
    "text": "Lab 3-4 Review: Axes\n\nInstead use facets to make each axis distinct"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#investigating-outliers",
    "href": "meetups/Meetup7/Meetup7.html#investigating-outliers",
    "title": "Meetup 7: Joins and SQL",
    "section": "Investigating Outliers",
    "text": "Investigating Outliers\n\nIn the diabetes dataset there was an observation with a skin thickness of 99\nShould we discard this measurement?\nMake scatterplot of BMI vs skin thickness:"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#project-proposal",
    "href": "meetups/Meetup7/Meetup7.html#project-proposal",
    "title": "Meetup 7: Joins and SQL",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nNo lab next week\nBut project proposal due October 20th\nTarget 2 Pages\n\nIntro\nData\nAnalysis Plan\n\nIdea: Demonstrate full data science workflow\nEmphasis on everything but modeling\nFeel free to ask me about your idea as you develop it"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#project-proposal-1",
    "href": "meetups/Meetup7/Meetup7.html#project-proposal-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Project Proposal",
    "text": "Project Proposal\n\nPick a topic that either:\n\nRelates to a specific professional goal or your current job\nRelates to a something you know a lot about\nIs something you have become very interested in\n\nRead about your topic when developing your proposal!\n\nHave a short section on with references explaining state-of-the-art"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#heilmeier-framework",
    "href": "meetups/Meetup7/Meetup7.html#heilmeier-framework",
    "title": "Meetup 7: Joins and SQL",
    "section": "Heilmeier Framework",
    "text": "Heilmeier Framework"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#developing-a-research-proposal-heilmeier-questions",
    "href": "meetups/Meetup7/Meetup7.html#developing-a-research-proposal-heilmeier-questions",
    "title": "Meetup 7: Joins and SQL",
    "section": "Developing a Research Proposal: Heilmeier Questions",
    "text": "Developing a Research Proposal: Heilmeier Questions\n\n\nGeorge Heilmeier developed a series of questions he would ask of potential DARPA grant recipients\nThe answers to these questions get you to the core of your idea"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#heilmeier-questions",
    "href": "meetups/Meetup7/Meetup7.html#heilmeier-questions",
    "title": "Meetup 7: Joins and SQL",
    "section": "Heilmeier Questions",
    "text": "Heilmeier Questions\n\nWhat are you trying to do? (Without Jargon)\nHow is it done today and what are the limits?\nWhat is your approach and why will it be successful?\nWho cares about this question? (Stakeholders?)\nWhat are the risks/obstacles\nWhat will it cost? (0$)\nHow long will it take? (Deadline)\nWhat does progress/success look like?"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#this-week",
    "href": "meetups/Meetup7/Meetup7.html#this-week",
    "title": "Meetup 7: Joins and SQL",
    "section": "This Week",
    "text": "This Week\n\nTwo main topics: Joins and SQL\nSeparate video for SQL with a coding vignette"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#combining-data-from-multiple-tables",
    "href": "meetups/Meetup7/Meetup7.html#combining-data-from-multiple-tables",
    "title": "Meetup 7: Joins and SQL",
    "section": "Combining Data From Multiple Tables",
    "text": "Combining Data From Multiple Tables\n\nData usually stored in separate files/data frames\nAnalysis requires data in a single frame\nNeed tools to combine data-frames"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-1",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nflights |&gt; select(time_hour,carrier,dep_time) |&gt; head(8) |&gt; kable()\n\n\n\n\ntime_hour\ncarrier\ndep_time\n\n\n\n\n2013-01-01 05:00:00\nUA\n517\n\n\n2013-01-01 05:00:00\nUA\n533\n\n\n2013-01-01 05:00:00\nAA\n542\n\n\n2013-01-01 05:00:00\nB6\n544\n\n\n2013-01-01 06:00:00\nDL\n554\n\n\n2013-01-01 05:00:00\nUA\n554\n\n\n2013-01-01 06:00:00\nB6\n555\n\n\n2013-01-01 06:00:00\nEV\n557"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-2",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-2",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nplanes |&gt; select(tailnum,year,model) |&gt; head(8) |&gt; kable()\n\n\n\n\ntailnum\nyear\nmodel\n\n\n\n\nN10156\n2004\nEMB-145XR\n\n\nN102UW\n1998\nA320-214\n\n\nN103US\n1999\nA320-214\n\n\nN104UW\n1999\nA320-214\n\n\nN10575\n2002\nEMB-145LR\n\n\nN105UW\n1999\nA320-214\n\n\nN107US\n1999\nA320-214\n\n\nN108UW\n1999\nA320-214"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-3",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-3",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nairports |&gt; select(faa,name,lat,lon) |&gt; head(8) |&gt; kable()\n\n\n\n\nfaa\nname\nlat\nlon\n\n\n\n\n04G\nLansdowne Airport\n41.13047\n-80.61958\n\n\n06A\nMoton Field Municipal Airport\n32.46057\n-85.68003\n\n\n06C\nSchaumburg Regional\n41.98934\n-88.10124\n\n\n06N\nRandall Airport\n41.43191\n-74.39156\n\n\n09J\nJekyll Island Airport\n31.07447\n-81.42778\n\n\n0A9\nElizabethton Municipal Airport\n36.37122\n-82.17342\n\n\n0G6\nWilliams County Airport\n41.46731\n-84.50678\n\n\n0G7\nFinger Lakes Regional Airport\n42.88356\n-76.78123"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-4",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-4",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nCan combine data frames along rows that share matching values of a common variable:\n\n\n\n\nflights |&gt; \n  select(dest,origin) |&gt; \n  head(6) |&gt;\n  kable()\n\n\n\n\ndest\norigin\n\n\n\n\nIAH\nEWR\n\n\nIAH\nLGA\n\n\nMIA\nJFK\n\n\nBQN\nJFK\n\n\nATL\nLGA\n\n\nORD\nEWR\n\n\n\n\n\n\n\nairports |&gt; \n  select(faa) |&gt; \n  head(8) |&gt; \n  kable()\n\n\n\n\nfaa\n\n\n\n\n04G\n\n\n06A\n\n\n06C\n\n\n06N\n\n\n09J\n\n\n0A9\n\n\n0G6\n\n\n0G7\n\n\n\n\n\n\n\nColumns which connect different data frames are Keys"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-5",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-5",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nCan combine data frames along rows that share matching values of a common variable:\n\n\n\n\nflights |&gt; \n  select(dest,origin) |&gt; \n  head(6) |&gt;\n  kable()\n\n\n\n\ndest\norigin\n\n\n\n\nIAH\nEWR\n\n\nIAH\nLGA\n\n\nMIA\nJFK\n\n\nBQN\nJFK\n\n\nATL\nLGA\n\n\nORD\nEWR\n\n\n\n\n\n\n\nairports |&gt; \n  select(faa) |&gt; \n  head(8) |&gt; \n  kable()\n\n\n\n\nfaa\n\n\n\n\n04G\n\n\n06A\n\n\n06C\n\n\n06N\n\n\n09J\n\n\n0A9\n\n\n0G6\n\n\n0G7\n\n\n\n\n\n\n\nColumns which connect different data frames are Keys"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#example-nycflights13-6",
    "href": "meetups/Meetup7/Meetup7.html#example-nycflights13-6",
    "title": "Meetup 7: Joins and SQL",
    "section": "Example: nycflights13",
    "text": "Example: nycflights13\n\nCan combine data frames along rows that share matching values of a common variable:\n\n\n\n\nflights |&gt; \n  select(tailnum) |&gt; \n  head(6) |&gt;\n  kable()\n\n\n\n\ntailnum\n\n\n\n\nN14228\n\n\nN24211\n\n\nN619AA\n\n\nN804JB\n\n\nN668DN\n\n\nN39463\n\n\n\n\n\n\n\nplanes |&gt; \n  select(tailnum) |&gt; \n  head(8) |&gt; \n  kable()\n\n\n\n\ntailnum\n\n\n\n\nN10156\n\n\nN102UW\n\n\nN103US\n\n\nN104UW\n\n\nN10575\n\n\nN105UW\n\n\nN107US\n\n\nN108UW\n\n\n\n\n\n\n\nColumns that are common “Keys” used to link data frames"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#primary-keys",
    "href": "meetups/Meetup7/Meetup7.html#primary-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "Primary Keys",
    "text": "Primary Keys\nA Primary Key is a column/group of columns whose values uniquely determine each row of the data frame.\n\nFor planes it is tailnum\n\n\nplanes |&gt; select(tailnum, year, type, model) |&gt; head(5) |&gt; kable()\n\n\n\n\ntailnum\nyear\ntype\nmodel\n\n\n\n\nN10156\n2004\nFixed wing multi engine\nEMB-145XR\n\n\nN102UW\n1998\nFixed wing multi engine\nA320-214\n\n\nN103US\n1999\nFixed wing multi engine\nA320-214\n\n\nN104UW\n1999\nFixed wing multi engine\nA320-214\n\n\nN10575\n2002\nFixed wing multi engine\nEMB-145LR"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#definition",
    "href": "meetups/Meetup7/Meetup7.html#definition",
    "title": "Meetup 7: Joins and SQL",
    "section": "Definition",
    "text": "Definition\nA Primary Key is a column/group of columns whose values uniquely determine each row of the data frame.\n\nFor planes it is tailnum\n\n\nplanes |&gt; \n  count(tailnum) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: tailnum &lt;chr&gt;, n &lt;int&gt;\n\n\n\nThis count command shows that each row has a unique value of tailnum proving that it is a primary key"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#compound-keys",
    "href": "meetups/Meetup7/Meetup7.html#compound-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "Compound Keys",
    "text": "Compound Keys\n\nSometimes no single variable uniquely identifies rows.\nMultiple variables can combine to be a primary key\nFor weather it requires origin and time-hour:\n\n\nweather |&gt; count(time_hour) |&gt; filter(n&gt;1) |&gt; nrow()\n\n[1] 8706\n\nweather |&gt; \n  count(origin,time_hour) |&gt; \n  filter(n &gt; 1)\n\n# A tibble: 0 × 3\n# ℹ 3 variables: origin &lt;chr&gt;, time_hour &lt;dttm&gt;, n &lt;int&gt;"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#foreign-keys",
    "href": "meetups/Meetup7/Meetup7.html#foreign-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "Foreign Keys",
    "text": "Foreign Keys\nA Foreign Key is a variable/group of variables that is a primary key for another data frame.\n\nflights |&gt; select(flight,time_hour,tailnum) |&gt; head(4) |&gt; kable()\n\n\n\n\nflight\ntime_hour\ntailnum\n\n\n\n\n1545\n2013-01-01 05:00:00\nN14228\n\n\n1714\n2013-01-01 05:00:00\nN24211\n\n\n1141\n2013-01-01 05:00:00\nN619AA\n\n\n725\n2013-01-01 05:00:00\nN804JB\n\n\n\n\n\n\ntailnum is a primary key for planes\nForeign key allows for rows to be matched between data frames"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#joins",
    "href": "meetups/Meetup7/Meetup7.html#joins",
    "title": "Meetup 7: Joins and SQL",
    "section": "Joins",
    "text": "Joins\n\nJoins are functions that combine two data frames\n\nsomething_join(x,y,by = join_by(...))\n\nHere something is either:\n\nleft_join\nright_join\nfull_join\ninner_join\nsemi_join\nanti_join"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#join-types",
    "href": "meetups/Meetup7/Meetup7.html#join-types",
    "title": "Meetup 7: Joins and SQL",
    "section": "Join Types",
    "text": "Join Types\nTwo main “types” of joins:\n\nMutating Joins:\n\nCombine data between data frames\nleft_join, right_join, full_join, inner_join\n\nFiltering Joins:\n\nFilter rows based on matches\nsemi_join, anti_join"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#mutating-joins",
    "href": "meetups/Meetup7/Meetup7.html#mutating-joins",
    "title": "Meetup 7: Joins and SQL",
    "section": "Mutating Joins",
    "text": "Mutating Joins\n\nleft_join: Keeps all rows of x, matches/adds from y\nright_join: reversed left_join\nfull_join: keeps all rows of both\ninner_join: only rows occurring in both\n\n\nR4DS 19.8"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#left-join",
    "href": "meetups/Meetup7/Meetup7.html#left-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Left Join",
    "text": "Left Join\n\nStarting Point:\n\n\nR4DS 19.2"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#left-join-1",
    "href": "meetups/Meetup7/Meetup7.html#left-join-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Left Join",
    "text": "Left Join\n\nR4DS 19.2\nleft_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2\n\n\n3\nx3\nNA"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#right-join",
    "href": "meetups/Meetup7/Meetup7.html#right-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Right Join",
    "text": "Right Join\n\nR4DS 19.2\nright_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2\n\n\n4\nNA\ny3"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#inner-join",
    "href": "meetups/Meetup7/Meetup7.html#inner-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Inner Join",
    "text": "Inner Join\n\nR4DS 19.2\ninner_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#full-join",
    "href": "meetups/Meetup7/Meetup7.html#full-join",
    "title": "Meetup 7: Joins and SQL",
    "section": "Full Join",
    "text": "Full Join\n\n\n\n\n\nR4DS 19.2\n\n\n\n\nfull_join(x,y) |&gt; kable()\n\n\n\n\nkey\nval_x\nval_y\n\n\n\n\n1\nx1\ny1\n\n\n2\nx2\ny2\n\n\n3\nx3\nNA\n\n\n4\nNA\ny3"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#what-about-keys",
    "href": "meetups/Meetup7/Meetup7.html#what-about-keys",
    "title": "Meetup 7: Joins and SQL",
    "section": "What about Keys?",
    "text": "What about Keys?\n\njoin functions automatically try to find a foreign key\nYou may specify a key using join_by and a formula:\n\n\nflights |&gt; left_join(airports,join_by(dest == faa)) |&gt; \n  select(year:dep_delay) |&gt; head(5)\n\n# A tibble: 5 × 6\n   year month   day dep_time sched_dep_time dep_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n1  2013     1     1      517            515         2\n2  2013     1     1      533            529         4\n3  2013     1     1      542            540         2\n4  2013     1     1      544            545        -1\n5  2013     1     1      554            600        -6\n\n\n\nPotential for multiple matches and duplication when there is no foreign key"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#filtering-joins",
    "href": "meetups/Meetup7/Meetup7.html#filtering-joins",
    "title": "Meetup 7: Joins and SQL",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nsemi_join(x,y) removes rows of x that don’t have matches in y\nUse semi_join to restrict airports to the destination airports\n\n\nairports |&gt; \n  semi_join(flights, join_by(faa == origin))\n\n# A tibble: 3 × 8\n  faa   name                  lat   lon   alt    tz dst   tzone           \n  &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;           \n1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#filtering-joins-1",
    "href": "meetups/Meetup7/Meetup7.html#filtering-joins-1",
    "title": "Meetup 7: Joins and SQL",
    "section": "Filtering Joins",
    "text": "Filtering Joins\n\nanti_join(x,y) removes rows of x that have matches in y\nUse anti_join to find implicit missing values\nFlights where weather info lacking:\n\n\nflights |&gt; \n  anti_join(weather) |&gt; \n  head(5) |&gt; \n  select(year:dep_delay) |&gt;\n  print()\n\n# A tibble: 5 × 6\n   year month   day dep_time sched_dep_time dep_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n1  2013     1     1     1153           1200        -7\n2  2013     1     1     1154           1200        -6\n3  2013     1     1     1155           1200        -5\n4  2013     1     1     1155           1200        -5\n5  2013     1     1     1157           1200        -3"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#data-science-in-context-presentations",
    "href": "meetups/Meetup7/Meetup7.html#data-science-in-context-presentations",
    "title": "Meetup 7: Joins and SQL",
    "section": "Data Science in Context Presentations",
    "text": "Data Science in Context Presentations"
  },
  {
    "objectID": "meetups/Meetup7/Meetup7.html#meetup-reflectionone-minute-paper",
    "href": "meetups/Meetup7/Meetup7.html#meetup-reflectionone-minute-paper",
    "title": "Meetup 7: Joins and SQL",
    "section": "Meetup Reflection/One Minute Paper",
    "text": "Meetup Reflection/One Minute Paper\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#about-me",
    "href": "meetups/IntroSlides/IntroSlides.html#about-me",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "About Me",
    "text": "About Me\n\nI’ll start by introducing myself.\nI have a Bachelor’s of Science and a PhD in physics and over 15 years experience applying Statistics, Machine Learning, and Computational Science to a variety of problems in plasma physics, ocean science, and quantitative social science\nBefore coming to CUNY, I taught math and performed research at the math department at NYU.\n\n\n\nStarted as a physicist\n\nB.Sci. (Caltech), Ph.D. (UT Austin)\nResearcher/Instructor at NYU"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#about-me-1",
    "href": "meetups/IntroSlides/IntroSlides.html#about-me-1",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "About Me",
    "text": "About Me\n\nAfter leaving NYU, I took a position as a research scientist at Princeton University in the Department of Ecology and Evolutionary Biology.\nOne of my main interests when I was there was finding ways to integrate unusual or complex data sets with genomic data into ocean science research, typically using Bayesian Statistics and Machine Learning.\nThis was quite a big change from the research I did before, and a very rewarding experience.\n\n\n\nSwitched to Quant. Ecology\n\nDecade at Princeton\nApply Bayesian Stats/ML to Ocean Science and Complex Systems"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#about-me-2",
    "href": "meetups/IntroSlides/IntroSlides.html#about-me-2",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "About Me",
    "text": "About Me\n\nI’ve lived in New York City for over a decade and have a lot of family from here, but I’ve also lived and worked in many different parts of the US.\n\n\n\n\nI’ve lived all over the US\nAlways connected to NYC\nLived here for 10+ years"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#about-me-3",
    "href": "meetups/IntroSlides/IntroSlides.html#about-me-3",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "About Me",
    "text": "About Me\n\nWhen I’m not working I enjoy cycling and exploring New York City.\n\n\n\n\nI like cycling for exercise/transport and to explore NYC"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#switching-areas",
    "href": "meetups/IntroSlides/IntroSlides.html#switching-areas",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Switching Areas",
    "text": "Switching Areas\n\nBefore discussing the details of the course I want to make two points about teaching and learning in a program like this.\nFirstly. I know many of you are in this program to move into a different type of work than what you currently do. I respect that greatly. I know it can be intimidating because I have had to do it before, when I switched from physics to ecology. I can relate to what you are experiencing.\n\n\n\n\n\n\nIt takes courage to change careers\nI have lots of experience moving fields and can relate and help make it easier"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#learning-quantitative-skills",
    "href": "meetups/IntroSlides/IntroSlides.html#learning-quantitative-skills",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Learning Quantitative Skills",
    "text": "Learning Quantitative Skills\n\nSecondly, I know that Mathematics and computer science are intimidating disciplines for many people.\nSociety has told us that the experts in these subjects have to be incredibly brilliant, or even geniuses.\nThat it is something you are born with.\nIn my experience this isn’t true at all.\nThere is no such thing as a math person, everyone can quantitative skills with hard work and the right environment. But you need to believe that you aren’t bad at math.\nI’ll try my best to help.\n\n\n\n\n\n\nApplies to programming too\nNo ‘genius’ requierd\nSkills you get better at with practice"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#course-overview",
    "href": "meetups/IntroSlides/IntroSlides.html#course-overview",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Course Overview",
    "text": "Course Overview\n\nThe main goal of this course is to provide you with a strong foundation in the non-modeling side of data science. You will learn how to take a messy data set and convert it into a format amenable to analysis. This includes rooting out problems in the data, visualizing the data and informally exploring patterns to generate hypotheses, storing data so it is easy to work with, acquiring data automatically from the web, working with complex data types like graphs, and using distributed computing techniques to work with very large datasets.\nWe will also talk about some data ethics issues, practice presentation skills, and learn how to use some collaborative software tools, and how to use LLMs\nAlthough people often think of models when they think about data science, the preparatory work you will learn to do in this course probably comprises 80% of the work that a data scientist does. By mastering the tools in 607, you will become much stronger as a data scientist, increasing the scope of data you can work with, decreasing the change of making mistakes, and opening the potential to link with other lucrative career paths like data engineering.\n\n\nEverything in Data Science other than modeling\nData tidying, wrangling, cleaning, visualizations, exploratory data analysis, collaborative programming, LLM basics\nWorking with a variety of data types including strings and text, graphs, hierarchical structures, and relational databases\nIntroduce more advanced programming concepts, tools for large data sets and distributed computing.\nLots of R, some SQL, some data ethics"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#course-design",
    "href": "meetups/IntroSlides/IntroSlides.html#course-design",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Course Design",
    "text": "Course Design\n\nThis course is centered around a weekly meetup. I strongly encourage you to attend each week, but if life gets in the way videos will be posted later. After watching each meetup you will be asked to complete a short reflection on what you learned.\nThe meetups will be a mixture of lecture, live coding, and discussion.\nWe will have 10 weekly lab assignments that are an important part of your grade. You are encouraged to discuss these with your classmates on slack but what you turn in should be your own work. There will be two “collaborative” programming assignments, one short data science in context presentation, and a final project where you pick an area of interest and assemble and study data related to it.\n\n\nWeekly meetup (Monday 6:45PM) - combo of lecture, live coding, discussion.\n10 Weekly lab assignments and two collaborative programming assignments\nWeekly meetup reflection and one data science in context presentation\nFinal project and presentation\nUse Slack page for discussion, arrange meetings to get help"
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#brightspace-to-turn-in-hw",
    "href": "meetups/IntroSlides/IntroSlides.html#brightspace-to-turn-in-hw",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Brightspace: To Turn in HW",
    "text": "Brightspace: To Turn in HW\n\nThis course uses brightspace just to turn in assignments and manage the grade book."
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff",
    "href": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Course Website: Where You Find Stuff",
    "text": "Course Website: Where You Find Stuff\n\nMost of the course material will be on a dedicated course web page, where you will be able to find the course policies, the homework assignments, and links to the recorded meetups, slides, and other course materials."
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff-1",
    "href": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff-1",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Course Website: Where You Find Stuff",
    "text": "Course Website: Where You Find Stuff\n\nMost of the course material will be on a dedicated course web page, where you will be able to find the course policies, the homework assignments, and links to the recorded meetups, slides, and other course materials."
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff-2",
    "href": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff-2",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Course Website: Where You Find Stuff",
    "text": "Course Website: Where You Find Stuff\n\nMost of the course material will be on a dedicated course web page, where you will be able to find the course policies, the homework assignments, and links to the recorded meetups, slides, and other course materials."
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff-3",
    "href": "meetups/IntroSlides/IntroSlides.html#course-website-where-you-find-stuff-3",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Course Website: Where You Find Stuff",
    "text": "Course Website: Where You Find Stuff\n\nMost of the course material will be on a dedicated course web page, where you will be able to find the course policies, the homework assignments, and links to the recorded meetups, slides, and other course materials."
  },
  {
    "objectID": "meetups/IntroSlides/IntroSlides.html#final-notes",
    "href": "meetups/IntroSlides/IntroSlides.html#final-notes",
    "title": "Welcome to DATA 607: Data Acquisition and Management",
    "section": "Final Notes",
    "text": "Final Notes\n\nLastly, although online courses may be more convenient than face to face courses, that doesn’t mean they are easy, and it is possible to fall behind. Try to stay on top of everything, and get in touch with me sooner rather than later if you are struggling.\nIf you have any questions about the syllabus or course information, or need to schedule a meeting don’t hesitate to send me an e-mail or a message on slack!\nI am looking forward to the semester and I hope that you are too, thank you!\n\n\nI’m here to help you. Contact me sooner rather than later\nThank you!\nEmail: georgehagstrom@cuny.edu"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#map-family-in-purrr",
    "href": "meetups/Meetup9/MapVignette.html#map-family-in-purrr",
    "title": "Purrr and map",
    "section": "map family in purrr",
    "text": "map family in purrr\n\nmap(vec,f) = [f(vec[1]),f(vec[2]),....] \n\n\nmap(1:3,exp)\n\n[[1]]\n[1] 2.718282\n\n[[2]]\n[1] 7.389056\n\n[[3]]\n[1] 20.08554"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#map_",
    "href": "meetups/Meetup9/MapVignette.html#map_",
    "title": "Purrr and map",
    "section": "map_*",
    "text": "map_*\n\nmap_dbl, map_chr, map_lgl, map_int: assume data type and return vectors\nOperate on data frames \nImagine the data frame has been rotated so the “rows” correspond to columns"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#map_-1",
    "href": "meetups/Meetup9/MapVignette.html#map_-1",
    "title": "Purrr and map",
    "section": "map_*",
    "text": "map_*\n\nmtcars |&gt; head(8)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\nmtcars |&gt; map_dbl(\\(x) length(unique(x)))\n\n mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n  25    3   27   22   22   29   30    2    2    3    6 \n\n\n\nYour function must return a single element of the same type as the map suffix"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#passing-arguments",
    "href": "meetups/Meetup9/MapVignette.html#passing-arguments",
    "title": "Purrr and map",
    "section": "Passing Arguments",
    "text": "Passing Arguments\n\nAdd arguments after the function\n\n\nx &lt;- list(1:5, c(1:10, NA))\nmap_dbl(x, mean,na.rm = TRUE)\n\n[1] 3.0 5.5\n\n\n\nAdv R"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#passing-arguments-1",
    "href": "meetups/Meetup9/MapVignette.html#passing-arguments-1",
    "title": "Purrr and map",
    "section": "Passing Arguments",
    "text": "Passing Arguments\n\nAdd arguments after the function\n\n\nx &lt;- list(1:5, c(1:10, NA))\nmap_dbl(x, mean,na.rm = TRUE)\n\n[1] 3.0 5.5\n\n\n\nNot vectorized over later args:"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#map2-and-others",
    "href": "meetups/Meetup9/MapVignette.html#map2-and-others",
    "title": "Purrr and map",
    "section": "map2 and others",
    "text": "map2 and others\n\nAdv R\nEntire family of other similar types of functions\npmap, imap, modify2, walk, etc"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#where-are-the-for-loops",
    "href": "meetups/Meetup9/MapVignette.html#where-are-the-for-loops",
    "title": "Purrr and map",
    "section": "Where are the for loops?",
    "text": "Where are the for loops?\n\nfor and while loops exist in R\nThey are discouraged in favor of tools like map\nOnly use them when you must (such as when you need side effects)\n\n\nfor (element in vector){\n  func(element) # Run some code that uses element\n}"
  },
  {
    "objectID": "meetups/Meetup9/MapVignette.html#while-loop-in-action",
    "href": "meetups/Meetup9/MapVignette.html#while-loop-in-action",
    "title": "Purrr and map",
    "section": "while loop in action",
    "text": "while loop in action\n\ncounter = 0\nwhile (rnorm(1) &lt; 2) {\n  counter = counter + 1\n  \n}\n\ncounter\n\n[1] 47"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#pre-thanksgiving-update",
    "href": "meetups/Meetup13/Meetup13.html#pre-thanksgiving-update",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Pre-Thanksgiving Update",
    "text": "Pre-Thanksgiving Update\n\nEntering Course Home Stretch\nAssignments Remaining:\n\ntidyverse extend and graph theory (due this Sunday)\nFinal Lab due December 4th\nFinal Real Meetup December 1st\nFinal Project: Record Video Presentation and upload to Brightspace"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#computer-architecture-basics",
    "href": "meetups/Meetup13/Meetup13.html#computer-architecture-basics",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Computer Architecture Basics",
    "text": "Computer Architecture Basics\n\n\n\n\n\nHierarchical Memory\nCache: Small and Fast\nRAM: Medium Size and Speed\nDisk: Huge and Slow"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#memory-limited-tasks",
    "href": "meetups/Meetup13/Meetup13.html#memory-limited-tasks",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Memory Limited Tasks",
    "text": "Memory Limited Tasks\n\nData processing and manipulation tasks are inherently memory/memory speed limited\nReading and writing files are also memory limited\nSome data analysis tasks are (but not the topic here)\nStart caring when your dataset ~1GB (or if things run slowly)"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#dplyr-tradeoffs",
    "href": "meetups/Meetup13/Meetup13.html#dplyr-tradeoffs",
    "title": "Meetup 13: Big Data and data.table",
    "section": "dplyr tradeoffs",
    "text": "dplyr tradeoffs\n“We optimize dplyr for expressiveness on medium data; feel free to use data.table for raw speed on bigger data”\n\nhttps://h2oai.github.io/db-benchmark/"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#data.table-summary",
    "href": "meetups/Meetup13/Meetup13.html#data.table-summary",
    "title": "Meetup 13: Big Data and data.table",
    "section": "data.table summary",
    "text": "data.table summary\n\ndata.table enhances the base R data.frame\nDesigned for high speed with large datasets\nStyle is polar opposite of the tidyverse:\n\nFew functions\nVery concise code\nVery stable syntax\n\ndtplyr and tidytable are two “tidy style” backends to data.table"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#data.table-top-features",
    "href": "meetups/Meetup13/Meetup13.html#data.table-top-features",
    "title": "Meetup 13: Big Data and data.table",
    "section": "data.table top features",
    "text": "data.table top features\n\ndata.table queries executed together\ndata.table := operator supports passing by reference\ndata.table supports reindexing which allows for extremely fast searches and subsetting\n\n\nPassing by reference and indexing are subtler and more advanced computing topics, won’t get to them here and will instead introudce them in a coding vignette\nAlso considering making a vignette on basic code benchmarking"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#creating-a-data-table",
    "href": "meetups/Meetup13/Meetup13.html#creating-a-data-table",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Creating a Data Table:",
    "text": "Creating a Data Table:\n\nusing data.table():\n\n\nlibrary(data.table)\n\nDT = data.table(\n  ID = c(\"b\",\"b\",\"b\",\"a\",\"a\",\"c\"),\n  a = 1:6,\n  b = 7:12,\n  c = 13:18\n)\nDT\n\n       ID     a     b     c\n   &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1:      b     1     7    13\n2:      b     2     8    14\n3:      b     3     9    15\n4:      a     4    10    16\n5:      a     5    11    17\n6:      c     6    12    18"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#creating-a-data-table-1",
    "href": "meetups/Meetup13/Meetup13.html#creating-a-data-table-1",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Creating a Data Table:",
    "text": "Creating a Data Table:\n\nusing data.table():\n\n\nDT = data.table(\n  ID = c(\"b\",\"b\",\"b\",\"a\",\"a\",\"c\"),\n  a = 1:6,\n  b = 7:12,\n  c = 13:18\n)\nDT\n\n       ID     a     b     c\n   &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1:      b     1     7    13\n2:      b     2     8    14\n3:      b     3     9    15\n4:      a     4    10    16\n5:      a     5    11    17\n6:      c     6    12    18"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#creating-a-data-table-2",
    "href": "meetups/Meetup13/Meetup13.html#creating-a-data-table-2",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Creating a Data Table",
    "text": "Creating a Data Table\n\nUsing fread\n\n\nflights = fread(\"/home/georgehagstrom/work/Teaching/DATA607/website/meetups/Meetup13/flights14.csv\")\n\nflights\n\n         year month   day dep_delay arr_delay carrier origin   dest air_time\n        &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n     1:  2014     1     1        14        13      AA    JFK    LAX      359\n     2:  2014     1     1        -3        13      AA    JFK    LAX      363\n     3:  2014     1     1         2         9      AA    JFK    LAX      351\n     4:  2014     1     1        -8       -26      AA    LGA    PBI      157\n     5:  2014     1     1         2         1      AA    JFK    LAX      350\n    ---                                                                     \n253312:  2014    10    31         1       -30      UA    LGA    IAH      201\n253313:  2014    10    31        -5       -14      UA    EWR    IAH      189\n253314:  2014    10    31        -8        16      MQ    LGA    RDU       83\n253315:  2014    10    31        -4        15      MQ    LGA    DTW       75\n253316:  2014    10    31        -5         1      MQ    LGA    SDF      110\n        distance  hour\n           &lt;int&gt; &lt;int&gt;\n     1:     2475     9\n     2:     2475    11\n     3:     2475    19\n     4:     1035     7\n     5:     2475    13\n    ---               \n253312:     1416    14\n253313:     1400     8\n253314:      431    11\n253315:      502    11\n253316:      659     8"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#creating-a-data-table-3",
    "href": "meetups/Meetup13/Meetup13.html#creating-a-data-table-3",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Creating a Data Table",
    "text": "Creating a Data Table\n\nCoercing a data frame, tibble, list, matrix, database etc:\n\n\ndata_cars = mtcars |&gt; data.table()\ndata_cars\n\n      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n    &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:  21.0     6 160.0   110  3.90 2.620 16.46     0     1     4     4\n 2:  21.0     6 160.0   110  3.90 2.875 17.02     0     1     4     4\n 3:  22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1\n 4:  21.4     6 258.0   110  3.08 3.215 19.44     1     0     3     1\n 5:  18.7     8 360.0   175  3.15 3.440 17.02     0     0     3     2\n 6:  18.1     6 225.0   105  2.76 3.460 20.22     1     0     3     1\n 7:  14.3     8 360.0   245  3.21 3.570 15.84     0     0     3     4\n 8:  24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2\n 9:  22.8     4 140.8    95  3.92 3.150 22.90     1     0     4     2\n10:  19.2     6 167.6   123  3.92 3.440 18.30     1     0     4     4\n11:  17.8     6 167.6   123  3.92 3.440 18.90     1     0     4     4\n12:  16.4     8 275.8   180  3.07 4.070 17.40     0     0     3     3\n13:  17.3     8 275.8   180  3.07 3.730 17.60     0     0     3     3\n14:  15.2     8 275.8   180  3.07 3.780 18.00     0     0     3     3\n15:  10.4     8 472.0   205  2.93 5.250 17.98     0     0     3     4\n16:  10.4     8 460.0   215  3.00 5.424 17.82     0     0     3     4\n17:  14.7     8 440.0   230  3.23 5.345 17.42     0     0     3     4\n18:  32.4     4  78.7    66  4.08 2.200 19.47     1     1     4     1\n19:  30.4     4  75.7    52  4.93 1.615 18.52     1     1     4     2\n20:  33.9     4  71.1    65  4.22 1.835 19.90     1     1     4     1\n21:  21.5     4 120.1    97  3.70 2.465 20.01     1     0     3     1\n22:  15.5     8 318.0   150  2.76 3.520 16.87     0     0     3     2\n23:  15.2     8 304.0   150  3.15 3.435 17.30     0     0     3     2\n24:  13.3     8 350.0   245  3.73 3.840 15.41     0     0     3     4\n25:  19.2     8 400.0   175  3.08 3.845 17.05     0     0     3     2\n26:  27.3     4  79.0    66  4.08 1.935 18.90     1     1     4     1\n27:  26.0     4 120.3    91  4.43 2.140 16.70     0     1     5     2\n28:  30.4     4  95.1   113  3.77 1.513 16.90     1     1     5     2\n29:  15.8     8 351.0   264  4.22 3.170 14.50     0     1     5     4\n30:  19.7     6 145.0   175  3.62 2.770 15.50     0     1     5     6\n31:  15.0     8 301.0   335  3.54 3.570 14.60     0     1     5     8\n32:  21.4     4 121.0   109  4.11 2.780 18.60     1     1     4     2\n      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#basic-syntax",
    "href": "meetups/Meetup13/Meetup13.html#basic-syntax",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Basic Syntax",
    "text": "Basic Syntax\n\nQuery data tables using square brackets:\n\n\nDT[ i   ,  j   ,   by   ]\n\n\nCode replaces each of i, j, and by\ni: code for logical subsetting\nj: code for transformation and aggregation\nby: code for grouping"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#subset-example",
    "href": "meetups/Meetup13/Meetup13.html#subset-example",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Subset Example",
    "text": "Subset Example\n\ni: subset by index/key or logical conditions\n\n\n flights[1:20,]\n\n     year month   day dep_delay arr_delay carrier origin   dest air_time\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n 1:  2014     1     1        14        13      AA    JFK    LAX      359\n 2:  2014     1     1        -3        13      AA    JFK    LAX      363\n 3:  2014     1     1         2         9      AA    JFK    LAX      351\n 4:  2014     1     1        -8       -26      AA    LGA    PBI      157\n 5:  2014     1     1         2         1      AA    JFK    LAX      350\n 6:  2014     1     1         4         0      AA    EWR    LAX      339\n 7:  2014     1     1        -2       -18      AA    JFK    LAX      338\n 8:  2014     1     1        -3       -14      AA    JFK    LAX      356\n 9:  2014     1     1        -1       -17      AA    JFK    MIA      161\n10:  2014     1     1        -2       -14      AA    JFK    SEA      349\n11:  2014     1     1        -5       -17      AA    EWR    MIA      161\n12:  2014     1     1         7        -5      AA    JFK    SFO      365\n13:  2014     1     1         3         1      AA    JFK    BOS       39\n14:  2014     1     1       142       133      AA    JFK    LAX      345\n15:  2014     1     1        -5       -26      AA    JFK    BOS       35\n16:  2014     1     1        18        69      AA    JFK    ORD      155\n17:  2014     1     1        25        36      AA    JFK    IAH      234\n18:  2014     1     1        -1         1      AA    JFK    AUS      232\n19:  2014     1     1       191       185      AA    EWR    DFW      214\n20:  2014     1     1        -7        -6      AA    LGA    ORD      142\n     year month   day dep_delay arr_delay carrier origin   dest air_time\n    distance  hour\n       &lt;int&gt; &lt;int&gt;\n 1:     2475     9\n 2:     2475    11\n 3:     2475    19\n 4:     1035     7\n 5:     2475    13\n 6:     2454    18\n 7:     2475    21\n 8:     2475    15\n 9:     1089    15\n10:     2422    18\n11:     1085    16\n12:     2586    17\n13:      187    12\n14:     2475    19\n15:      187    17\n16:      740    17\n17:     1417    16\n18:     1521    17\n19:     1372    16\n20:      733     5\n    distance  hour"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#subset-example-1",
    "href": "meetups/Meetup13/Meetup13.html#subset-example-1",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Subset Example",
    "text": "Subset Example\n\ni: subset by index/key or logical conditions\n\n\n flights[origin == \"JFK\" & dest ==\"ORD\" & arr_delay &gt;60, ] |&gt; head(n=20)\n\n     year month   day dep_delay arr_delay carrier origin   dest air_time\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n 1:  2014     1     1        18        69      AA    JFK    ORD      155\n 2:  2014     1     2        82       113      AA    JFK    ORD      120\n 3:  2014     1     2        41       107      B6    JFK    ORD      136\n 4:  2014     1     3       150       173      B6    JFK    ORD      122\n 5:  2014     1     3       351       378      B6    JFK    ORD      123\n 6:  2014     1     4       121       147      B6    JFK    ORD      126\n 7:  2014     1     5        99        70      AA    JFK    ORD      119\n 8:  2014     1     5       269       248      B6    JFK    ORD      122\n 9:  2014     1     6        96        74      AA    JFK    ORD      124\n10:  2014     1     7        65        63      B6    JFK    ORD      132\n11:  2014     1    10       113       105      AA    JFK    ORD      139\n12:  2014     1    11       354       326      AA    JFK    ORD      111\n13:  2014     1    11       241       230      B6    JFK    ORD      113\n14:  2014     1    15       116        88      B6    JFK    ORD      115\n15:  2014     1    20        53        61      B6    JFK    ORD      132\n16:  2014     1    24        89        87      AA    JFK    ORD      143\n17:  2014     1    25        74        69      AA    JFK    ORD      120\n18:  2014     1    26        58        62      B6    JFK    ORD      137\n19:  2014     2     3       300       326      AA    JFK    ORD      146\n20:  2014     2     9        -9        74      B6    JFK    ORD      130\n     year month   day dep_delay arr_delay carrier origin   dest air_time\n    distance  hour\n       &lt;int&gt; &lt;int&gt;\n 1:      740    17\n 2:      740    18\n 3:      740     7\n 4:      740    23\n 5:      740    12\n 6:      740     9\n 7:      740    18\n 8:      740     1\n 9:      740    18\n10:      740    21\n11:      740    19\n12:      740    23\n13:      740     0\n14:      740    22\n15:      740     7\n16:      740    18\n17:      740    18\n18:      740    21\n19:      740    22\n20:      740    19\n    distance  hour"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#subset-example-2",
    "href": "meetups/Meetup13/Meetup13.html#subset-example-2",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Subset Example",
    "text": "Subset Example\n\ni: subset by index/key or logical conditions\n\n\n flights[origin == \"JFK\" & dest ==\"ORD\" & arr_delay &gt;60, ] |&gt; head(n=20)\n\n     year month   day dep_delay arr_delay carrier origin   dest air_time\n    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n 1:  2014     1     1        18        69      AA    JFK    ORD      155\n 2:  2014     1     2        82       113      AA    JFK    ORD      120\n 3:  2014     1     2        41       107      B6    JFK    ORD      136\n 4:  2014     1     3       150       173      B6    JFK    ORD      122\n 5:  2014     1     3       351       378      B6    JFK    ORD      123\n 6:  2014     1     4       121       147      B6    JFK    ORD      126\n 7:  2014     1     5        99        70      AA    JFK    ORD      119\n 8:  2014     1     5       269       248      B6    JFK    ORD      122\n 9:  2014     1     6        96        74      AA    JFK    ORD      124\n10:  2014     1     7        65        63      B6    JFK    ORD      132\n11:  2014     1    10       113       105      AA    JFK    ORD      139\n12:  2014     1    11       354       326      AA    JFK    ORD      111\n13:  2014     1    11       241       230      B6    JFK    ORD      113\n14:  2014     1    15       116        88      B6    JFK    ORD      115\n15:  2014     1    20        53        61      B6    JFK    ORD      132\n16:  2014     1    24        89        87      AA    JFK    ORD      143\n17:  2014     1    25        74        69      AA    JFK    ORD      120\n18:  2014     1    26        58        62      B6    JFK    ORD      137\n19:  2014     2     3       300       326      AA    JFK    ORD      146\n20:  2014     2     9        -9        74      B6    JFK    ORD      130\n     year month   day dep_delay arr_delay carrier origin   dest air_time\n    distance  hour\n       &lt;int&gt; &lt;int&gt;\n 1:      740    17\n 2:      740    18\n 3:      740     7\n 4:      740    23\n 5:      740    12\n 6:      740     9\n 7:      740    18\n 8:      740     1\n 9:      740    18\n10:      740    21\n11:      740    19\n12:      740    23\n13:      740     0\n14:      740    22\n15:      740     7\n16:      740    18\n17:      740    18\n18:      740    21\n19:      740    22\n20:      740    19\n    distance  hour"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#sort-example",
    "href": "meetups/Meetup13/Meetup13.html#sort-example",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Sort Example",
    "text": "Sort Example\n\nCan also rearrange data.table using the order() function\nAutomatically uses data.table fast sort\n\n\nflights[order(origin, -dest)] |&gt; \n  head()\n\n    year month   day dep_delay arr_delay carrier origin   dest air_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n1:  2014     1     5         6        49      EV    EWR    XNA      195\n2:  2014     1     6         7        13      EV    EWR    XNA      190\n3:  2014     1     7        -6       -13      EV    EWR    XNA      179\n4:  2014     1     8        -7       -12      EV    EWR    XNA      184\n5:  2014     1     9        16         7      EV    EWR    XNA      181\n6:  2014     1    13        66        66      EV    EWR    XNA      188\n   distance  hour\n      &lt;int&gt; &lt;int&gt;\n1:     1131     8\n2:     1131     8\n3:     1131     8\n4:     1131     8\n5:     1131     8\n6:     1131     9"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#selecting-columns",
    "href": "meetups/Meetup13/Meetup13.html#selecting-columns",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Selecting Columns",
    "text": "Selecting Columns\n\nCan choose a subset of columns in the second argument of []\n\n\nflights[ , air_time ] |&gt; \n  head()\n\n[1] 359 363 351 157 350 339\n\n\n\nNot “tidy” by default\nUse list to select multiple columns and return data table:\n\n\nflights[, list(arr_delay,air_time)] |&gt; \n  head()\n\n   arr_delay air_time\n       &lt;int&gt;    &lt;int&gt;\n1:        13      359\n2:        13      363\n3:         9      351\n4:       -26      157\n5:         1      350\n6:         0      339"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#list-alias-.",
    "href": "meetups/Meetup13/Meetup13.html#list-alias-.",
    "title": "Meetup 13: Big Data and data.table",
    "section": "list alias .()",
    "text": "list alias .()\n\nCan use . instead of list\n\n\nflights[ , .(origin,dest,dep_delay,arr_delay,air_time)] |&gt; \n  head()\n\n   origin   dest dep_delay arr_delay air_time\n   &lt;char&gt; &lt;char&gt;     &lt;int&gt;     &lt;int&gt;    &lt;int&gt;\n1:    JFK    LAX        14        13      359\n2:    JFK    LAX        -3        13      363\n3:    JFK    LAX         2         9      351\n4:    LGA    PBI        -8       -26      157\n5:    JFK    LAX         2         1      350\n6:    EWR    LAX         4         0      339\n\n\n\nCan also select using a vector with .. notation:\n\n\nsel = c(\"origin\",\"dest\",\"dep_delay\",\"arr_delay\",\"air_time\")\n\nflights[ , ..sel ] |&gt; head()\n\n   origin   dest dep_delay arr_delay air_time\n   &lt;char&gt; &lt;char&gt;     &lt;int&gt;     &lt;int&gt;    &lt;int&gt;\n1:    JFK    LAX        14        13      359\n2:    JFK    LAX        -3        13      363\n3:    JFK    LAX         2         9      351\n4:    LGA    PBI        -8       -26      157\n5:    JFK    LAX         2         1      350\n6:    EWR    LAX         4         0      339"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#logical-selection",
    "href": "meetups/Meetup13/Meetup13.html#logical-selection",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Logical selection",
    "text": "Logical selection\n\nCan select range of columns:\n\n\nflights[ , year:carrier  ] |&gt; head()\n\n    year month   day dep_delay arr_delay carrier\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt;\n1:  2014     1     1        14        13      AA\n2:  2014     1     1        -3        13      AA\n3:  2014     1     1         2         9      AA\n4:  2014     1     1        -8       -26      AA\n5:  2014     1     1         2         1      AA\n6:  2014     1     1         4         0      AA\n\nflights[ , !(year:carrier)  ]\n\n        origin   dest air_time distance  hour\n        &lt;char&gt; &lt;char&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n     1:    JFK    LAX      359     2475     9\n     2:    JFK    LAX      363     2475    11\n     3:    JFK    LAX      351     2475    19\n     4:    LGA    PBI      157     1035     7\n     5:    JFK    LAX      350     2475    13\n    ---                                      \n253312:    LGA    IAH      201     1416    14\n253313:    EWR    IAH      189     1400     8\n253314:    LGA    RDU       83      431    11\n253315:    LGA    DTW       75      502    11\n253316:    LGA    SDF      110      659     8"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#rename",
    "href": "meetups/Meetup13/Meetup13.html#rename",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Rename",
    "text": "Rename\n\nCan rename columns when you select just like you name columns in a list\n\n\nflights[ , list(\n  arrival_delay = arr_delay, \n  departure_delay = dep_delay, \n  time_in_air = air_time\n  )] |&gt; \n  head()\n\n   arrival_delay departure_delay time_in_air\n           &lt;int&gt;           &lt;int&gt;       &lt;int&gt;\n1:            13              14         359\n2:            13              -3         363\n3:             9               2         351\n4:           -26              -8         157\n5:             1               2         350\n6:             0               4         339"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#calculations",
    "href": "meetups/Meetup13/Meetup13.html#calculations",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Calculations",
    "text": "Calculations\n\nCan perform mathematical operations in the j/2nd entry\nHow many flights with no delay:\n\n\nflights[ , sum( arr_delay + dep_delay &lt; 0  )   ] \n\n[1] 141814\n\n\n\nYou can combine sorting/subsetting in i with selection and computation in j\nEfficient because calculations done together, unnecessary options are avoided\n\n\nflights[origin == \"LGA\" & month == 1 , \n        sum( arr_delay + dep_delay &lt; 0  )   ] \n\n[1] 3671"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#counting",
    "href": "meetups/Meetup13/Meetup13.html#counting",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Counting",
    "text": "Counting\n\n.N is an alias for counting rows\n\n\nflights[origin == \"LGA\" & month == 1 & (arr_delay + dep_delay &lt; 0), \n       .N  ] \n\n[1] 3671\n\n\n\nNote that computation can also take place in the i column"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#aggregation",
    "href": "meetups/Meetup13/Meetup13.html#aggregation",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Aggregation",
    "text": "Aggregation\n\nThird column by is for aggregation (group_by)\nHow many trips from each airport?\n\n\nflights[ , .N, by = origin]\n\n   origin     N\n   &lt;char&gt; &lt;int&gt;\n1:    JFK 81483\n2:    LGA 84433\n3:    EWR 87400\n\n\n\nNeed . alias for multiple entries:\n\n\nflights[ carrier %in% c(\"DL\",\"AA\") , \n         .(number = .N, mean_dep_delay = mean(dep_delay)) ,\n         by = .(origin,dest)    ] |&gt; \n  head()\n\n   origin   dest number mean_dep_delay\n   &lt;char&gt; &lt;char&gt;  &lt;int&gt;          &lt;num&gt;\n1:    JFK    LAX   5594       8.845370\n2:    LGA    PBI   1594       7.720828\n3:    EWR    LAX     62       4.870968\n4:    JFK    MIA   2750      10.008364\n5:    JFK    SEA   1376      11.578488\n6:    EWR    MIA    848       7.503538"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#sorting-groups",
    "href": "meetups/Meetup13/Meetup13.html#sorting-groups",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Sorting Groups",
    "text": "Sorting Groups\n\nkeyby lets you resort by groups:\nCreates a key\n\n\nflights[ carrier %in% c(\"DL\",\"AA\") , \n         .(number = .N, mean_dep_delay = mean(dep_delay)) ,\n         keyby = .(origin,dest)    ] |&gt; \n  head()\n\nKey: &lt;origin, dest&gt;\n   origin   dest number mean_dep_delay\n   &lt;char&gt; &lt;char&gt;  &lt;int&gt;          &lt;num&gt;\n1:    EWR    ATL   2966      17.108227\n2:    EWR    DFW   1618      17.715698\n3:    EWR    DTW    568       5.357394\n4:    EWR    LAX     62       4.870968\n5:    EWR    MIA    848       7.503538\n6:    EWR    MSP    334      10.164671"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#chaining-expressions",
    "href": "meetups/Meetup13/Meetup13.html#chaining-expressions",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Chaining Expressions",
    "text": "Chaining Expressions\n\nWhat if you want to perform a series of operations that don’t neatly fit in a single bracket?\nCan chain together multiple brackets\nSuppose we want to get number of trips for each origin dest pair but have a different order for origin and dest?\n\n\nflights[carrier %in% c(\"DL\",\"AA\",\"UA\"),\n        .N,\n        by = .(origin, dest)][order(origin,-dest)] |&gt; \n  head(10) \n\n    origin   dest     N\n    &lt;char&gt; &lt;char&gt; &lt;int&gt;\n 1:    EWR    TPA  1539\n 2:    EWR    STT   174\n 3:    EWR    SNA   657\n 4:    EWR    SLC   285\n 5:    EWR    SJU   608\n 6:    EWR    SFO  3725\n 7:    EWR    SEA   787\n 8:    EWR    SDF     2\n 9:    EWR    SAT   205\n10:    EWR    SAN   884"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#group-by-expressions",
    "href": "meetups/Meetup13/Meetup13.html#group-by-expressions",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Group By Expressions",
    "text": "Group By Expressions\n\nCan include expressions in the by column\nGroup by whether the flights arrived or departed late\n\n\nflights[ , .N, by = .(origin, dep_delay &gt; 0, arr_delay &gt; 0)][\n  order(origin,dep_delay,arr_delay)]\n\n    origin dep_delay arr_delay     N\n    &lt;char&gt;    &lt;lgcl&gt;    &lt;lgcl&gt; &lt;int&gt;\n 1:    EWR     FALSE     FALSE 38082\n 2:    EWR     FALSE      TRUE  9298\n 3:    EWR      TRUE     FALSE 11265\n 4:    EWR      TRUE      TRUE 28755\n 5:    JFK     FALSE     FALSE 38631\n 6:    JFK     FALSE      TRUE 12749\n 7:    JFK      TRUE     FALSE  8216\n 8:    JFK      TRUE      TRUE 21887\n 9:    LGA     FALSE     FALSE 42591\n10:    LGA     FALSE      TRUE 12536\n11:    LGA      TRUE     FALSE  7112\n12:    LGA      TRUE      TRUE 22194"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#applying-functions-across-columns",
    "href": "meetups/Meetup13/Meetup13.html#applying-functions-across-columns",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Applying functions across columns",
    "text": "Applying functions across columns\n\n.SD in the j column stands for the data for each group in .by\nUse lapply similar to how we used across\n.SDcols argument after by\n\n\nflights[ , lapply(.SD, mean), by = .(origin, dest, month)\n         , .SDcols = c(\"arr_delay\",\"dep_delay\")  ]\n\n      origin   dest month  arr_delay dep_delay\n      &lt;char&gt; &lt;char&gt; &lt;int&gt;      &lt;num&gt;     &lt;num&gt;\n   1:    JFK    LAX     1  11.342956 20.521940\n   2:    LGA    PBI     1  23.000000 21.382143\n   3:    EWR    LAX     1   7.906494 18.745455\n   4:    JFK    MIA     1  20.454902 22.505882\n   5:    JFK    SEA     1  15.101449 27.253623\n  ---                                         \n1913:    EWR    SAT    10 -15.500000  9.333333\n1914:    EWR    BTV    10  -2.000000 -3.800000\n1915:    EWR    ORF    10   6.833333 12.000000\n1916:    LGA    DAL    10 -16.000000 -6.266667\n1917:    LGA    CVG    10 -11.333333 -5.666667"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#applying-functions-across-columns-1",
    "href": "meetups/Meetup13/Meetup13.html#applying-functions-across-columns-1",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Applying functions across columns",
    "text": "Applying functions across columns\n\n.SD in the j column stands for the data for each group in .by\nUse lapply similar to how we used across\n`.SDcols can use logic to select columns\n\n\nflights[ , lapply(.SD, mean), by = .(origin, dest, month)\n         , .SDcols = is.numeric  ]\n\n      origin   dest month  year month      day dep_delay  arr_delay  air_time\n      &lt;char&gt; &lt;char&gt; &lt;int&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n   1:    JFK    LAX     1  2014     1 16.28984 20.521940  11.342956 339.16513\n   2:    LGA    PBI     1  2014     1 16.13929 21.382143  23.000000 159.17500\n   3:    EWR    LAX     1  2014     1 15.70649 18.745455   7.906494 335.64416\n   4:    JFK    MIA     1  2014     1 16.18824 22.505882  20.454902 163.15294\n   5:    JFK    SEA     1  2014     1 16.34783 27.253623  15.101449 336.10870\n  ---                                                                        \n1913:    EWR    SAT    10  2014    10 28.50000  9.333333 -15.500000 214.66667\n1914:    EWR    BTV    10  2014    10 29.00000 -3.800000  -2.000000  46.20000\n1915:    EWR    ORF    10  2014    10 29.33333 12.000000   6.833333  50.16667\n1916:    LGA    DAL    10  2014    10 29.60000 -6.266667 -16.000000 196.60000\n1917:    LGA    CVG    10  2014    10 30.00000 -5.666667 -11.333333  97.33333\n      distance      hour\n         &lt;num&gt;     &lt;num&gt;\n   1:     2475 13.547344\n   2:     1035 12.285714\n   3:     2454 13.911688\n   4:     1089 11.501961\n   5:     2422 15.260870\n  ---                   \n1913:     1569 18.166667\n1914:      266  8.000000\n1915:      284  9.833333\n1916:     1381 12.933333\n1917:      585 13.000000"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#applying-functions-across-columns-2",
    "href": "meetups/Meetup13/Meetup13.html#applying-functions-across-columns-2",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Applying functions across columns",
    "text": "Applying functions across columns\n\n.SD in the j column stands for the data for each group in .by\nUse lapply similar to how we used across\n`.SDcols can use logic to select columns\n\n\nflights[ , lapply(.SD, mean), by = .(origin, dest, month)\n         , .SDcols = patterns(\"delay\")  ]\n\n      origin   dest month dep_delay  arr_delay\n      &lt;char&gt; &lt;char&gt; &lt;int&gt;     &lt;num&gt;      &lt;num&gt;\n   1:    JFK    LAX     1 20.521940  11.342956\n   2:    LGA    PBI     1 21.382143  23.000000\n   3:    EWR    LAX     1 18.745455   7.906494\n   4:    JFK    MIA     1 22.505882  20.454902\n   5:    JFK    SEA     1 27.253623  15.101449\n  ---                                         \n1913:    EWR    SAT    10  9.333333 -15.500000\n1914:    EWR    BTV    10 -3.800000  -2.000000\n1915:    EWR    ORF    10 12.000000   6.833333\n1916:    LGA    DAL    10 -6.266667 -16.000000\n1917:    LGA    CVG    10 -5.666667 -11.333333"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#joins",
    "href": "meetups/Meetup13/Meetup13.html#joins",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Joins",
    "text": "Joins\n\nTwo ways to do joins, put one DT in the i column of a bracket\nUsing merge\nDefine “holiday” DT (hat tip Toby Hocking)\n\n\nholiday = data.table(month = c(1,7,11,12),\n                     day = c(1,4,27,25),\n                     holiday = c(\"NYE\",\n                                 \"Independence_Day\",\n                                 \"Thanksgiving\",\n                                 \"Christmas\"))\n\nholiday\n\n   month   day          holiday\n   &lt;num&gt; &lt;num&gt;           &lt;char&gt;\n1:     1     1              NYE\n2:     7     4 Independence_Day\n3:    11    27     Thanksgiving\n4:    12    25        Christmas"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#right-join",
    "href": "meetups/Meetup13/Meetup13.html#right-join",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Right Join",
    "text": "Right Join\n\nDT1[DT2, on = c(keys)] does a Right Join\n\n\nflights[holiday, on = c(\"month\",\"day\")]\n\n       year month   day dep_delay arr_delay carrier origin   dest air_time\n      &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n   1:  2014     1     1        14        13      AA    JFK    LAX      359\n   2:  2014     1     1        -3        13      AA    JFK    LAX      363\n   3:  2014     1     1         2         9      AA    JFK    LAX      351\n   4:  2014     1     1        -8       -26      AA    LGA    PBI      157\n   5:  2014     1     1         2         1      AA    JFK    LAX      350\n  ---                                                                     \n1389:  2014     7     4        -7       -18      US    LGA    DCA       43\n1390:  2014     7     4        -6       -12      US    LGA    DCA       43\n1391:  2014     7     4        -7       -18      US    LGA    DCA       42\n1392:    NA    11    27        NA        NA    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;       NA\n1393:    NA    12    25        NA        NA    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;       NA\n      distance  hour          holiday\n         &lt;int&gt; &lt;int&gt;           &lt;char&gt;\n   1:     2475     9              NYE\n   2:     2475    11              NYE\n   3:     2475    19              NYE\n   4:     1035     7              NYE\n   5:     2475    13              NYE\n  ---                                \n1389:      214     8 Independence_Day\n1390:      214    10 Independence_Day\n1391:      214    12 Independence_Day\n1392:       NA    NA     Thanksgiving\n1393:       NA    NA        Christmas"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#left-join",
    "href": "meetups/Meetup13/Meetup13.html#left-join",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Left Join",
    "text": "Left Join\n\nReverse the arguments for left join\n\n\nholiday[flights, on = c(\"month\",\"day\")]\n\n        month   day holiday  year dep_delay arr_delay carrier origin   dest\n        &lt;int&gt; &lt;int&gt;  &lt;char&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;\n     1:     1     1     NYE  2014        14        13      AA    JFK    LAX\n     2:     1     1     NYE  2014        -3        13      AA    JFK    LAX\n     3:     1     1     NYE  2014         2         9      AA    JFK    LAX\n     4:     1     1     NYE  2014        -8       -26      AA    LGA    PBI\n     5:     1     1     NYE  2014         2         1      AA    JFK    LAX\n    ---                                                                    \n253312:    10    31    &lt;NA&gt;  2014         1       -30      UA    LGA    IAH\n253313:    10    31    &lt;NA&gt;  2014        -5       -14      UA    EWR    IAH\n253314:    10    31    &lt;NA&gt;  2014        -8        16      MQ    LGA    RDU\n253315:    10    31    &lt;NA&gt;  2014        -4        15      MQ    LGA    DTW\n253316:    10    31    &lt;NA&gt;  2014        -5         1      MQ    LGA    SDF\n        air_time distance  hour\n           &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n     1:      359     2475     9\n     2:      363     2475    11\n     3:      351     2475    19\n     4:      157     1035     7\n     5:      350     2475    13\n    ---                        \n253312:      201     1416    14\n253313:      189     1400     8\n253314:       83      431    11\n253315:       75      502    11\n253316:      110      659     8"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#inner-join",
    "href": "meetups/Meetup13/Meetup13.html#inner-join",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Inner Join",
    "text": "Inner Join\n\nAdd nomatch = 0 to make it an inner join ()\nDrops all the NA values\n\n\nholiday[flights, on = c(\"month\",\"day\"), nomatch = 0]\n\n      month   day          holiday  year dep_delay arr_delay carrier origin\n      &lt;int&gt; &lt;int&gt;           &lt;char&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt;\n   1:     1     1              NYE  2014        14        13      AA    JFK\n   2:     1     1              NYE  2014        -3        13      AA    JFK\n   3:     1     1              NYE  2014         2         9      AA    JFK\n   4:     1     1              NYE  2014        -8       -26      AA    LGA\n   5:     1     1              NYE  2014         2         1      AA    JFK\n  ---                                                                      \n1387:     7     4 Independence_Day  2014        -4         7      US    EWR\n1388:     7     4 Independence_Day  2014        -3        -7      US    JFK\n1389:     7     4 Independence_Day  2014        -7       -18      US    LGA\n1390:     7     4 Independence_Day  2014        -6       -12      US    LGA\n1391:     7     4 Independence_Day  2014        -7       -18      US    LGA\n        dest air_time distance  hour\n      &lt;char&gt;    &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n   1:    LAX      359     2475     9\n   2:    LAX      363     2475    11\n   3:    LAX      351     2475    19\n   4:    PBI      157     1035     7\n   5:    LAX      350     2475    13\n  ---                               \n1387:    CLT       89      529    13\n1388:    CLT       86      541    11\n1389:    DCA       43      214     8\n1390:    DCA       43      214    10\n1391:    DCA       42      214    12"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#full-join",
    "href": "meetups/Meetup13/Meetup13.html#full-join",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Full Join",
    "text": "Full Join\n\nSeveral ways, but merge with all = TRUE works\nCan use DT1[DT2] notation but clunkier\n\n\nmerge(flights,holiday,all = TRUE)\n\nKey: &lt;month, day&gt;\n        month   day  year dep_delay arr_delay carrier origin   dest air_time\n        &lt;num&gt; &lt;num&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n     1:     1     1  2014        14        13      AA    JFK    LAX      359\n     2:     1     1  2014        -3        13      AA    JFK    LAX      363\n     3:     1     1  2014         2         9      AA    JFK    LAX      351\n     4:     1     1  2014        -8       -26      AA    LGA    PBI      157\n     5:     1     1  2014         2         1      AA    JFK    LAX      350\n    ---                                                                     \n253314:    10    31  2014        -8        16      MQ    LGA    RDU       83\n253315:    10    31  2014        -4        15      MQ    LGA    DTW       75\n253316:    10    31  2014        -5         1      MQ    LGA    SDF      110\n253317:    11    27    NA        NA        NA    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;       NA\n253318:    12    25    NA        NA        NA    &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;       NA\n        distance  hour      holiday\n           &lt;int&gt; &lt;int&gt;       &lt;char&gt;\n     1:     2475     9          NYE\n     2:     2475    11          NYE\n     3:     2475    19          NYE\n     4:     1035     7          NYE\n     5:     2475    13          NYE\n    ---                            \n253314:      431    11         &lt;NA&gt;\n253315:      502    11         &lt;NA&gt;\n253316:      659     8         &lt;NA&gt;\n253317:       NA    NA Thanksgiving\n253318:       NA    NA    Christmas"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#anti-join",
    "href": "meetups/Meetup13/Meetup13.html#anti-join",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Anti Join",
    "text": "Anti Join\n\nUse DT[!DT2, ...] for an anti-join\n\n\nflights[!holiday, on = c(\"month\",\"day\")]\n\n         year month   day dep_delay arr_delay carrier origin   dest air_time\n        &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n     1:  2014     1     2        -3         1      AA    JFK    LAX      340\n     2:  2014     1     2        -3         9      AA    JFK    LAX      345\n     3:  2014     1     2       109       150      AA    JFK    LAX      316\n     4:  2014     1     2        83       103      AA    LGA    PBI      169\n     5:  2014     1     2         8        34      AA    JFK    LAX      344\n    ---                                                                     \n251921:  2014    10    31         1       -30      UA    LGA    IAH      201\n251922:  2014    10    31        -5       -14      UA    EWR    IAH      189\n251923:  2014    10    31        -8        16      MQ    LGA    RDU       83\n251924:  2014    10    31        -4        15      MQ    LGA    DTW       75\n251925:  2014    10    31        -5         1      MQ    LGA    SDF      110\n        distance  hour\n           &lt;int&gt; &lt;int&gt;\n     1:     2475     8\n     2:     2475    11\n     3:     2475    20\n     4:     1035     8\n     5:     2475     7\n    ---               \n251921:     1416    14\n251922:     1400     8\n251923:      431    11\n251924:      502    11\n251925:      659     8"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#semi-join",
    "href": "meetups/Meetup13/Meetup13.html#semi-join",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Semi Join",
    "text": "Semi Join\n\nA little clunky, need to use na.omit and which\n\n\nflights[na.omit(flights[holiday, on = c(\"month\",\"day\"),which = TRUE])]\n\n       year month   day dep_delay arr_delay carrier origin   dest air_time\n      &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt;     &lt;int&gt;  &lt;char&gt; &lt;char&gt; &lt;char&gt;    &lt;int&gt;\n   1:  2014     1     1        14        13      AA    JFK    LAX      359\n   2:  2014     1     1        -3        13      AA    JFK    LAX      363\n   3:  2014     1     1         2         9      AA    JFK    LAX      351\n   4:  2014     1     1        -8       -26      AA    LGA    PBI      157\n   5:  2014     1     1         2         1      AA    JFK    LAX      350\n  ---                                                                     \n1387:  2014     7     4        -4         7      US    EWR    CLT       89\n1388:  2014     7     4        -3        -7      US    JFK    CLT       86\n1389:  2014     7     4        -7       -18      US    LGA    DCA       43\n1390:  2014     7     4        -6       -12      US    LGA    DCA       43\n1391:  2014     7     4        -7       -18      US    LGA    DCA       42\n      distance  hour\n         &lt;int&gt; &lt;int&gt;\n   1:     2475     9\n   2:     2475    11\n   3:     2475    19\n   4:     1035     7\n   5:     2475    13\n  ---               \n1387:      529    13\n1388:      541    11\n1389:      214     8\n1390:      214    10\n1391:      214    12"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#reflection-meetup",
    "href": "meetups/Meetup13/Meetup13.html#reflection-meetup",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Reflection Meetup",
    "text": "Reflection Meetup\nPlease fill out the following google form after the meeting or watching the video:\nClick Here"
  },
  {
    "objectID": "meetups/Meetup13/Meetup13.html#data-science-in-context-presentations",
    "href": "meetups/Meetup13/Meetup13.html#data-science-in-context-presentations",
    "title": "Meetup 13: Big Data and data.table",
    "section": "Data Science in Context Presentations",
    "text": "Data Science in Context Presentations"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "modules/module11.html",
    "href": "modules/module11.html",
    "title": "Module 11 - Tidy Text and NLP",
    "section": "",
    "text": "Text data is unlike other data formats which naturally have a rectangular or tabular form, necessitating a different type of analysis. The field known as Natural Language Processing, or NLP for short, has developed tools and techniques for deriving insight from textual data. In this module we will learn some of the basics of NLP and practice using them in a tidyverse context using the tidytext package.\nIn particular, we will learn about tokenization and how to tokenize text into a tidy format. Then we will explore some sentiment lexicons, which classify words according to whether they convey negative or positive emotions (either categorically or numerically), or as exemplars of some other emotional category. Sentiment analysis is an important part of many commercial applications of data science, but is challenging due to the nuance and complexities of language (e.g. sarcasm, satire). We will also study how to interpret the frequency of words in a text, using the underlying frequency distribution in a corpus as a base rate to normalize against. This can allow us to learn the topic or infer the most important context from a body of text automatically. Finally, we will study groupings of words, i.e. n-grams, using them to understand correlations between terms or when the presence of two words together alters meaning.\n\nLearning Objectives\n\nThe tidy text format\nLexicons and sentiment analysis\nWord frequency analysis\nn-grams and correlations\n\n\n\nReadings\n\nText Mining with R: Chapters 1-4\n\n\n\nVideos\n\nVideo Intro to Sentiment Analysis",
    "crumbs": [
      "Topics",
      "11 - Tidy Text and NLP"
    ]
  },
  {
    "objectID": "modules/module8.html",
    "href": "modules/module8.html",
    "title": "Module 8 - Advanced R Programming",
    "section": "",
    "text": "Learning Objectives\n\nDifferent function types and reusing code\nIteration and flow control with purrr\nApplications of iteration to data processing\nLearn important concepts from base R\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 25-27\n\nThe most important R package we will introduce this week is called purrr, which you can read more about by clicking here\n\n\nAdditional Resources:\n\nAdvanced R Programming Chapter on Functional Programming",
    "crumbs": [
      "Topics",
      "8 - Advanced R Programming"
    ]
  },
  {
    "objectID": "modules/module10.html",
    "href": "modules/module10.html",
    "title": "Module 10 - Git and Collaboration",
    "section": "",
    "text": "Learning Objectives\n\nBasic github commands\nGithub workflows and Branching\n\n\n\nReadings\n\nHappy Git and GitHub for the useR : Sections 15-25, 28 and 29 if you need\nhttr2 API package\n\n\n\nAdditional Resources:\n\nSoftware Carpentries git tutorial\nLearn Git Branching interactive tutorial\nusethis package\nWeb APIs Book Club Page I recommend the intro slides and the httr2 specific slides.",
    "crumbs": [
      "Topics",
      "10 - GIT and Collaboration"
    ]
  },
  {
    "objectID": "modules/module2.html",
    "href": "modules/module2.html",
    "title": "Module 2 - Data Vizualization and Basic Transformations",
    "section": "",
    "text": "Overview and Deliverables\nThis module focuses on visualization, which is not only one of the first tools that you should use when faced with a new dataset but also the primary way that we communicate quantitative information with other people. We will discuss principles of good visualizations and how to use the ggplot2 package for combinations of numerical and categorical data. ggplot2 is based on a conceptual framework called the “grammar of graphics”, which you will become familiar with as we build plots. Your first homework assignment, Lab 1, will be due at the end of this week. Your reading will also cover some of the basics of using R which might be a review if you already are an experienced user. Here are your deliverables:\n\n9/1: Attend the class meetup (remember to submit the meetup reflection)\n9/7: Submit Lab 1 on the Brightspace page for the course\n\n\n\nLearning Objectives\n\nData Visualization for Numerical and Categorical Variables with ggplot2\nLearn principles of good visualizations\ntidyverse code conventions\nBasic Data Transofrmations with dplyr\n\n\n\nReadings\nThe basics of using the tidyverse:\n\nRDS (R for Data Science): Chapters 1-4\n\nTwo good articles on making honest visualizations and identifying dishonest ones:\n\nCalling BS: Principle of Proportional Ink\nCalling BS: Misleading Axes\n\n\n\nAdditional Resources:\n\nFundamentals of Data Visualization. Claus O. Wilkey. (2019). O’Reilly.\n\nThis is my favorite modern book on data visualization, and it also uses ggplot2 as the primary tool. If you are uncertain about what visualizations might be appropriate for a given problem, this is a good place to check for inspiration. The intended audience is scientists but it is equally relevant for people working in other industries. Chapters 1-5 and Chapter 29 would be good additional readings if you wanted to go a little more in depth.\n\nData Visualization: A Practical Introduction. Kieran Healey.\n\nWell written book on R and data visualization. This is a good place to turn if you are struggling with the style of the main course textbook.\n\nA Layered Grammar of Graphics. Hadley Wickham. Journal of computational and graphical statistics 19.1 (2010): 3-28.\n\nThis paper explains the concepts and thinking behind ggplot2.\n\nggplot2 website. This is another excellent source of resources and information on ggplot2",
    "crumbs": [
      "Topics",
      "2 - Visualizing Data"
    ]
  },
  {
    "objectID": "modules/module13.html",
    "href": "modules/module13.html",
    "title": "Module 13 - Large Data and Distributed Computing",
    "section": "",
    "text": "Learning Objectives\n\nBasic Introduction to Computer Architecture\nWrangling large data sets using data.table\nCloud and Parrallel computing models\nUsing R and Apache Spark together with sparklyr\n\n\n\nReadings\n\ndata.table vignette\nAdvanced R: Why is R slow?\nMastering Spark with R: Chapters 1-3",
    "crumbs": [
      "Topics",
      "13 - Big Data and Cloud Computing"
    ]
  },
  {
    "objectID": "modules/module4.html",
    "href": "modules/module4.html",
    "title": "Module 4 - Exploratory Data Analysis",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand how Exploratory Data Analysis (EDA) informs Data Preparation and Modeling\nVisualizing variation and covariation for categorical and numerical data\nData checks, data cleaning, outliers, missing data summary statistics\nLayers and Stats in ggplot\nReproducible Examples\n\n\n\nReadings\n\nRDS (R for Data Science): Chapters 8-11 (Chapter 10 is the most substantial part of the reading assignment)\n\n\n\nAdditional Reading:\nThere are several classic texts on Exploratory Data Analysis. These are somewhat dated but contain important insights:\n\nExploratory Data Analysis. J. W. Tukey. (1977).\nVisualizing Data. W. S. Cleveland. (1993). Hobart press.\nExploratory data mining and data cleaning. T. Dasu and T. Johnson. (2003). John Wiley & Sons.\n\nHere is an example of a newer book which is also excellent:\n\nExploratory Data Analysis Using R. Roland Pearson. CRC Press. (2015).\n\n\n\nVideos\n\nWhole Game Live demo of an Exploratory Data Analysis by Hadley Wickham. This isn’t a super polished video but its really nice to see examples of experts going through their process. The code he typed and data that he used can be found on github. This video has more of a focus on exploring and finding patterns rather than finding problems in a poor dataset.\nDebugging Video This is an excellent lecture on debugging by the author of your github textbook. Debugging is an absolutely crucial skill and having a systematic approach can save you an enormous amount of time. The video is one hour but if you get good at debugging that will pay off very quickly\n\n\n\nOther Resources\n\npredictive package for tidymodels This is an LLM driven coding agent that runs from R studio. Requires a Claude API key, but it is very good for suggesting things to try (and trying them) in an EDA context (though it is not that great for its primary task which is modeling).",
    "crumbs": [
      "Topics",
      "4 - Advanced Visualizations and EDA"
    ]
  },
  {
    "objectID": "posts/2025-11-13-LLM-Vignette.html",
    "href": "posts/2025-11-13-LLM-Vignette.html",
    "title": "Spam Email Classification using Ellmer and Malls",
    "section": "",
    "text": "In this coding vignette I use the ellmer and mall packages to classify the text messages in the UCI ML database SMS Spam Collection into spam or legitimate. For some reason I thought these were emails which is a bit funny if you look at them closely, so forgive me for any confusion.\nTo follow along with the vignette you need to have an api-key or use a local model, which is what you will need to do the homework.\nClick here to watch the video\nThe live code that was used for this vignette can be found on github at LLM-vignette-live.R. In that same directory you can find the SMSSpamCollection file and the two prompt markdown files that I used."
  },
  {
    "objectID": "posts/2025-09-28-Week-6-Info.html",
    "href": "posts/2025-09-28-Week-6-Info.html",
    "title": "Week 6 Info: Working With Text and Strings",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 6, where we will learn tools for working with text, strings, and categories. These data types lack the strict mathematical structures that come with logical, numerical, and temporal data, and as a result require their own distinct set of tools. This week will involve a lot of practice learning new tools and functions for finding and matching patterns in text, such as regular expressions, and these tools will form the basis for a module in a few wooks on analyzing datasets derived from text mining or web scraping.\nYou will have a lab assignment titled Working with Text and Strings due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 6."
  },
  {
    "objectID": "posts/2025-10-19-Week-9-Hierarchical-Data-And_Webscraping.html",
    "href": "posts/2025-10-19-Week-9-Hierarchical-Data-And_Webscraping.html",
    "title": "Week 9 Info: Webscraping and Hierarchical Data",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 9, where we will learn how to work with hierarchical data sets and how to use R to scrape websites for data. Web datasets are usually stored in a hierarchical, rather than tabular format, which requires different tools. We will make use of the rvest package to webscrape data into R and also use tools from the purrr package and data contained in repurrrsive.\nThis week we will have a lab assignment which involves extracting data stored in a hierarchical format, and combining webscraping tools with functions and iteration to automatically download many files at once. This assignment is due Sunday by midnight: Lab 7.\nFor your reading assignments and more details about this week, head over to Module 9. ."
  },
  {
    "objectID": "posts/2025-10-26-Week-10-Git-and-APIs.html",
    "href": "posts/2025-10-26-Week-10-Git-and-APIs.html",
    "title": "Week 10 Info: Collaboration, git, and APIs",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 10, where we will continue our exploration of tools for extracting data from the internet by learning how to work with APIs using the httr2 package and also begin to practice using github to work on a collaborative software project.\nThis week we will do the tidyverse create assignment, in which each of you create a qmd file where you extract data from a web API and perform an analysis on the data, and then contribute this markdown file to a group repository which on my github page. This assignment is due Sunday by midnight: tidy create.\nFor your reading assignments and more details about this week, head over to Module 10."
  },
  {
    "objectID": "posts/2025-09-09-Pivot-Vignette.html",
    "href": "posts/2025-09-09-Pivot-Vignette.html",
    "title": "Pivot Vignette",
    "section": "",
    "text": "I have posted the video for the coding demonstration that I could not finish during Meetup 3. You can find it on youtube by clicking here .\nI go through 4 different pivot examples. The first 3 come from chapter 5 of R for Data Science, and the last one comes from the tidyr package website.\nIf you want to follow along with the coding vignette, you can download the data and the code here:\n\nWeather Station Vignette Code\nWeather Station Vignette Data\nLive code that I typed in during the video"
  },
  {
    "objectID": "posts/2025-09-07-Week-3-Info.html",
    "href": "posts/2025-09-07-Week-3-Info.html",
    "title": "Week 3 Info: Working with Tidy Data",
    "section": "",
    "text": "Week 3 has now begun. You have now had some basic practice using some key tidyverse packages, in particular dplyr for manipulating data and ggplot2 for making visualizations. So far in this class we have intentionally worked with nice data sets which were in a form ready to analyze using the tidyverse tools. In this week we will discuss what the tidy in tidyverse means and how a concept called tidy data underlies the design philosophy of the entire group of software packages in the tidyverse. Along the way we will learn tools for importing data and tidying it so that it can be analyzed easily with tidyverse tools.\nYou will have a lab assignment titled Tidy Data due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 3. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2025-10-13-Week-8-Info-Functions-and-Iteration.html",
    "href": "posts/2025-10-13-Week-8-Info-Functions-and-Iteration.html",
    "title": "Week 8 Info: Functions and Iteration and Project Proposals",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 8, where we will learn several more sophisticated programming concepts in R which will enable you to write cleaner, more concise, and more understandable code and to also accomplish more complex programming tasks than previously. The key topics will be functions (Chapter 25); which allow you to re-use and re-factor code by aggregating chunks of code that perform specific functions into function definitions, iteration (Chapter 26); which allows you to perform the same operation on many different objects simultaneously, and important base R functions (Chapter 27); which will round out your R knowledge. Chapter 26 will introduce you to a programming paradigm known as functional programming, which has distinctive strengths and weaknesses compared to object oriented and imperative programming and is likely to be less familiar to most of you.\nThere will be no lab assignment this week, but you will need to complete a short (~2 page) project proposal by next Sunday: Project Proposal.\nFor your reading assignments and more details about this week, head over to Module 8. Though we have no lab assignment this week, we will work the concepts of this week into future assignments and they are important for your success as an R programmer."
  },
  {
    "objectID": "posts/2025-09-14-Week-4.html",
    "href": "posts/2025-09-14-Week-4.html",
    "title": "Week 4 Info: Exploratory Data Analysis",
    "section": "",
    "text": "Week 4 has now begun. During this week we will learn about a technique called exploratory data analysis (EDA). EDA is a methodology applied at the beginning of a statistical analysis to find errors in the data, to discover patterns, and to explore hypotheses. EDA uses visualizations as one of its primary tools, and we will also deepen our knowledge of how to make visualizations in ggplot.\nYou will have a lab assignment titled Exploratory Data Analysis due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 4."
  },
  {
    "objectID": "posts/2025-09-23-Correlation-Trick.html",
    "href": "posts/2025-09-23-Correlation-Trick.html",
    "title": "Meetup 5 Continuation: Correlation Trick",
    "section": "",
    "text": "Click here to watch the last few slides of Meetup 5. We couldn’t quite finish all of the slides in this week’s meetup 5. I recorded a short video on the material we missed, which is a very useful window function called rank. The rank of your data points is invariant under monotone transformations and is the basis of the Spearman Correlation, which is a measure of correlation that holds even under nonlinear relationships."
  },
  {
    "objectID": "posts/2025-08-31-Week-2-Info.html",
    "href": "posts/2025-08-31-Week-2-Info.html",
    "title": "Week 2 Info: Data Visualizations and Transformations",
    "section": "",
    "text": "Hello Class, it is the start of Week 2. During the first week you should have completed the reading assignments, familiarized yourself with R, RStudio, quarto, and git, and made sure these are working on your computer. If you haven’t done these things its extremely important that you do them asap to avoid falling behind. It was really nice to read all of your intro posts on Brightspace, thanks for putting in the effort to describe your interests and background in data science.\nThe biggest difference from Week 2 to Week 1 is that you will have your first homework assignment, a lab assignment titled Airbnb in NYC. It will be due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 2. If you need help with with anything, don’t hesitate to post in slack or contact me directly."
  },
  {
    "objectID": "posts/2025-09-21-Week-5.html",
    "href": "posts/2025-09-21-Week-5.html",
    "title": "Week 5 Info: Data Transformations",
    "section": "",
    "text": "Hello Class,\nToday is the start of week 5, where we will learn about data transformations. Data transformations allow you to view your dataset in a new light, and are key tools for constructing models, performing EDA and visualizations, and eventually building features for machine learning. We will focus on using boolean operations (i.e. logic) to generate new features from existing data, and practice using window functions to transform time series data. Lastly, we will discuss some issues related to dates and times.\nYou will have a lab assignment titled Data Transformations due next Sunday at midnight.\nFor your reading assignments and more details about this week, head over to Module 5."
  },
  {
    "objectID": "posts/2025-10-14-Functional-Programming-Video.html",
    "href": "posts/2025-10-14-Functional-Programming-Video.html",
    "title": "Functional Programming with Purrr",
    "section": "",
    "text": "This video completes the coverage of the material we ended to cover in last night’s meetup. We go over the definition of functional programming, functions as first class objects, anonymous functions, and the across and map_* family of functions\nYou can watch the video on youtube youtube by clicking here.\nThe slides are the same as the meetup 8 slides found by clicking here"
  },
  {
    "objectID": "posts/2025-09-16-Meetup-4-Extra-Video-Posted.html",
    "href": "posts/2025-09-16-Meetup-4-Extra-Video-Posted.html",
    "title": "Meetup 4 Extra Video",
    "section": "",
    "text": "Hello all, I have finished recording a video of the content we didn’t get to on Monday. Below find links both to the original video and to the extra content:\nHere is the original meetup 4 video Watch the extra meetup 4 video here"
  },
  {
    "objectID": "posts/2025-09-15-Meetup-4-Slides.html",
    "href": "posts/2025-09-15-Meetup-4-Slides.html",
    "title": "Meetup 4 Slides",
    "section": "",
    "text": "I have posted the slides for Meetup 4 which you can view by clicking here\nDuring this meetup we will learn about a set of methods called exploratory data analysis, which is usually the first step that you take when dealing with a new dataset. These methods allow you to both find problems with the data and develop hypotheses. We will cover just the basics, but entire textbooks have been written about this subject.\nSome things if you need:\n\nMeetup Link Zoom\nUse the google doodle poll to sign up for your Data Science in Context presentation"
  },
  {
    "objectID": "posts/2025-11-04-Sentiment-Analysis-Vignette.html",
    "href": "posts/2025-11-04-Sentiment-Analysis-Vignette.html",
    "title": "Large Movie Reviews Sentiment Analysis Using Bigrams",
    "section": "",
    "text": "In this coding vignette I perform a sentiment analysis on the large movie review dataset, which you can find here https://ai.stanford.edu/~amaas/data/sentiment/. I start by performing a simple analysis just using the afinn sentiment lexicon. Then I show how to incorporate some basic word associations, modifying the sentiment value if the sentiment word occurs after a negative word or an extreme word.\nYou can watch the video on youtube by clicking here\nThe code I wrote for this vignette can be found here: movieReviewsVignette.R\nI also uploaded some additional video covering the slides that we did not cover in the meetup this week, which was on n-grams. You can watch that video by clicking here"
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: George Hagstrom, Ph.D. Class Meetup: 6:45-7:45 Eastern Office Hours: By appointment Email: george.hagstrom@cuny.edu\n\nCourse Description\nIn this course students will learn about core concepts of contemporary data collection and its management. Topics will include an introduction to programming and collaboration in statistical software packages, data visualization techniques, data wrangling and transformation, exploratory data analysis and data quality checks, data acquisition from a variety of sources including databases and the web, tools for working with textual and graph data, feature engineering, and working with large datasets in a cloud computing environment.\nStudents will complete a project to create a working system for a large volume of data using publicly available data sets.\n\n\nCourse Learning Outcomes:\nBy then end of the course, students should be able to:\n\nLoad data into R from various data sources, including CSV files, Excel spreadsheets, relational databases, APIs, and web pages.\nPerform various data cleansing and transformation work, including splitting, combining; resampling; variable creation; data aggregation; sorting and filtering data; strategies for working with outliers and missing data; data visualization and analysis in support of data cleansing activities.\nUnderstand different information architectures, data types, and data structures.\nUnderstand relational and non-relational database design and querying.\nCover relevant ethical issues including data privacy and misinformation.\n\n\n\nProgram Learning Outcomes addressed by the course:\n\nBusiness Understanding. Apply frameworks and processes to build out data analytics solutions from understanding of business goals.\nData Culture. Embody and champion the highest standards for the ethical and moral use of data; understand issues related to data privacy and data security.\nSolid foundational data programming skills, using industry standard tools, essential algorithms, and design patterns for working with structured data, unstructured data and big data.\nData understanding. Collect, describe, model, explore and verify data.\nData preparation. Selecting, cleaning, constructing, integrating, and formatting data.\n\n\n\nHow is this course relevant for data analytics professionals?\nMost data analytics professionals spend most of their time getting data and preparing it for analysis. This is the course that teaches these key skills, as we work with both structured and unstructured data.\n\n\nGrading\n\nMeetup Reflections (10%)\nLabs (50%)\nTidyVerse Recipes\nData Science in Context Presentation (5%)\nProject (25%)\n\n\nGrade Distribution\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter Grade\nRange %\nGPA\n\n\n\n\nExcellent - work is of exceptional quality\nA\n93 - 100\n4\n\n\nExcellent\nA-\n90 - 92.9\n3.7\n\n\nGood - work is above average\nB+\n87 - 89.9\n3.3\n\n\nSatisfactory\nB\n83 - 86.9\n3\n\n\nBelow Average\nB-\n80 - 82.9\n2.7\n\n\nPoor\nC+\n77 - 79.9\n2.3\n\n\nPoor\nC\n70 - 76.9\n2\n\n\nFailure\nF\n&lt; 70\n0\n\n\n\n\n\n\nHow This Course Works\nThis course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. Most weeks will have homework assignments and labs to be submitted (although some chapters will take more than one week, see the schedule for details). There will also be a presentation required and a forum post introduction required. You are expected to complete all assignments by their due dates.\nYou are expected to attend or watch every Meetup. I highly recommend attending the Meetups live if possible but understand that may not be possible for everyone. Recordings will be made available by the next morning on the Meetups page. In addition to highlighting key concepts from each learning module, some topics will be discussed that are not in the textbook. Moreover, we regularly make announcements in the Meetups that will be important to being successful in this course. At the end of each Meetup there will be a short reflective exercise. These will contribute to your participation grade.\nEach students will have to complete one short “Data Science in Context Presentation”- you will sign up for presentation times at the beginning of the semester.\nThe culmination of the course will be the presentation of the analysis of a dataset of your choosing. See the project for more information.\n\n\nTextbooks and Course Materials\n\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. This is the primary text for the course. Available online for free at: https://r4ds.hadley.nz/\nHappy Git and GitHub for the userR, Jennifer Bryan. Available online for free at happygitwithr.com/. This is a short book introducing git and github from the perspective of the statistical computing and data science use cases, and showing how it can be integrated with R and RStudio.\nText Mining with R: A Tidy Approach, Julia Silge and David Robinson. O’Reilly, 2017. Available online for free at https://www.tidytextmining.com/\n\n\n\nAccessibility and Accommodations\nThe CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\n\n\nOnline Etiquette and Anti-Harassment Policy\nThe University strictly prohibits the use of University online resources or facilities, including Brightspace, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University’s policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\n\n\nAcademic Integrity\nAcademic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students’ personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\n\n\nStudent Support Services\nIf you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "Click Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\n\n\n\n\n\n\n\n\nDate\nStart Time\nModule\nSlides\nVideo\nMain Deliverables\n\n\n\n\nAug 25\n06:45PM\nData Science Workflows and Toolkit\nMeetup 1 Slides\nMeetup 1 Video\nBrightspace Intro\n\n\nSep 1\n06:45PM\nVisualizing Data\nMeetup 2 Slides\nMeetup 2 Video\nSep 7 Lab 1\n\n\nSep 8\n06:45PM\nData Tidying and Wrangling\nMeetup 3 Slides\nMeetup 3 Video\nSep 14 Lab 2\n\n\nSep 15\n06:45PM\nExploratory Data Analysis\nMeetup 4 Slides\nMeetup 4 Video\nSep 21 Lab 3\n\n\nSep 22\n06:45PM\nData Transformations\nMeetup 5 Slides\nMeetup 5 Video\nSep 28 Lab 4\n\n\nSep 29\n06:45PM\nText and Strings\nMeetup 6 Slides\nMeetup 6 Video\nOct 5 Lab 5\n\n\nOct 6\n06:45PM\nDatabases and SQL\nMeetup 7 Slides\nMeetup 7 Video\nOct 12 Lab 6\n\n\nOct 13\n06:45PM\nAdvanced R Programming\nMeetup 8 Slides\nMeetup 8 Video\nOct 19 Proj. Proposal\n\n\nOct 20\n06:45PM\nWebscraping and APIs\nMeetup 9 Slides\nMeetup 9 Video\nOct 26 Lab 7\n\n\nOct 27\n06:45PM\nGit and Collaboration\nMeetup 10 Slides\nMeetup 10 Video\nNov 2 TV Create\n\n\nNov 3\n06:45PM\nTidy Text and NLP\nMeetup 11 Slides\nMeetup 11 Video\nNov 9 Lab 8\n\n\nNov 10\n06:45PM\nLarge Language Models\nMeetup 12 Slides\nMeetup 12 Videp\nNov 16 Lab 9\n\n\nNov 17\n06:45PM\nBig Data\n\n\nNov 23 TV Extend\n\n\nNov 24\n\nNo Meetup (Thanksgiving)\n\n\n\n\n\nDec 1\n06:45PM\nCloud Computing\n\n\nDec 7 Lab 10\n\n\nDec 8\n06:45PM\n\n\n\nDec 14 Final Project",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#meetup-link",
    "href": "course/schedule.html#meetup-link",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "Click Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\n\n\n\n\n\n\n\n\nDate\nStart Time\nModule\nSlides\nVideo\nMain Deliverables\n\n\n\n\nAug 25\n06:45PM\nData Science Workflows and Toolkit\nMeetup 1 Slides\nMeetup 1 Video\nBrightspace Intro\n\n\nSep 1\n06:45PM\nVisualizing Data\nMeetup 2 Slides\nMeetup 2 Video\nSep 7 Lab 1\n\n\nSep 8\n06:45PM\nData Tidying and Wrangling\nMeetup 3 Slides\nMeetup 3 Video\nSep 14 Lab 2\n\n\nSep 15\n06:45PM\nExploratory Data Analysis\nMeetup 4 Slides\nMeetup 4 Video\nSep 21 Lab 3\n\n\nSep 22\n06:45PM\nData Transformations\nMeetup 5 Slides\nMeetup 5 Video\nSep 28 Lab 4\n\n\nSep 29\n06:45PM\nText and Strings\nMeetup 6 Slides\nMeetup 6 Video\nOct 5 Lab 5\n\n\nOct 6\n06:45PM\nDatabases and SQL\nMeetup 7 Slides\nMeetup 7 Video\nOct 12 Lab 6\n\n\nOct 13\n06:45PM\nAdvanced R Programming\nMeetup 8 Slides\nMeetup 8 Video\nOct 19 Proj. Proposal\n\n\nOct 20\n06:45PM\nWebscraping and APIs\nMeetup 9 Slides\nMeetup 9 Video\nOct 26 Lab 7\n\n\nOct 27\n06:45PM\nGit and Collaboration\nMeetup 10 Slides\nMeetup 10 Video\nNov 2 TV Create\n\n\nNov 3\n06:45PM\nTidy Text and NLP\nMeetup 11 Slides\nMeetup 11 Video\nNov 9 Lab 8\n\n\nNov 10\n06:45PM\nLarge Language Models\nMeetup 12 Slides\nMeetup 12 Videp\nNov 16 Lab 9\n\n\nNov 17\n06:45PM\nBig Data\n\n\nNov 23 TV Extend\n\n\nNov 24\n\nNo Meetup (Thanksgiving)\n\n\n\n\n\nDec 1\n06:45PM\nCloud Computing\n\n\nDec 7 Lab 10\n\n\nDec 8\n06:45PM\n\n\n\nDec 14 Final Project",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#links-to-vignettes",
    "href": "course/schedule.html#links-to-vignettes",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "Links to Vignettes",
    "text": "Links to Vignettes\n\n\n\n\n\n\n\n\nWeek\nVideo Link\nAnnouncement Link\n\n\n\n\nWeek 3\nPivot Vignette\nDetails\n\n\nWeek 4\nWeek 4 Additional Content\nDetails\n\n\nWeek 5\nReprex Debugging Vignette\nDetails\n\n\nWeek 5\nCorrelation Trick Video\nDetails\n\n\nWeek 6\nBaby Names String Analysis\nDetails\n\n\nWeek 7\nSQL Extra Slides\nDetails\n\n\nWeek 7\nJoins Vignette\nDetails\n\n\nWeek 7\nBillboard Vignette\nDetails\n\n\nWeek 8\nFunctional Programming\nDetails\n\n\nWeek 9\nWebscraping\nDetails\n\n\nWeek 10\nAPI Vignette\nDetails\n\n\nWeek 10\nGithub Vignette\nDetails\n\n\nWeek 11\nN-Gram Extra Slides\n\n\n\nWeek 11\nSentiment Analysis Vignette\nDetails\n\n\nWeek 12\nStructured Data and Tool Calling Slides\nDetails",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/overview.html",
    "href": "course/overview.html",
    "title": "DATA607 - Data Acquisition and Management",
    "section": "",
    "text": "In this course students will learn about core concepts of contemporary data collection and its management. Topics will include an introduction to programming and collaboration in statistical software packages, data visualization techniques, data wrangling and transformation, exploratory data analysis and data quality checks, data acquisition from a variety of sources including databases and the web, tools for working with textual and graph data, and working with large datasets in a cloud computing environment.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  }
]